---
layout : single
title: "[ViT] Visual Transformer Model"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---

Classification에 사용할 Visual Transformer(ViT) 모델 구축    

## 0. ViT Model    

&nbsp;

<div align="center">
  <img src="/assets/images/WM/63.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ViT Model 설명**   
  - 이번에 구현한 ViT Model의 경우 다음과 같은 Process를 가지고 있음   
  - 이미지를 4x4 Patch로 분할한 후, 각 Patch에 Linear Projection을 적용하여 Dense layer로 인코딩을 수행   
  - 인코딩된 Feature는 Position Embedding과 결합되어 Transformer Block으로 전달됨   
  - 최종적으로 Position Embedding과 결합된 Feature를 MLP에 통과시켜 Output Predict를 생성    

&nbsp;

## 1. MLP Head   

```python
import torch
import torch.nn as nn

# --- MLP 헤드 정의 ---
class MLPHead(nn.Module):
    def __init__(self, in_features, hidden_units, num_classes, dropout_rate):
        super().__init__()
        layers = []
        for units in hidden_units:
            layers += [nn.Linear(in_features, units), nn.GELU(), nn.Dropout(dropout_rate)]
            in_features = units
        layers.append(nn.Linear(in_features, num_classes))
        self.mlp = nn.Sequential(*layers)

    def forward(self, x):
        return self.mlp(x)
```

- **MPLHead**   
  - **Transformer의 출력부**   
    - Input Image Patch를 Transformer에 통과시킨 후에는 Class token이 추출됨   
    - 이를 여러 Encoder Block과 최종 Layer Normalization을 통과 후, [B,D] 크기의 1D Representation vector $$y$$로 사용    
  - **CNN Classificer와의 유사성**   
    - 일반 CNN : Final Feature Map → Flatten → FC Layer → Class Proportion    
    - ViT : Class token $$y$$ → MLPHead → FC Layer → Class Proportion   
  - **MLPHead의 역할**   
    - Class Token, $$y$$를 입력으로 받음   
    - `Linear → GELU → Dropout`의 hidden layer로 구성    
    - 마지막 Linear layer를 거쳐 `num_classes` size의 logits 생성    

&nbsp;

## 2. Visual Transformer Model   

```python
import torch
import torch.nn as nn


# --- Vision Transformer 모델 정의 ---
class VisionTransformer(nn.Module):
    def __init__(
        self,
        image_size: int,
        patch_size: int,
        in_channels: int,
        num_layers: int,
        num_heads: int,
        embed_dim: int,
        transformer_units: list,
        mlp_head_units: list,
        num_classes: int,
        dropout_rate: float = 0.1,
    ):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        self.patch_dim = patch_size * patch_size * in_channels

        # Patch Split
        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)
        # Patch → Embedding
        self.patch_proj = nn.Linear(self.patch_dim, embed_dim)
        # Position Embedding
        self.position_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))

        # Transformer Block 여러 개 쌓기
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, transformer_units, dropout_rate)
            for _ in range(num_layers)
        ])

        # Classification을 위한 MLP Head
        self.mlp_head = MLPHead(embed_dim * self.num_patches, mlp_head_units, num_classes, dropout_rate)

    def get_patches(self, images: torch.Tensor) -> torch.Tensor:
        """
        images: (B, C, H, W)
        return: (B, num_patches, patch_dim)
        """
        B = images.size(0)
        patches = self.unfold(images)               # (B, patch_dim, num_patches)
        patches = patches.transpose(1, 2)           # (B, num_patches, patch_dim)
        return patches

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (B, C, H, W)
        return: logits (B, num_classes)
        """
        # 1) Patch Split & Linear Projection
        patches = self.get_patches(x)               # (B, N, patch_dim)
        x = self.patch_proj(patches)               # (B, N, embed_dim)
        x = x + self.position_embed                # Position Embedding 더하기

        # 2) Transformer Block
        for block in self.transformer_blocks:
            x = block(x)                           # (B, N, embed_dim)

        # 3) Flatten 후, MLPHead로 Classification
        x = x.flatten(1)                           # (B, N * embed_dim)
        logits = self.mlp_head(x)                  # (B, num_classes)
        return logits
```

- **ViT Model**   
  - **Patch Split & Projection**   
    - ``nn.Unfold`로 이미지를 `(B, C , H, W)` → `(B, patch_dim, num_patches)` 형태로 Split    
    - `nn,Linear(patch_dim, embed_dim)`을 통해 각 patch를 embedding 차원으로 projection    
    - `position_embed`로 Patch의 순서 정보를 더함    
  - **Transformer Encoder Block**   
    - `num_layers`만큼 `TransformerBlock`을 쌓아 입력 시퀀스 `(B, N, embed_dim)` 처리  
    - 각 Block은 MultiHead Attention + MLP sub-layer로 구성됨    
  - **Classification MLP Head**   
    - 모든 Patch Embedding을 Flatten → `N * embed_dim`의 vector 생성   
    - `MLPHead`를 통해 `Linear → GELU → Dropout`의 hidden layer를 통과 → 마지막 `Linear(num_classes)` 연산으로 Class Logit 생성   
  - **Output**   
    - `forward`의 반환값, `logits`은 `(B, num_classes)`의 형태이므로 Softmax 등 후처리를 통해 Classification의 결과로 사용    

&nbsp;

## 3. VisTransformer Class    

<details>
<summary>VisTransformer code</summary>
<div markdown="1">

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
from torch.utils.data import TensorDataset, DataLoader, random_split
from torchvision import transforms
import matplotlib.pyplot as plt
import pickle
import math


# --- 전체 파이프라인 클래스 ---
class VisTransformer:
    def __init__(self, x_train, x_test, y_train, y_test):
        """
        Args:
            x_train (Tensor): (N, C, H, W) 형태의 Train Image
            x_test  (Tensor): (M, C, H, W) 형태의 Test Image
            y_train (Tensor): (N, num_classes) 형태의 원-핫 라벨
            y_test  (Tensor): (M, num_classes) 형태의 원-핫 라벨
        """
        # 데이터 저장
        self.x_train = x_train
        self.x_test = x_test
        self.y_train = y_train
        self.y_test = y_test

        # Input size, Patch 관련 파라미터 계산
        _, c, H, W = x_train.shape
        assert H == W, "이미지는 정사각형이어야 합니다."
        self.image_size = H
        self.input_shape = (H, W, c)

        self.label_size = y_test.size(1)
        self.patch_size = 13
        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.projection_dim = 96
        self.num_heads = 4
        self.transformer_units = [self.projection_dim * 2, self.projection_dim]
        self.transformer_layers = 16
        self.mlp_head_units = [2048, 1024]
        self.dropout_rate = 0.1

        # Model Instance 생성  
        self.model = VisionTransformer(
            image_size=self.image_size,
            patch_size=self.patch_size,
            in_channels=c,
            num_layers=self.transformer_layers,
            num_heads=self.num_heads,
            embed_dim=self.projection_dim,
            transformer_units=self.transformer_units,
            mlp_head_units=self.mlp_head_units,
            num_classes=self.label_size,
            dropout_rate=self.dropout_rate
        )

    def augmentation(self):
        """
        Data Augmentation
        """
        mean = self.x_train.mean().item()
        std  = self.x_train.std().item()
        return transforms.Compose([
            transforms.RandomRotation(2),                           # ±2도 회전
            transforms.RandomResizedCrop(self.image_size, scale=(0.8,1.2)),
            transforms.Normalize([mean], [std])
        ])

    def get_patches(self, images):
        """
        Extract patches from images
        """
        return self.model.get_patches(images)

    def see_patches(self, num_image: int):
        """
        Visualizes the patches of a test image at the specified index.
        """
        img = self.x_train[num_image]          # (C, H, W)
        patches = self.get_patches(img.unsqueeze(0))  # (1, N, patch_dim)

        # 원본 이미지
        plt.figure()
        plt.title(f"Wafer #{num_image}")
        plt.imshow(img.squeeze().cpu().numpy(), cmap="gray")
        plt.axis("off")
        plt.show()

        print(f"Image size: {self.image_size} x {self.image_size}")
        print(f"Patch size: {self.patch_size} x {self.patch_size}")
        print(f"Patches per image: {patches.size(1)}")
        print(f"Elements per patch (dim): {patches.size(2)}")
        print(f"Num patch (self.num_patches): {self.num_patches}")

        # 패치 그리드
        n = int(math.sqrt(patches.size(1)))
        fig, axes = plt.subplots(n, n, figsize=(4, 4))
        for i in range(patches.size(1)):
            patch = patches[0, i].reshape(self.patch_size, self.patch_size).cpu().detach().numpy()
            ax = axes[i // n, i % n]
            ax.imshow(patch, cmap="gray")
            ax.axis("off")
        plt.show()

    def train_model(self, batch_size=512, num_epochs=40, save_path=None):
        """
        Train the model with the given parameters and save the best weights.
        """
        # Dataset / DataLoader 준비
        dataset = TensorDataset(self.x_train, self.y_train.float())
        val_n = int(len(dataset) * 0.1)
        train_n = len(dataset) - val_n
        train_ds, val_ds = random_split(dataset, [train_n, val_n])
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        val_loader   = DataLoader(val_ds,   batch_size=batch_size)
        test_loader  = DataLoader(TensorDataset(self.x_test, self.y_test.float()),
                                  batch_size=batch_size)

        optimizer = optim.Adam(self.model.parameters(), lr=1e-3)
        scheduler = ExponentialLR(optimizer, gamma=0.9)
        criterion = nn.BCEWithLogitsLoss()

        best_val_acc = 0.0
        history = {'loss':[], 'accuracy':[], 'val_loss':[], 'val_accuracy':[]}

        for epoch in range(num_epochs):
            # --- Train ---
            self.model.train()
            running_loss = running_acc = 0.0
            total = 0
            for xb, yb in train_loader:
                optimizer.zero_grad()
                logits = self.model(xb)
                loss = criterion(logits, yb)
                loss.backward()
                optimizer.step()

                preds = (torch.sigmoid(logits) > 0.5).float()
                acc = (preds == yb).float().mean().item()

                running_loss += loss.item() * xb.size(0)
                running_acc  += acc * xb.size(0)
                total += xb.size(0)

            train_loss = running_loss / total
            train_acc  = running_acc  / total

            # --- Validation ---
            self.model.eval()
            val_loss = val_acc = 0.0
            total_v = 0
            with torch.no_grad():
                for xb, yb in val_loader:
                    logits = self.model(xb)
                    loss = criterion(logits, yb)
                    preds = (torch.sigmoid(logits) > 0.5).float()
                    acc = (preds == yb).float().mean().item()

                    val_loss += loss.item() * xb.size(0)
                    val_acc  += acc * xb.size(0)
                    total_v += xb.size(0)

            val_loss /= total_v
            val_acc  /= total_v

            
            history['loss'].append(train_loss)
            history['accuracy'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_accuracy'].append(val_acc)

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_wts = self.model.state_dict()
                if save_path:
                    torch.save(best_wts, save_path)
                    with open(save_path + '_history', 'wb') as f:
                        pickle.dump(history, f)

            scheduler.step()
            print(f"Epoch {epoch+1}/{num_epochs}  "
                  f"train_loss={train_loss:.4f} train_acc={train_acc:.4f}  "
                  f"val_loss={val_loss:.4f} val_acc={val_acc:.4f}")

        # Test
        self.model.load_state_dict(best_wts)
        self.model.eval()
        test_loss = test_acc = 0.0
        total_t = 0
        with torch.no_grad():
            for xb, yb in test_loader:
                logits = self.model(xb)
                loss = criterion(logits, yb)
                preds = (torch.sigmoid(logits) > 0.5).float()
                acc = (preds == yb).float().mean().item()

                test_loss += loss.item() * xb.size(0)
                test_acc  += acc * xb.size(0)
                total_t   += xb.size(0)

        test_acc = test_acc / total_t
        print(f"Test accuracy: {test_acc * 100:.2f}%")
        return history

    def load_model(self, path, plot=False):
        """
        Loads the saved weights and optionally plots the training history.
        """
        self.model.load_state_dict(torch.load(path))
        if plot:
            with open(path + '_history', 'rb') as f:
                history = pickle.load(f)
            plt.figure()
            plt.plot(history['accuracy'], label='Train Acc')
            plt.plot(history['val_accuracy'], label='Val Acc')
            plt.plot(history['loss'], label='Train Loss')
            plt.plot(history['val_loss'], label='Val Loss')
            plt.xlabel('Epoch')
            plt.legend()
            plt.show()
        return self.model

    def model_eval(self, x_test=None, y_test=None, batch_size=512):
        """
        Evaluates the trained model and prints loss and accuracy.
        """
        x_t = x_test if x_test is not None else self.x_test
        y_t = y_test if y_test is not None else self.y_test
        loader = DataLoader(TensorDataset(x_t, y_t.float()), batch_size=batch_size)

        self.model.eval()
        criterion = nn.BCEWithLogitsLoss()
        total_loss = total_acc = 0.0
        total = 0
        with torch.no_grad():
            for xb, yb in loader:
                logits = self.model(xb)
                loss = criterion(logits, yb)
                preds = (torch.sigmoid(logits) > 0.5).float()
                acc = (preds == yb).float().mean().item()

                total_loss += loss.item() * xb.size(0)
                total_acc  += acc * xb.size(0)
                total      += xb.size(0)

        avg_loss = total_loss / total
        avg_acc  = total_acc  / total
        print(f"Eval loss: {avg_loss:.4f}, Eval accuracy: {avg_acc * 100:.2f}%")
        return avg_loss, avg_acc
```

</div>
</details>

- **VisTransformer Class**   
  - 해당 클래스는 다음의 기능들을 Pipeline의 형태로 종합하여 구현   
    - **Date Augmentation & Patch 추출**      
    - **ViT Model Instance**    
    - **Visualization**    
    - **Train & Validation**     
    - **Save Checkpoint**    
    - **Model Load & Evaluation**      

&nbsp;

