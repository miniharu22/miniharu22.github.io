---
layout : single
title: "[CNN] CNN Model with Decreasing Complexity"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---

Model의 Complexity를 낮춰도 Performance가 유지되는지 검증   

## 0. Define CNN Model with Less Complexity   

```python
import torch
import torch.nn as nn

from torchsummary import summary

class Model7(nn.Module):
    def __init__(self, num_classes=9):
        super(Model7, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2, padding=1),

            nn.Conv2d(16, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2, padding=1),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveMaxPool2d((1, 1))
            )
        
        self.dropout = nn.Dropout(0.2)
        self.classifier = nn.Linear(64, num_classes)
        
        self._init_weights()


    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

model7 = Model7(num_classes=9)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
summary(model7, input_size=(3, 53, 52), device=device.type)
```

**Model Architecture**   
        Layer (type)               Output Shape         Param #  
================================================================  
            Conv2d-1           [-1, 16, 53, 52]             448  
       BatchNorm2d-2           [-1, 16, 53, 52]              32  
              ReLU-3           [-1, 16, 53, 52]               0  
         MaxPool2d-4           [-1, 16, 27, 27]               0  
            Conv2d-5           [-1, 32, 27, 27]           4,640  
       BatchNorm2d-6           [-1, 32, 27, 27]              64  
              ReLU-7           [-1, 32, 27, 27]               0  
         MaxPool2d-8           [-1, 32, 14, 14]               0  
            Conv2d-9           [-1, 64, 14, 14]          18,496  
      BatchNorm2d-10           [-1, 64, 14, 14]             128  
             ReLU-11           [-1, 64, 14, 14]               0  
AdaptiveMaxPool2d-12             [-1, 64, 1, 1]               0  
           Linear-13                    [-1, 9]             585  
================================================================  
Total params: 24,393  
Trainable params: 24,393  
Non-trainable params: 0  
----------------------------------------------------------------  
Input size (MB): 0.03  
Forward/backward pass size (MB): 1.97  
Params size (MB): 0.09  
Estimated Total Size (MB): 2.09  
----------------------------------------------------------------  
{: .notice}   

&nbsp;

## 1. Train & Validation with L2 Regularization   

```python
import torch
import torch.nn as nn
import torch.optim as optim

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model7 = Model7(num_classes=9).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model7.parameters(), lr=1e-4, weight_decay=1e-4)

num_epochs = 100

# Early stopping parameters
patience = 10
best_val_loss = float('inf')
epochs_no_improve = 0


history7 = {
    "loss": [],
    "accuracy": [],
    "val_loss": [],
    "val_accuracy": []
}

for epoch in range(1, num_epochs + 1):
    model7.train()
    running_loss = running_corrects = running_total = 0

    for imgs, labels in train_aug_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        logits = model7(imgs)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        batch_size = imgs.size(0)
        running_loss    += loss.item() * batch_size
        preds = logits.argmax(dim=1)
        running_corrects += (preds == labels).sum().item()
        running_total   += batch_size

    epoch_loss = running_loss / running_total
    epoch_acc  = running_corrects / running_total
    history7["loss"].append(epoch_loss)
    history7["accuracy"].append(epoch_acc)

    model7.eval()
    val_loss = val_corrects = val_total = 0

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            logits = model7(imgs)
            loss = criterion(logits, labels)

            bs = imgs.size(0)
            val_loss     += loss.item() * bs
            preds        = logits.argmax(dim=1)
            val_corrects += (preds == labels).sum().item()
            val_total   += bs

    epoch_val_loss = val_loss / val_total
    epoch_val_acc  = val_corrects / val_total
    history7["val_loss"].append(epoch_val_loss)
    history7["val_accuracy"].append(epoch_val_acc)

    print(f'Epoch {epoch}/{num_epochs}  '
          f'Train Loss: {epoch_loss:.4f}  Train Acc: {epoch_acc:.4f}  '
          f'Val Loss: {epoch_val_loss:.4f}  Val Acc: {epoch_val_acc:.4f}')

    # Early stopping check
    if epoch_val_loss < best_val_loss:
        # Update best validation loss and reset counter
        best_val_loss = epoch_val_loss
        epochs_no_improve = 0
        # Save the best model
        torch.save(model7.state_dict(), 'best_model.pth')
    # If no improvement, increment the counter
    else:
        # No improvement in validation loss
        epochs_no_improve += 1
        print(epochs_no_improve)
        if epochs_no_improve >= patience:
            print('Early stopping triggered')
            break
```

**Train & Val 결과**   
Epoch 1/100  Train Loss: 0.3937  Train Acc: 0.8820  Val Loss: 0.1393  Val Acc: 0.9608  
Epoch 2/100  Train Loss: 0.1962  Train Acc: 0.9337  Val Loss: 0.1198  Val Acc: 0.9648  
Epoch 3/100  Train Loss: 0.1661  Train Acc: 0.9427  Val Loss: 0.1117  Val Acc: 0.9678  
Epoch 4/100  Train Loss: 0.1489  Train Acc: 0.9485  Val Loss: 0.1031  Val Acc: 0.9682  
Epoch 5/100  Train Loss: 0.1367  Train Acc: 0.9520  Val Loss: 0.1035  Val Acc: 0.9689  
Epoch 6/100  Train Loss: 0.1281  Train Acc: 0.9546  Val Loss: 0.0957  Val Acc: 0.9701  
Epoch 7/100  Train Loss: 0.1208  Train Acc: 0.9576  Val Loss: 0.0890  Val Acc: 0.9713  
Epoch 8/100  Train Loss: 0.1151  Train Acc: 0.9594  Val Loss: 0.0879  Val Acc: 0.9728  
Epoch 9/100  Train Loss: 0.1106  Train Acc: 0.9607  Val Loss: 0.0855  Val Acc: 0.9726  
Epoch 10/100  Train Loss: 0.1053  Train Acc: 0.9625  Val Loss: 0.0861  Val Acc: 0.9723  
Epoch 11/100  Train Loss: 0.1015  Train Acc: 0.9639  Val Loss: 0.0821  Val Acc: 0.9733  
Epoch 12/100  Train Loss: 0.0981  Train Acc: 0.9652  Val Loss: 0.0809  Val Acc: 0.9733  
Epoch 13/100  Train Loss: 0.0947  Train Acc: 0.9660  Val Loss: 0.0831  Val Acc: 0.9730  
Epoch 14/100  Train Loss: 0.0918  Train Acc: 0.9673  Val Loss: 0.0814  Val Acc: 0.9734  
Epoch 15/100  Train Loss: 0.0885  Train Acc: 0.9687  Val Loss: 0.0803  Val Acc: 0.9737  
Epoch 16/100  Train Loss: 0.0867  Train Acc: 0.9690  Val Loss: 0.0923  Val Acc: 0.9697  
Epoch 17/100  Train Loss: 0.0842  Train Acc: 0.9703  Val Loss: 0.0789  Val Acc: 0.9738  
Epoch 18/100  Train Loss: 0.0817  Train Acc: 0.9707  Val Loss: 0.0862  Val Acc: 0.9722  
Epoch 19/100  Train Loss: 0.0800  Train Acc: 0.9717  Val Loss: 0.0797  Val Acc: 0.9739  
...  
Epoch 40/100  Train Loss: 0.0534  Train Acc: 0.9812  Val Loss: 0.0805  Val Acc: 0.9744  
Early stopping triggered  
{: .notice}   

&nbsp;

## 2. Loss & Accuracy Analysis    

```python
plt.subplot(1,2,1)
plt.plot(history7['loss'])
plt.plot(history7['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.ylim([0, 1])
plt.legend(['training loss', 'validation loss'])

plt.subplot(1,2,2)
plt.plot(history7['accuracy'])
plt.plot(history7['val_accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.ylim([0.9, 1])
plt.legend(['training accuracy', 'validation accuracy'])
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/56.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}  

&nbsp;

- Accuracy는 일부 감소했지만, loss가 감소한 것을 확인 가능   
- 즉, Model의 Complexity는 줄어들었지만 오히려 Overfitting으로부터 조금 자유로워짐을 알 수 있음   

&nbsp;

## 3. Correctly Classified Proportion   

```python
failure_types = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]

val_dataset = torch.load("C://Users/isang/OneDrive/Desktop/WM/data/Val.pt")
val_tuple = from_DataBatch_to_list(val_dataset)

cm2 = confusion_matrix(model7, failure_types, val_tuple, threshold2)
p = classes_proportion_correctly_classified(cm2, failure_types)

print(p)

print(f"Mean Proportion : {np.mean(list(p.values()))}")
```

{'Center': 0.9430656934306569,   
 'Donut': 0.9101123595505618,   
 'Edge-Loc': 0.9083023543990086,   
 'Edge-Ring': 0.9902975420439845,   
 'Loc': 0.8629893238434164,   
 'Near-full': 1.0,    
 'Random': 0.8478260869565217,   
 'Scratch': 0.8702702702702703,   
 'none': 0.9880332314579656}  
Mean Proportion : 0.9147626256104911   
{: .notice}   

- Model의 Complexity는 줄어듦었음에도 실제 Classification에 대한 Correct Proportion은 거의 동일한 수준을 유지함을 알 수 있음   
- 해당 모델을 토대로 Test Set에 대해 Evaluation을 수행할 예정    

&nbsp;

