---
layout : single
title: "[CNN] Prepare Dataset for Base Model"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---

Simple한 CNN 모델을 사용하기 전 필요한 데이터셋을 준비   

## 0. Prepare Train & Valid set  
### 0-1. Load JSON Data   

```python
import json
import pandas as pd

with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_padded_resized_not_augmented.json', 'r') as f:
  data = json.load(f)

cols = data['columns']

dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(data['data'])):
    if i == 0:
      dict_[cols[i]].append(data['data'][j][i])
    elif i == 1:
      dict_[cols[i]].append(data['data'][j][i])
      
data = pd.DataFrame(dict_)


classes = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]
training = [0] * 9

for i in range(len(training)):
  training[i] = data[data['failureType'] == classes[i]]
  training[i] = training[i].reset_index(drop = True)
```

&nbsp;

### 0-2. Extract Validation Set   

```python
validation = []

for i in range(len(training)):
  validation.append(training[i].sample(frac = 0.2))
  training[i].drop(validation[i].index, inplace=True)
```

&nbsp;

### 0-3. Folders including Train & Valid Set   

```python
import os
import shutil
import matplotlib

base_dir   = r"C://Users/isang/OneDrive/Desktop/WM/data/"
train_dir  = os.path.join(base_dir, "WM811K_train")
val_dir    = os.path.join(base_dir, "WM811K_val")

for d in (train_dir, val_dir):
    if os.path.exists(d):
        shutil.rmtree(d)

os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir,   exist_ok=True)

for cls in classes:
    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)
    os.makedirs(os.path.join(val_dir,   cls), exist_ok=True)

for i in range(len(classes)):
  for j in range(len(training[i])):
    if(j%1000 == 0):
      print('/WM811K_train/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train/'+classes[i]+'/'+str(j)+'.png', training[i]['waferMap'].iloc[j])


for i in range(len(classes)):
  for j in range(len(validation[i])):
    if(j%1000 == 0):
      print('/WM811K_val/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_val/'+classes[i]+'/'+str(j)+'.png', validation[i]['waferMap'].iloc[j])
```

&nbsp;

### Save Validation Set   

```python
validation_preprocessed = pd.concat([validation[0],validation[1],validation[2],
                        validation[3],validation[4],validation[5],
                        validation[6],validation[7],validation[8]])

validation_preprocessed['failureType'] = list(map(lambda x: str(x), validation_preprocessed['failureType']))

parsed = json.loads(validation_preprocessed.to_json(orient = "split"))

with open('C://Users/isang/OneDrive/Desktop/WM/data/Validation.json','w') as f:
    json.dump(parsed, f)
del parsed
```

&nbsp;

## 1. Prepare Test set   
### 1-1. Load JSON Data   

```python
with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_test_padded_resized_not_augmented.json', 'r') as f:
  data = json.load(f)

cols = data['columns']

dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(data['data'])):
    if i == 0:
      dict_[cols[i]].append(data['data'][j][i])
    elif i == 1:
      dict_[cols[i]].append(data['data'][j][i])
      
data = pd.DataFrame(dict_)

classes = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]
test = [0] * 9

for i in range(len(test)):
  test[i] = data[data['failureType'] == classes[i]]
  test[i] = test[i].reset_index(drop = True)
```

&nbsp;

### 1-2. Folders including Test Set     

```python
base_dir   = r"C://Users/isang/OneDrive/Desktop/WM/data/"
test_dir  = os.path.join(base_dir, "WM811K_test")

if os.path.exists(test_dir):
    shutil.rmtree(test_dir)

os.makedirs(test_dir, exist_ok=True)

for cls in classes:
    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)

for i in range(len(classes)):
  for j in range(len(test[i])):
    if(j%1000 == 0):
      print('/WM811K_test/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_test/'+classes[i]+'/'+str(j)+'.png', test[i]['waferMap'].iloc[j])
```

&nbsp;

## 2. Prepare Augmentated set   
### 2-1. Load JSON Data for Train_Aug    

```python
with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_final_train.json', 'r') as f:
  data_aug = json.load(f)

cols = data_aug['columns']

dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(data_aug['data'])):
    if i == 0:
      dict_[cols[i]].append(data_aug['data'][j][i])
    elif i == 1:
      dict_[cols[i]].append(data_aug['data'][j][i])

data_aug = pd.DataFrame(dict_)

classes = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]
training_aug = [0] * 9

for i in range(len(training_aug)):
  training_aug[i] = data_aug[data_aug['failureType'] == classes[i]]
  print(len(training_aug[i]))
  training_aug[i] = training_aug[i].reset_index(drop = True)
```

&nbsp;

### 2-2. Load JSON Data for Vaild     

```python
with open('C://Users/isang/OneDrive/Desktop/WM/data/Validation.json', 'r') as f:
  validation = json.load(f)

dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(validation['data'])):
    if i == 0:
      dict_[cols[i]].append(validation['data'][j][i])
    elif i == 1:
      dict_[cols[i]].append(validation['data'][j][i])

validation = pd.DataFrame(dict_)

validation_set = [0] * 9

for i in range(len(validation_set)):
  validation_set[i] = validation[validation['failureType'] == classes[i]]
  print(len(validation_set[i]))
  validation_set[i] = validation_set[i].reset_index(drop = True)
```

&nbsp;

### 2-3. Rotation & Flip for Valid Data  

```python
from PIL import Image

for i in range(len(validation_set) - 1):
  for j in range(len(validation_set[i]['waferMap'])):
    img = copy.deepcopy(Image.fromarray(np.array(validation_set[i]['waferMap'][j], dtype = np.uint8)))
    horiz_img = img.transpose(method = Image.FLIP_LEFT_RIGHT)
    vert_img = img.transpose(method = Image.FLIP_TOP_BOTTOM)
    rotated_img_1 = img.transpose(method = Image.ROTATE_90)
    rotated_img_2 = img.transpose(method = Image.ROTATE_270)
    validation_set[i].loc[validation_set[i].shape[0]] = [horiz_img, classes[i]]
    validation_set[i].loc[validation_set[i].shape[0]] = [vert_img, classes[i]]
    validation_set[i].loc[validation_set[i].shape[0]] = [rotated_img_1, classes[i]]
    validation_set[i].loc[validation_set[i].shape[0]] = [rotated_img_2, classes[i]]

for i in range(len(validation_set)):
  for j in range(len(validation_set[i]['waferMap'])):
    validation_set[i]['waferMap'][j] = np.array(validation_set[i]['waferMap'][j]).tolist()
```

### 2-4. List to Tuple Conversion   

```python
def to_hashable(lista):
  for i in range(len(lista)):
    lista[i] = tuple(lista[i])
  
  return lista

for i in range(len(training_aug)):
  new_col1 = list(map(lambda x: to_hashable(x), training_aug[i]['waferMap']))
  new_col1 = list(map(lambda x: tuple(x), new_col1))

  new_col2 = list(map(lambda x: to_hashable(x), validation_set[i]['waferMap']))
  new_col2 = list(map(lambda x: tuple(x), new_col2))

  training_aug[i]['waferMap'] = new_col1
  validation_set[i]['waferMap'] = new_col2

  del new_col1
  del new_col2

# Remove the validation-set from the training augmented set.
for i in range(len(training_aug)):
  training_aug[i] = pd.merge(training_aug[i], validation_set[i], indicator=True, how='outer').query('_merge=="left_only"').drop('_merge', axis=1)
```

&nbsp;

### 2-5. Folders including Augmentated Set   

```python
base_dir   = r"C://Users/isang/OneDrive/Desktop/WM/data/"
train_aug_dir  = os.path.join(base_dir, "WM811K_train_aug")

if os.path.exists(train_aug_dir):
    shutil.rmtree(train_aug_dir)

os.makedirs(train_aug_dir, exist_ok=True)

for cls in classes:
    os.makedirs(os.path.join(train_aug_dir, cls), exist_ok=True)

for i in range(len(classes)):
  for j in range(len(training_aug[i])):
    if(j%1000 == 0):
      print('/WM811K_train_aug/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_aug/'+classes[i]+'/'+str(j)+'.png', training_aug[i]['waferMap'].iloc[j])
```

&nbsp;

## 3. Data Loader  

```python
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

train_dir      = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train'
val_dir        = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_val'
test_dir       = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_test'
train_aug_dir  = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_aug'

transform = transforms.Compose([
    transforms.Resize((53, 52)),
    transforms.ToTensor(),
])

batch_size = 32
num_workers = 4
pin_memory = True

train_dataset     = datasets.ImageFolder(train_dir,     
                                         transform=transform)
val_dataset       = datasets.ImageFolder(val_dir,       
                                         transform=transform)
test_dataset      = datasets.ImageFolder(test_dir,      
                                         transform=transform)
train_aug_dataset = datasets.ImageFolder(train_aug_dir, 
                                         transform=transform)

train_loader     = DataLoader(train_dataset,     
                              batch_size=batch_size, 
                              shuffle=True,  
                              num_workers=num_workers, 
                              pin_memory=pin_memory)

val_loader       = DataLoader(val_dataset,       
                              batch_size=batch_size, 
                              shuffle=True,  
                              num_workers=num_workers, 
                              pin_memory=pin_memory)

test_loader      = DataLoader(test_dataset,      
                              batch_size=batch_size, 
                              shuffle=True,  
                              num_workers=num_workers, 
                              pin_memory=pin_memory)

train_aug_loader = DataLoader(train_aug_dataset, 
                              batch_size=batch_size, 
                              shuffle=True,  
                              num_workers=num_workers, 
                              pin_memory=pin_memory)

import torch

# Save validation DataLoader batches to disk
val_batches = list(val_dataset)
torch.save(val_batches, "C://Users/isang/OneDrive/Desktop/WM/data/Val.pt")

# Load them back
val_dataset = torch.load("C://Users/isang/OneDrive/Desktop/WM/data/Val.pt")
val_tuple = from_DataBatch_to_list(val_dataset)
```

&nbsp;


