---
layout : single
title: "[CNN] Dataset Split & Oversampling"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---

전체 데이터셋에서 Train/Test Set으로 분할, 데이터 간의 불균형을 해결하기 위해 직접 데이터를 제작 + 간단한 CNN 모델을 통해 원본 이미지와의 비교 및 평가    

## 0. Dataset Split to Train & Test Set  
### 0-1. Remove unneccessary columns from dataset   

```python
with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_clean.json', 'r') as f:
  data = json.load(f)

cols = data['columns']
dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(data['data'])):
    if i == 6:
      dict_[cols[i]].append(tuple(data['data'][j][i]))
    elif i == 5:
      dict_[cols[i]].append(np.array(data['data'][j][i]))
    else:
      dict_[cols[i]].append(data['data'][j][i])

data = pd.DataFrame(dict_)

data.drop(['dieSize', 'lotName', 'waferIndex', 'trainTestLabel', 'shape'], axis = 1, inplace = True)
data.head()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/30.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

- 오로지 Failure Type class에 대해서만 이미지 분류를 진행할 것이기 때문에 이외에 불필요한 데이터는 제거    

&nbsp;

### 0-2. Dataset Split    

```python
classes = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]
training = [0] * 9
test = [0] * 9

for i in range(9):
  training[i] = data[data['failureType'] == classes[i]]
  test[i] = training[i].sample(frac = 0.2)
  training[i] = training[i].drop(test[i].index)

print(sum(len(i) for i in training))
print(sum(len(i) for i in test))

training[0].head()
```

<div align="left">
  <strong>출력 결과</strong><br>
  136196<br> 
  34050
  <img src="/assets/images/WM/31.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

- 전체 Dataset에서 80:20의 비율로 Train Set과 Test Set을 분리함    


&nbsp;

### 0-3. Reset Index   

```python
# Reset Index
classes = failure_types

for i in range(len(training)):
  training[i] =  training[i].reset_index(drop = True)
  test[i] =  test[i].reset_index(drop = True)

training[0].head()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/32.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

## 1. Data Augmentation : Resizing    
### 1-1. Image Padding   

```python
import math
import copy

# Padding Function   
def apply_padding(a):
  dim1, dim2 = a.shape
  pad11 = 0
  pad12 = 0
  pad21 = 0
  pad22 = 0
  if(dim1 < 53):
    pad11 = int((53 - dim1)/2)
    pad12 = 53 - dim1 - pad11
  if(dim2 < 52):
    pad21 = int((52 - dim2)/2)
    pad22 = 52 - dim2 - pad21
  if(dim1 > 53):
    pad11 = int((math.ceil(dim1/53) * 53 - dim1)/2)
    pad12 = math.ceil(dim1/53) * 53 - dim1 - pad11
  if(dim2 > 52):
    pad21 = int((math.ceil(dim2/52) * 52 - dim2)/2)
    pad22 = math.ceil(dim2/52) * 52 - dim2 - pad21

  return np.pad(a, ((pad11,pad12),(pad21,pad22)), 'constant', constant_values = ((0,0),(0,0)))

# Train Set Padding   
training_padded = copy.deepcopy(training)

for i in range(len(training_padded)):
  for j in range(len(training_padded[i]['waferMap'])):
    if (j % 500 == 0):
      print(i, j)
    training_padded[i]['waferMap'][j] = np.array(training_padded[i]['waferMap'][j])
    training_padded[i]['waferMap'][j] = apply_padding(training_padded[i]['waferMap'][j])

# Test Set Padding   
test_padded = copy.deepcopy(test)

for i in range(len(test_padded)):
  for j in range(len(test_padded[i]['waferMap'])):
    if (j % 500 == 0):
      print(i, j)
    test_padded[i]['waferMap'][j] = np.array(test_padded[i]['waferMap'][j])
    test_padded[i]['waferMap'][j] = apply_padding(test_padded[i]['waferMap'][j])
```

- Data Augmentation을 본격적으로 수행하기 전에 가장 먼저 이미지의 사이즈를 동일하게 규격화시켜야 함    
- 이를 위해 이미지에 **Background Pixel을 추가하는 Padding 함수**를 정의하여 Train Set과 Test Set 각각에 수행     

&nbsp;

### 1-2. Image Resizing   

```python
import copy
import numpy as np
import torch
import torch.nn.functional as F

training_padded_resize = copy.deepcopy(training_padded)

for i, subgroup in enumerate(training_padded_resize):
    wafer_maps = subgroup['waferMap']
    for j, img_np in enumerate(wafer_maps):
        if j % 500 == 0:
            print(i, j)
        img_t = torch.from_numpy(img_np.astype(np.float32)).unsqueeze(0).unsqueeze(0)
        resized_t = F.interpolate(img_t, size=(53, 52), mode='nearest')
        resized_np = resized_t.squeeze().numpy().astype(int)
        training_padded_resize[i]['waferMap'][j] = resized_np

test_padded_resize = copy.deepcopy(test_padded)

for i, subgroup in enumerate(test_padded_resize):
    wafer_maps = subgroup['waferMap']
    for j, img_np in enumerate(wafer_maps):
        if j % 500 == 0:
            print(i, j)
        img_t = torch.from_numpy(img_np.astype(np.float32)).unsqueeze(0).unsqueeze(0)
        resized_t = F.interpolate(img_t, size=(53, 52), mode='nearest')
        resized_np = resized_t.squeeze().numpy().astype(int)
        test_padded_resize[i]['waferMap'][j] = resized_np

for i in range(9):
  print(np.unique(list(map(lambda x: x.shape, training_padded_resize[i]['waferMap']))))


for i in range(12):
  plt.subplot(3,4,i+1)
  plt.imshow(training_padded_resize[0]['waferMap'][i])
  plt.xticks([])
  plt.yticks([])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong><br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>
  [52 53]<br>  
  [52 53]<br>  
  <img src="/assets/images/WM/33.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

- Train Set과 Test Set의 모든 이미지에 대해 (52,53)의 크기로 동일하게 규격화를 진행   
  - **그 결과, 모든 이미지는 중심값인 (53,52)로 크기가 규격화되는 동시에 원본의 이미지를 그대로 보존할 수 있음**   
- 일부 이미지를 size와 함께 출력한 결과, Wafer의 size에는 일부 차이가 있더라도 모든 이미지가 동일한 크기로 맞춰져 있음을 확인   

&nbsp;

### 1-3. Store Dataset    

```python
training_preprocessed = pd.concat([training_padded_resize[0],training_padded_resize[1],training_padded_resize[2],
                                   training_padded_resize[3],training_padded_resize[4],training_padded_resize[5],
                                   training_padded_resize[6],training_padded_resize[7],training_padded_resize[8]])
del training_padded_resize

training_preprocessed['failureType'] = list(map(lambda x: str(x), training_preprocessed['failureType']))

parsed = json.loads(training_preprocessed.to_json(orient = "split"))
del training_preprocessed

with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_padded_resized_not_augmented.json','w') as f:
    json.dump(parsed, f)
del parsed

test_preprocessed = pd.concat([test_padded_resize[0],test_padded_resize[1],test_padded_resize[2],
                               test_padded_resize[3],test_padded_resize[4],test_padded_resize[5],
                               test_padded_resize[6],test_padded_resize[7],test_padded_resize[8]])
del test_padded_resize

test_preprocessed['failureType'] = list(map(lambda x: str(x), test_preprocessed['failureType']))

parsed = json.loads(test_preprocessed.to_json(orient = "split"))
del test_preprocessed

with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_test_padded_resized_not_augmented.json','w') as f:
    json.dump(parsed, f)
del parsed
```

- Padding과 Resizing이 끝난 각각의 Dataset은 `.josn` 포맷의 파일로 저장    

&nbsp;

## 2. Oversampling with Hand-craft
### 2-1. Load Dataset   

```python
import json
import pandas as pd

with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_padded_resized_not_augmented.json', 'r') as f:
  data_pad_resize = json.load(f)


cols = data_pad_resize['columns']

dict_ = dict()

for col in cols:
  dict_[col] = list()

for i in range(len(cols)):
  for j in range(len(data_pad_resize['data'])):
    if i == 0:
      dict_[cols[i]].append(data_pad_resize['data'][j][i])
    elif i == 1:
      dict_[cols[i]].append(data_pad_resize['data'][j][i])


data_pad_resize = pd.DataFrame(dict_)
data_pad_resize.head()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/34.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-2. Prepare Trainset   

```python
classes = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]
training_aug_padded = [0] * 9

for i in range(9):
  training_aug_padded[i] = data_pad_resize[data_pad_resize['failureType'] == classes[i]]

for i in range(len(training_aug_padded)):
  training_aug_padded[i] =  training_aug_padded[i].reset_index(drop = True)

training_aug_padded[7].sample(10)
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/35.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

- Train dataset의 경우, 각 class 별로 이미지를 저장할 필요가 있으므로 각 row 별로 dataframe을 구축   

&nbsp;

### 2-3. Template without Chips   

```python
import matplotlib.pyplot as plt
import numpy as np
import copy

template = copy.deepcopy(training_aug_padded[3]['waferMap'][6000])
for i in range(len(template)):
  for j in range(len(template[0])):
    if(template[i][j] == 2):
      template[i][j] = 1

plt.imshow(template)

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/36.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

- 데이터 간의 불균형을 해결하기 위해 각 class 별로 이미지를 직접 제작하여 Oversampling을 수행    
- 이를 위해 Wafer 상에 아무런 칩도 없는 템플릿을 만듦   

&nbsp;

### 2-4. Donut class Image   

```python
import random

# Euclidean distance
for n in range(2556):
  img = copy.deepcopy(template)
  for i in range(len(img)):
    for j in range(len(img[i])):
      if(img[i][j] == 1):
        dist = np.sqrt((i-26)**2 + (j-26)**2) 
        if(2.5 <= dist < 5):
          if(random.random() <= 0.1):
            img[i][j] = 2
        if(5 <= dist < 10):
          if(random.random() <= 0.5):
            img[i][j] = 2
        if(10 <= dist <= 17):
          if(random.random() <= 0.7):
            img[i][j] = 2
        if(17 < dist <= 20):
          if(random.random() <= 0.5):
            img[i][j] = 2
        if(20 < dist <= 24):
          if(random.random() <= 0.1):
            img[i][j] = 2
      # Let's add random mutations to the whole picture
      if(random.random() <= 0.05):
        if(img[i][j] == 1):
          img[i][j] = 2
        elif(img[i][j] == 2):
          img[i][j] = 1
  training_aug_padded[1].loc[training_aug_padded[1].shape[0]] = [img, np.array('Donut')]

plt.imshow(training_aug_padded[1]['waferMap'][2999])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/37.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-5. Loc class Image

```python
for n in range(189):
  img = copy.deepcopy(template)
  n_loc = random.randint(1, 3)
  for i in range(n_loc):
  # Loc doesn't have to be edge!
    loc_x = random.randint(12, 38)
    loc_y = random.randint(12, 38)
    ray_x = random.randint(2, 8)
    ray_y = random.randint(2, 8)
    for i in range(loc_x - ray_x, loc_x + ray_x):
      for j in range(loc_y - ray_y, loc_y + ray_y):
        if(img[i][j] == 1):
          img[i][j] = 2
        # Let's add random mutations to the loc anomaly
        if(random.random() <= 0.05):
          if(img[i][j] == 1):
            img[i][j] = 2
          elif(img[i][j] == 2):
            img[i][j] = 1
  # Let's add random mutations to the whole picture
  for i in range(len(img)):
      for j in range(len(img[i])):
        if(random.random() <= 0.03):
          if(img[i][j] == 1):
            img[i][j] = 2
          elif(img[i][j] == 2):
              img[i][j] = 1
  training_aug_padded[4].loc[training_aug_padded[4].shape[0]] = [img, np.array('Loc')]


print(len(training_aug_padded[4]))
plt.imshow(training_aug_padded[4]['waferMap'][2999])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/38.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-6. Near-full class Image  

```python
for n in range(2881):
  img = copy.deepcopy(template)
  for i in range(len(img)):
    for j in range(len(img[i])):
      if(img[i][j] != 0):
        if(random.random() <= 0.9):
          img[i][j] = 2
        else:
          img[i][j] = 1
  training_aug_padded[5].loc[training_aug_padded[5].shape[0]] = [img, np.array('Near-full')]

print(len(training_aug_padded[5]))
plt.imshow(training_aug_padded[5]['waferMap'][2999])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/39.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-7. Random class Image  

```python
for n in range(2312):
  img = copy.deepcopy(template)
  max = random.uniform(0.25, 0.45)
  for i in range(len(img)):
    for j in range(len(img[i])):
      k = random.random()
      if(k <= max):
        if(img[i][j] == 1):
          img[i][j] = 2
        elif(img[i][j] == 2):
          img[i][j] = 1
  training_aug_padded[6].loc[training_aug_padded[6].shape[0]] = [img, np.array('Random')]

print(len(training_aug_padded[6]))
plt.imshow(training_aug_padded[6]['waferMap'][2999])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/40.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-8. Scratch class Image  

```python
for n in range(2073):
  img = copy.deepcopy(template)
  n_scratch = random.randint(1, 3)

  for _ in range(n_scratch):
    # Scratch starting point
    y_start = random.randint(0, 52)
    x_start = random.randint(0, 51)
    dist = np.sqrt((y_start-26)**2 + (x_start-26)**2) 
    trovato = False
    while(not trovato):
      if((img[y_start][x_start] != 0) and (dist > 24)):
        trovato = True
      else:
        y_start = random.randint(0, 52)
        x_start = random.randint(0, 51)
        dist = np.sqrt((y_start-26)**2 + (x_start-26)**2)

    # Choosing moving direction and intensity 
    move = random.randint(8, 20)
    x = x_start
    y = y_start
    if(x_start < 26 and y_start > 26):
      # Starting from the bottom left
      for k in range(move):
        if(random.random() < 0.7):
          j = x + 1
        else:
          j = x
        i = y - 1
        if(i < 52 and j < 51):
          if(img[i][j] != 0):
            img[i][j] = 2
            if(random.random() < 0.05):
              img[i][j-1] = 2
            if(random.random() < 0.05):
              img[i][j+1] = 2
          x = j
          y = i
    elif(x_start < 26 and y_start <= 26):
      # Starting from the top left
      for k in range(move):
        if(random.random() < 0.7):
          j = x + 1
        else:
          j = x
        i = y + 1
        if(i < 52 and j < 51):
          if(img[i][j] != 0):
            img[i][j] = 2
            if(random.random() < 0.05):
              img[i][j-1] = 2
            if(random.random() < 0.05):
              img[i][j+1] = 2
          x = j
          y = i
    elif(x_start >= 26 and y_start > 26):
      # Starting from the bottom right
      for k in range(move):
        if(random.random() < 0.7):
          j = x - 1
        else:
          j = x
        i = y - 1
        if(i < 52 and j < 51):
          if(img[i][j] != 0):
            img[i][j] = 2
            if(random.random() < 0.05):
              img[i][j-1] = 2
            if(random.random() < 0.05):
              img[i][j+1] = 2
          x = j
          y = i
    elif(x_start >= 26 and y_start <= 26):
      # Starting from the top right
      for k in range(move):
        if(random.random() < 0.7):
          j = x - 1
        else:
          j = x
        i = y + 1
        if(i < 52 and j < 51):
          if(img[i][j] != 0):
            img[i][j] = 2
            if(random.random() < 0.05):
              img[i][j-1] = 2
            if(random.random() < 0.05):
              img[i][j+1] = 2
          x = j
          y = i

  # Let's add random mutations to the whole picture
  for i in range(len(img)):
    for j in range(len(img[i])):
      if(random.random() <= 0.03):
        if(img[i][j] == 1):
          img[i][j] = 2
        elif(img[i][j] == 2):
          img[i][j] = 1

  training_aug_padded[7].loc[training_aug_padded[7].shape[0]] = [img, np.array('Scratch')]

print(len(training_aug_padded[7]))
plt.imshow(training_aug_padded[7]['waferMap'][2999])

plt.tight_layout()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/41.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;

### 2-9. Store Augmentated Dataset  

```python
aug_train_data = pd.concat([training_aug_padded[0],training_aug_padded[1],training_aug_padded[2],
                        training_aug_padded[3],training_aug_padded[4],training_aug_padded[5],
                        training_aug_padded[6],training_aug_padded[7],training_aug_padded[8]])
del training_aug_padded

aug_train_data['failureType'] = list(map(lambda x: str(x), aug_train_data['failureType']))

parsed = json.loads(aug_train_data.to_json(orient = "split"))
del aug_train_data

with open('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_padded_resized_augmented.json','w') as f:
    json.dump(parsed, f)
del parsed
```

&nbsp;

## 3. Hand-crafted Dataset Evaluation   
### 3-1. Store Image Data into TrainSet   

```python
import os
import matplotlib.image
import ijson

json_path       = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train_padded_resized_not_augmented.json'
output_base     = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train'
classes         = ["Center","Donut","Edge-Loc","Edge-Ring","Loc","Near-full","Random","Scratch","none"]
counters        = {cls: 0 for cls in classes}

with open(json_path, 'r') as f:
    cols = []
    for prefix, event, value in ijson.parse(f):
        if prefix == 'columns.item' and event == 'string':
            cols.append(value)
        elif prefix == 'columns' and event == 'end_array':
            break

idx_map  = cols.index('waferMap')
idx_type = cols.index('failureType')

# Create Folder for each classes
for cls in classes:
    os.makedirs(os.path.join(output_base, cls), exist_ok=True)


with open(json_path, 'r') as f:
    for record in ijson.items(f, 'data.item'):
        ftype = record[idx_type]
        if ftype not in counters:
            continue
        cnt = counters[ftype]

        if cnt % 1000 == 0:
            print(f'/WM811K_train/{ftype}/{cnt}.png')
        # Store Image in File   
        wafer_map = record[idx_map]
        out_path  = os.path.join(output_base, ftype, f'{cnt}.png')
        matplotlib.image.imsave(out_path, wafer_map)
        counters[ftype] += 1
```

&nbsp;

### 3-2. Store Image Data into TestSet 

```python
import os
import matplotlib

!rm -rf WM811K_craft_test
!rm -rf WM811K_orig_test

!mkdir WM811K_craft_test
!mkdir WM811K_orig_test

path = "C://Users/isang/OneDrive/Desktop/WM/data/WM811K_craft_test/"
for i in range(len(classes) - 1):
  os.mkdir(path+classes[i])

path = "C://Users/isang/OneDrive/Desktop/WM/data/WM811K_orig_test/"
for i in range(len(classes) - 1):
  os.mkdir(path+classes[i])

for i in range(len(classes) - 1):
  for j in range(2900, 3000):
    if(j%10 == 0):
      print('/WM811K_craft_test/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_craft_test/'
                             +classes[i]
                             +'/'+str(j)+'.png', 
                             test_aug[i]['waferMap'].iloc[j])

for i in range(len(classes) - 1):
  for j in range(0, 100):
    if(j%10 == 0):
      print('/WM811K_orig_test/'+classes[i]+'/'+str(j)+'.png')
    matplotlib.image.imsave('C://Users/isang/OneDrive/Desktop/WM/data/WM811K_orig_test/'
                             +classes[i]+'/'
                             +str(j)+'.png', 
                             test_aug[i]['waferMap'].iloc[j])
```

&nbsp;

### 3-3. Data Loader   

```python
import os
import torch
from torch.utils.data import DataLoader, random_split, Dataset
from torchvision import datasets, transforms
import torch.nn.functional as F

! rm -rf C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train/none

train_path      = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_train'
craft_test_path = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_craft_test'
orig_test_path  = 'C://Users/isang/OneDrive/Desktop/WM/data/WM811K_orig_test'

transform = transforms.Compose([
    transforms.Resize((53, 52)),
    transforms.ToTensor(),
])

full_dataset = datasets.ImageFolder(train_path, transform=transform)
num_classes  = len(full_dataset.classes)

val_size   = int(0.2 * len(full_dataset))
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(
    full_dataset,
    [train_size, val_size],
    generator=torch.Generator().manual_seed(1)
)

batch_size = 32
train_loader = DataLoader(train_dataset, 
                          batch_size=batch_size, 
                          shuffle=True,  
                          num_workers=4, 
                          pin_memory=True)

val_loader   = DataLoader(val_dataset,   
                          batch_size=batch_size, 
                          shuffle=False, 
                          num_workers=4, 
                          pin_memory=True)

craft_test_dataset = datasets.ImageFolder(craft_test_path, transform=transform)
orig_test_dataset  = datasets.ImageFolder(orig_test_path,  transform=transform)

craft_test_loader  = DataLoader(craft_test_dataset, 
                                batch_size=batch_size, 
                                shuffle=False, 
                                num_workers=4, 
                                pin_memory=True)

orig_test_loader   = DataLoader(orig_test_dataset,  
                                batch_size=batch_size, 
                                shuffle=False, 
                                num_workers=4, 
                                pin_memory=True)
```

&nbsp;

### 3-4. Simple CNN Model  

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Simple CNN Model
class CraftNet(nn.Module):
    def __init__(self, num_classes=8):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3,  32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=3, padding=1),
            
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=3, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveMaxPool2d((1,1))
        )
        self.classifier = nn.Linear(128, num_classes)
        
    def forward(self, x):
        x = self.features(x)           # (128,1,1)
        x = x.view(x.size(0), -1)      # flatten to (128)
        return self.classifier(x)      # (num_classes)
```

&nbsp;

### 3-5. Train & Validation   

```python
device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model     = CraftNet(num_classes=8).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 3

for epoch in range(1, num_epochs+1):
    # — train —
    model.train()
    running_loss = 0.0
    running_correct = 0
    total = 0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        out = model(imgs)                       
        loss = criterion(out, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item() * imgs.size(0)
        preds = out.argmax(dim=1)
        running_correct += (preds == labels).sum().item()
        total += imgs.size(0)
    
    train_loss = running_loss / total
    train_acc  = running_correct / total
    
    # — validation —
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            out = model(imgs)
            val_loss += criterion(out, labels).item() * imgs.size(0)
            preds = out.argmax(dim=1)
            val_correct += (preds == labels).sum().item()
            val_total += imgs.size(0)
    val_loss /= val_total
    val_acc  = val_correct / val_total
    
    print(f"[Epoch {epoch}/{num_epochs}] "
          f"Train loss: {train_loss:.4f}, acc: {train_acc:.4f} | "
          f"Val loss:   {val_loss:.4f}, acc: {val_acc:.4f}")
```

**Train 결과**   
[Epoch 1/3] Train loss: 0.7359, acc: 0.7201 | Val loss:   0.3930, acc: 0.8513  
[Epoch 2/3] Train loss: 0.3317, acc: 0.8753 | Val loss:   0.2873, acc: 0.8919  
[Epoch 3/3] Train loss: 0.2698, acc: 0.8970 | Val loss:   0.2704, acc: 0.9043  
{: .notice}

&nbsp;

### 3-6. Evaluation for Orig & Craft Dataset   

```python
def evaluate(loader, name):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            out = model(imgs)
            preds = out.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += imgs.size(0)
    print(f"{name} accuracy: {correct/total:.4f}")

evaluate(craft_test_loader, "Craft Test")
evaluate(orig_test_loader,  "Orig Test")
```

**Test 결과**   
Craft Test accuracy: 0.8950    
Orig Test accuracy: 0.9137   
{: .notice}   

- 원본 이미지 데이터셋과 비교했을 때, 직접 제작한 데이터셋의 Accuracy는 약 2%의 차이를 보임    
- 큰 차이는 아니라고 생각되며 원본 데이터셋에 Craft 데이터셋을 추가하여 Oversampling을 완료    

&nbsp;


