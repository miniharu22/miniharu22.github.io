---
layout : single
title: "[CNN] CNN Model with Early Stopping"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---


## 0. Define CNN Model   

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

from torchsummary import summary

class Model1(nn.Module):
    def __init__(self, num_classes=9):
        super(Model1, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=3, padding=1),
        
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=3, padding=1),
        
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveMaxPool2d((1, 1)),
        )
        
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.features(x)  # → (B, 128, 1, 1)
        x = x.view(x.size(0), -1)
        return self.fc(x)

model1 = Model1(num_classes=9)

summary(model1, input_size=(3, 53, 52), device=device.type)
```

**Model Architecture**   
----------------------------------------------------------------  
        Layer (type)               Output Shape         Param #  
================================================================  
            Conv2d-1           [-1, 32, 53, 52]             896  
              ReLU-2           [-1, 32, 53, 52]               0  
         MaxPool2d-3           [-1, 32, 18, 18]               0  
            Conv2d-4           [-1, 64, 18, 18]          18,496  
              ReLU-5           [-1, 64, 18, 18]               0  
         MaxPool2d-6             [-1, 64, 6, 6]               0  
            Conv2d-7            [-1, 128, 6, 6]          73,856  
              ReLU-8            [-1, 128, 6, 6]               0  
 AdaptiveMaxPool2d-9            [-1, 128, 1, 1]               0  
           Linear-10                    [-1, 9]           1,161  
================================================================  
Total params: 94,409  
Trainable params: 94,409  
Non-trainable params: 0  
----------------------------------------------------------------   
Input size (MB): 0.03   
Forward/backward pass size (MB): 1.83   
Params size (MB): 0.36  
Estimated Total Size (MB): 2.22  
----------------------------------------------------------------  
{: .notice}

&nbsp;

## 1. Train & Validation with Early Stopping   

```python
import torch
import torch.nn as nn
import torch.optim as optim

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model1 = Model1(num_classes=9).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model1.parameters(), lr=0.001)

num_epochs = 50

# Early stopping parameters
patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0


history1 = {
    "loss": [],
    "accuracy": [],
    "val_loss": [],
    "val_accuracy": []
}

for epoch in range(1, num_epochs + 1):
    model1.train()
    running_loss = running_corrects = running_total = 0

    for imgs, labels in train_aug_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        logits = model1(imgs)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        batch_size = imgs.size(0)
        running_loss    += loss.item() * batch_size
        preds = logits.argmax(dim=1)
        running_corrects += (preds == labels).sum().item()
        running_total   += batch_size

    epoch_loss = running_loss / running_total
    epoch_acc  = running_corrects / running_total
    history1["loss"].append(epoch_loss)
    history1["accuracy"].append(epoch_acc)

    model1.eval()
    val_loss = val_corrects = val_total = 0

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            logits = model1(imgs)
            loss = criterion(logits, labels)

            bs = imgs.size(0)
            val_loss     += loss.item() * bs
            preds        = logits.argmax(dim=1)
            val_corrects += (preds == labels).sum().item()
            val_total   += bs

    epoch_val_loss = val_loss / val_total
    epoch_val_acc  = val_corrects / val_total
    history1["val_loss"].append(epoch_val_loss)
    history1["val_accuracy"].append(epoch_val_acc)

    print(f'Epoch {epoch}/{num_epochs}  '
          f'Train Loss: {epoch_loss:.4f}  Train Acc: {epoch_acc:.4f}  '
          f'Val Loss: {epoch_val_loss:.4f}  Val Acc: {epoch_val_acc:.4f}')

    # Early stopping check
    if epoch_val_loss < best_val_loss:
        # Update best validation loss and reset counter
        best_val_loss = epoch_val_loss
        epochs_no_improve = 0
        # Save the best model
        torch.save(model1.state_dict(), 'best_model.pth')
    # If no improvement, increment the counter
    else:
        # No improvement in validation loss
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print('Early stopping triggered')
            break
```

**Train & Val 결과**     
Epoch 1/50  Train Loss: 0.2311  Train Acc: 0.9234  Val Loss: 0.1030  Val Acc: 0.9694  
Epoch 2/50  Train Loss: 0.1402  Train Acc: 0.9512  Val Loss: 0.0988  Val Acc: 0.9682  
Epoch 3/50  Train Loss: 0.1142  Train Acc: 0.9601  Val Loss: 0.1042  Val Acc: 0.9674  
Epoch 4/50  Train Loss: 0.0995  Train Acc: 0.9650  Val Loss: 0.1144  Val Acc: 0.9626  
Epoch 5/50  Train Loss: 0.0891  Train Acc: 0.9682  Val Loss: 0.0896  Val Acc: 0.9714  
Epoch 6/50  Train Loss: 0.0804  Train Acc: 0.9713  Val Loss: 0.0869  Val Acc: 0.9744  
Epoch 7/50  Train Loss: 0.0745  Train Acc: 0.9736  Val Loss: 0.0787  Val Acc: 0.9760  
Epoch 8/50  Train Loss: 0.0683  Train Acc: 0.9752  Val Loss: 0.1179  Val Acc: 0.9654  
Epoch 9/50  Train Loss: 0.0632  Train Acc: 0.9772  Val Loss: 0.0888  Val Acc: 0.9742  
Epoch 10/50  Train Loss: 0.0591  Train Acc: 0.9786  Val Loss: 0.1046  Val Acc: 0.9704  
Epoch 11/50  Train Loss: 0.0550  Train Acc: 0.9800  Val Loss: 0.0995  Val Acc: 0.9719  
Epoch 12/50  Train Loss: 0.0527  Train Acc: 0.9810  Val Loss: 0.0959  Val Acc: 0.9739  
Early stopping triggered  
{: .notice}   

&nbsp;

## 2. Loss & Accuracy Analysis   

```python
plt.subplot(1,2,1)
plt.plot(history1['loss'])
plt.plot(history1['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.ylim([0, 1])
plt.legend(['training loss', 'validation loss'])

plt.subplot(1,2,2)
plt.plot(history1['accuracy'])
plt.plot(history1['val_accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.ylim([0.9, 1])
plt.legend(['training accuracy', 'validation accuracy'])
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/50.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

- Early Stopping을 적용하기 전과 비교하면 Validation loss의 증가 경향은 확실히 줄어든 것을 확인 가능   
- 또한 Validation Accuracy가 크게 감소하는 구간이 제거된 것을 알 수 있음   

&nbsp;

## 3. Correctly Classified Proportion   

```python
failure_types = ["Center", "Donut", "Edge-Loc", "Edge-Ring", "Loc", "Near-full", "Random", "Scratch", "none"]

val_dataset = torch.load("C://Users/isang/OneDrive/Desktop/WM/data/Val.pt")
val_tuple = from_DataBatch_to_list(val_dataset)

cm2 = confusion_matrix(model1, failure_types, val_tuple, threshold2)
p = classes_proportion_correctly_classified(cm2, failure_types)

print(p)

print(f"Mean Proportion : {np.mean(list(p.values()))}")
```

**출력 결과**   
{'Center': 0.9693430656934306,   
 'Donut': 0.8876404494382022,   
 'Edge-Loc': 0.9182156133828996,   
 'Edge-Ring': 0.9902975420439845,   
 'Loc': 0.8078291814946619,   
 'Near-full': 0.9583333333333334,   
 'Random': 0.927536231884058,   
 'Scratch': 0.8486486486486486,   
 'none': 0.9823511686969997}  
Mean Proportion : 0.9119826739910243  
{: .notice}  

- 평균 백분율은 1% 증가했지만, none class에 대해서도 1% 증가한 관계로 Failure Type Classification에 있어 그렇게 효과적인지는 판단이 애매함   

&nbsp;

