---
layout : single
title: "[Machine Learning] One-Vs-One multi-class SVM"
categories: 
  - Wafer Map Defect Detection
toc: true
toc_sticky: true
use_math: true
---

지금까지 추출한 Features를 종합하여 Dataset을 구축, One-Vs-One Multi-class SVM을 적용     


## 0. Dataset for Train & Test    

```python
df_all=df_withpattern_copy.copy()
a=[df_all.fea_reg[i] for i in range(df_all.shape[0])] 
b=[df_all.fea_cub_mean[i] for i in range(df_all.shape[0])] 
c=[df_all.fea_cub_std[i] for i in range(df_all.shape[0])] 
d=[df_all.fea_geom[i] for i in range(df_all.shape[0])]
fea_all = np.concatenate((np.array(a),np.array(b),np.array(c),np.array(d)),axis=1) 

label=[df_all.failureNum[i] for i in range(df_all.shape[0])]
label=np.array(label)



from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from collections import  Counter

X = fea_all
y = label

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)                      
print('Training target statistics: {}'.format(Counter(y_train)))
print('Testing target statistics: {}'.format(Counter(y_test)))

RANDOM_STATE =42
```

**출력 결과**   
Training target statistics: Counter({3: 7299, 2: 3860, 0: 3238, 4: 2677, 6: 905, 5: 640, 1: 404, 7: 116})   
Testing target statistics: Counter({3: 2381, 2: 1329, 0: 1056, 4: 916, 6: 288, 5: 226, 1: 151, 7: 33})   
{: .notice}


&nbsp;

## 1. One-Vs-One SVM Classification     

```python
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsOneClassifier

clf2 = OneVsOneClassifier(LinearSVC(random_state= RANDOM_STATE)).fit(X_train, y_train)

y_train_pred = clf2.predict(X_train)
y_test_pred = clf2.predict(X_test)

train_acc2 = np.sum(y_train == y_train_pred, axis=0, dtype='float') / X_train.shape[0]
test_acc2 = np.sum(y_test == y_test_pred, axis=0, dtype='float') / X_test.shape[0]

print('One-Vs-One Training acc: {}'.format(train_acc2*100)) 
print('One-Vs-One Testing acc: {}'.format(test_acc2*100))
```

**출력 결과**    
One-Vs-One Training acc: 82.75249490568996   
One-Vs-One Testing acc: 82.38244514106583   
{: .notice}

&nbsp;

## 2. Classification Report for Train set   

```python
print(classification_report(y_train, y_train_pred))
print('Acc Score :', accuracy_score(y_train, y_train_pred))
```

**출력 결과**      
              precision    recall  f1-score   support   
           0       0.91      0.94      0.92      3238   
           1       0.85      0.79      0.82       404  
           2       0.69      0.73      0.71      3860  
           3       0.92      0.94      0.93      7299  
           4       0.68      0.64      0.66      2677  
           5       0.91      0.87      0.89       640  
           6       0.70      0.52      0.59       905  
           7       0.97      0.99      0.98       116  
    accuracy                           0.83     19139   
   macro avg       0.83      0.80      0.81     19139   
weighted avg       0.83      0.83      0.83     19139  
Acc Score : 0.8275249490568996   
{: .notice}   

&nbsp;

## 3. Classification Report for Test set    

```python
print(classification_report(y_test, y_test_pred))
print('Acc Score :', accuracy_score(y_test, y_test_pred))
```

**출력 결과**   
              precision    recall  f1-score   support   
           0       0.91      0.93      0.92      1056  
           1       0.83      0.79      0.81       151  
           2       0.69      0.72      0.70      1329  
           3       0.91      0.93      0.92      2381  
           4       0.68      0.65      0.67       916  
           5       0.93      0.89      0.91       226  
           6       0.67      0.56      0.61       288  
           7       0.92      1.00      0.96        33  
    accuracy                           0.82      6380  
   macro avg       0.82      0.81      0.81      6380  
weighted avg       0.82      0.82      0.82      6380  
Acc Score : 0.8238244514106583   
{: .notice}   

&nbsp;

## 4. Confusion Matrix    

```python
import itertools
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')  

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_test_pred)
# np.set_printoptions(precision=2)

from matplotlib import gridspec
fig = plt.figure(figsize=(15, 8)) 
gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1]) 

## Plot non-normalized confusion matrix
plt.subplot(gs[0])
plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# Plot normalized confusion matrix
plt.subplot(gs[1])
plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

plt.tight_layout()
plt.show()
```

<div align="left">
  <strong>출력 결과</strong>
  <img src="/assets/images/WM/18.png" width="100%" height="100%" alt=""/>
  <p><em></em></p>
</div>
{: .notice}

&nbsp;



