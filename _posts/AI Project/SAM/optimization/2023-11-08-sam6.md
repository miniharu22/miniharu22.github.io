---
layout : single
title: "[Compression] Pruning"
categories: 
  - Post Training Quantization SAM
toc: true
toc_sticky: true
use_math: true
---

모델 경량화 기법 중 하나인 Pruning에 대해서 정리    

## 0. About Pruning    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/15.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Pruning**   
  - Pruning(가지치기)는 neural network를 경량화하고자 할 때 사용하는 방법 중 하나   
  - 위 자료를 보면 모든 node가 연결되어 있는 왼쪽과는 달리 pruning을 사용하면 synapse와 node가 제거되는 것을 알 수 있음    
    - 물론 무작정 없애는 것이 아니라 parameter가 0에 가깝거나 훈련 지표 등을 기준으로 판단하여 pruning을 수행함     
  - Drop out과는 착각하면 안되는 것이 Drop out 기법은 학습 도중에 수행한다면 pruning은 학습이 전부 끝난 model에서 필요없다고 판단된 부분을 줄이는 것   

&nbsp;

## 1. Structured vs Unstructured    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/16.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Structured Pruning**   
  - 대표적으로 **Channel Pruning**이 있으며, Conv network에서 상대적으로 필요 없는 channel을 제거하는 방법     
  - 이런식으로 structured pruning은 어떤 구조를 통째로 날려버린다는 특징이 있음    
  - 구조 자체를 날리는 것이므로 그만큼 별도로 matrix 연산을 거치지 않기 때문에 pytorch와 같은 딥러닝 프레임워크와 잘 호환되는 동시에 inference 속도를 개선 가능함   
  - 단점으로는 구조를 통째로 날리는 것이다보니 pruning하는 비율을 높게하기는 어려움   

&nbsp;

- **Unstructured Pruning**   
  - 구조와 상관없이 그냥 특정 기준을 세워서 가지치기하듯이 weight를 0으로 처리하는 방법    
  - 필요 없다고 판단된 weight를 0으로 만드는 것이므로 높은 비율로 pruning할 수 있기 때문에 95%까지 pruning하더라도 좋은 performance를 보임     
  - 단점으로는 pruning을 했으나 실제로는 0의 값을 가지므로 기존의 프레임워크를 사용하여 matrix 연산을 수행할 때, 일단은 계산을 하기는 해야하므로 실질적인 inference 속도를 개선하지는 못함     

&nbsp;

## 2. Lottery ticket hypothesis 

&nbsp;

<div align="center">
  <img src="/assets/images/sam/17.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Unstructured without Finetuning**   
  - 일반적으로 Unstrutured Pruning은 Pruning 이후에 Finetuning을 수행    
  - 반면, Finetuning을 하지 않더라도 다시 Initialize를 통해 훈련한 모델이 오히려 더 좋은 성능을 낸다는 연구도 제시됨    
    - 다만, 이는 기존과는 다르게 Intialize 수행 시 처음 train을 하기 전 paramter로 다시 돌려보내어 훈련하는 것을 가정함     
    - 해당 연구에서는 큰 틀의 Neural Network 안에는 여러 개의 sub-network가 있으며 그 안에는 성능이 원본 모델과 비슷하거나 더 좋은 모델이 있다고 가정함     
    - 이러한 sub-network를 **winning ticket**이라고 칭하는데 이 winning ticket은 훈련 후 pruning을 통해 구할 수 있으며 이를 다시 처음으로 돌려보내 학습을 수행하면 상당히 좋은 성능을 얻을 수 있다고 설명함    

&nbsp;

## 3. Gradient Flow in sparse neural networks   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/18.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Initialize의 중요성**   
  - 결국 Intialize를 기준으로 Pruning model의 성능의 편차가 심한 이유는 일반적인 Weight Initialization은 모든 노드가 잘 연결되어 있을 때를 가정하기 때문    
  - 위 자료를 보면 왼쪽과 같은 dense layer는 기존의 Initialize 방식이 잘 통하지만, 오른쪽과 같이 Pruning을 거쳤을 경우는 기존의 Initialize 방식을 따르면 안됨    
  - 따라서 각 output node 별로 연결된 node를 따로 계산하여 intialize하는 방식이 보통 제시됨     

&nbsp;

<div align="center">
  <img src="/assets/images/sam/19.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Comparison with Gradient Norm**   
  - 위 자료에서는 spare layer에서 기존과 같이 intialize를 하면 훈련 시작 시 gradient norm이 굉장히 작아 학습이 어렵다는 것을 보여줌    
  - Scratch가 기존의 intialize를 사용한 것이고 Scartch+가 앞서 제시된 intialize 방법으로 학습된 것인데 이들과는 동일하게 Lottery 방법 또한 gradient norm이 작은 것을 알 수 있음    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/20.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Lottery with Gradient Norm**   
  - Gradeint Norm이 작음에도 불구하고 Lottery 방법의 성능이 우수한 이유는 위 자료에서 확인 가능    
  - Lottery 방식은 Intialize를 수행했을 때, random initialize보다 pruning solution에 가까운 동시에, 학습 이후에는 훨씬 pruning에 가까워져 같은 basin에 들어가게 되기 때문    
    - 정리하면 기존의 unstructured pruning은 initialize 문제로 재학습했을 때 잘 작동하지 않는 반면, Lottery ticker 방법은 애초에 잘 배웠던 대로 재학습을 했기 때문에 좋은 성능을 보이는 것     

&nbsp;

