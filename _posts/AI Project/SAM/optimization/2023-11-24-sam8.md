---
layout : single
title: "[Compression] Post Trainging Quantization"
categories: 
  - Post Training Quantization SAM
toc: true
toc_sticky: true
use_math: true
---

Quantization 기법 중 하나인 Post Training Quantization(PTQ)에 대해 TensorRT 기반으로 정리    

## 0. About PTQ    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/27.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
  <br>
  <img src="/assets/images/sam/28.png" width="40%" height="40%" alt=""/>
</div>

&nbsp;

- **Symmetric Linear Quantization**   
  - 가장 간단한 양자화 기법으로 위 수식과 같이 Bias=0을 가정, FP32 값을 lineary INT8 영역으로 맵핑하는 방식    
  - max 값을 threshold를 통해 제한하여, Dequantize 시 손실되는 정보의 양을 줆임    
  - 해당 방식은 어떤 max/threshold/α를 선택하는지에 따라서 정보 손실의 정도가 달라짐

&nbsp;

- **Post Training Quantization**   
  - **Post Training Quantization(PTQ)**는 이미 학습이 완료된 네트워크를 통계적으로 분석하여, 가장 정보 손실이 적은 max/threshold/α를 선택하는 것을 목표로 함   
  - 기존 FP32 기반 네트워크의 모든 Quantizable layer에 대하여 출력값의 분포를 분석, 이후 다양한 threshold 값에 대하여 layer를 INT8 타입으로 변환한 후, INT8 layer의 출력값의 분포를 기존 FP32 layer의 출력값과 비교함    
  - 최종적으로 두 출력값의 분포의 차이가 가장 적은 threshold 값을 선택하여 최적의 α를 결정하고, 이를 통해 FP32 layer를 INT8 layer로 변환하는데 이러한 일련의 과정을 **Calibration**이라고 함    

&nbsp;

## 1. Kullback-Leibler Divergence   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/29.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **정보의 손실 측정**   
  - Calibration이 FP32 layer와 INT8 layer의 출력값 분포의 차이가 가작 적은 α를 찾는 것이 목표라면 이 분포의 차이는 **Kullback-Leibler Divergence**를 통해 측정함    
  - **Kullback-Leibler Divergence**   
    - relative entropy 혹은 information divergence 라고도 하며, 두 확률 분포의 차이를 측정하는 함수    
      - 특히 어떤 확률 분포에 대하여 그것을 근사하는 다른 확률 분포를 생설할 때, 어느 정도의 손실이 발생하는 지를 측정하는데에 사용함     
    - 위 자료를 보면 두 확률 분포 $$p, q$$에 대해 KL divergence를 적용하면 두 확률 분포가 보유한 정보의 차이가 얼마나 나느지 산출할 수 있음     
  - Quantization에서는 FP32 layer의 출력값 분포인 $$P$$와 INT8 layer의 출력값 분포인 $$Q$$의 차이를 측정, 정보 손실의 정도를 파악하는데 KL divergence를 사용함   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/30.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **KL divergence 측정 공식**   
  - 위 수식은 이산확률분포 $$P, Q$$에 대하여 KL divergence를 측정하는 공식   
  - Calibration 과정은 $$P$$에 대하여 가장 작은 KL divergence 값을 가지는 분포 $$Q$$와 그때의 α를 찾는 것     

&nbsp;

## 2. Post Train Quantization    

&nbsp;

- **Calibration 과정**   
  - 딥러닝 네트워크의 모든 layer들에 대해 Calibration dataset을 이용하여 FP32 layer의 출력 분포 계산    
  - 다양한 threshold를 이용하여 Calibration 진행 → INT8 layer 생성    
  - Calibration dataset을 이용하여 INT8 layer의 출력 분포 계산    
  - 가장 KL divergence가 적은 threshold 선택   
  - 최적의 α 계산 및 INT8 type으로 layer 변환    


&nbsp;

- **Calibration Dataset**   
  - Calibration 수행 시, layer의 출력 분포를 계산하기 위해서 사용되는 dataset    
  - 해당 dataset을 통해 INT8 layer의 정확도가 측정되기에, 이상적으로는 실제 사용되는 dataset의 특징을 모두 담고 있는게 좋음    
    - 물론 data가 다양하고 개수가 많으면 좋지만 dataset의 크기가 증가하면 calibration 과정에 소요되는 시간이 많으므로 trade-off를 고려하여 선택해야 함   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/32.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ResNet에서의 예시**   
  - 위 자료는 calibration 수행 후, ResNet-152의 res4b30 layer의 출력 분포의 변화를 보여줌     
  - 왼쪽은 기존 FP32 layer의 출력 분포이며, 오른쪽은 양자화된 INT8 layer의 출력 분포에 해당하는데 왼쪽 분포에서 흰색 선이 threshold를 나타냄     
    - Calibration 결과, threshold 이상의 값을 가지는 출력값들은 모두 threshold 값으로 saturation된 것을 확인 가능함    
  - 위 자료에서 확인할 수 있듯이, 양자화 이후에는 딥러닝 네트워크의 출력값이 어느 정도 변함, 물론 정확도가 크게 떨어지지는 않음    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/33.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **정확도 감소**   
  - 위 자료는 TensorRT Calibration을 통해 양자화된 네트워크들의 정확도를 정리한 표   
  - 이를 통해 확인할 수 있듯이 대부분 소수점대의 정확도 감소를 보여주는데 이는 dropuout 등을 통해 이미 모델이 최적화된 상태에서의 추론도 학습되었다고 추측됨   
    - 물론 각 parameter들이 어느 정도의 영향력을 가지는 지는 분석할 수 없기 때문에 edge case에 대해서 추론이 실패하지 않을 것이라고 보장할 수는 없음     

&nbsp;

<div align="center">
  <img src="/assets/images/sam/34.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **추론 속도 향상**   
  - 위 자료는 기존 네트워크 대비 TensorRT를 통해 양자화된 네트워크의 추론 속도 향상을 보여주는 표    
  - 모든 case에 대해 추론 속도 향상이 있으며, batch size가 커질수록 2~3배 이상의 추론 속도 향상을 보여줌    

&nbsp;


