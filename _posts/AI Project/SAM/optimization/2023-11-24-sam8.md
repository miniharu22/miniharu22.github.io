---
layout : single
title: "[Compression] Post Trainging Quantization"
categories: 
  - Post Training Quantization SAM
toc: true
toc_sticky: true
use_math: true
---

Quantization 기법 중 하나인 Post Training Quantization(PTQ)에 대해 TensorRT 기반으로 정리    

## 0. About PTQ    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/27.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
  <br>
  <img src="/assets/images/sam/28.png" width="20%" height="20%" alt=""/>
</div>

&nbsp;

- **Symmetric Linear Quantization**   
  - 가장 간단한 양자화 기법으로 위 수식과 같이 Bias=0을 가정, FP32 값을 lineary INT8 영역으로 맵핑하는 방식    
  - max 값을 threshold를 통해 제한하여, Dequantize 시 손실되는 정보의 양을 줆임    
  - 해당 방식은 어떤 max/threshold/α를 선택하는지에 따라서 정보 손실의 정도가 달라짐

&nbsp;

- **Post Training Quantization**   
  - **Post Training Quantization(PTQ)**는 이미 학습이 완료된 네트워크를 통계적으로 분석하여, 가장 정보 손실이 적은 max/threshold/α를 선택하는 것을 목표로 함   
  - 기존 FP32 기반 네트워크의 모든 Quantizable layer에 대하여 출력값의 분포를 분석, 이후 다양한 threshold 값에 대하여 layer를 INT8 타입으로 변환한 후, INT8 layer의 출력값의 분포를 기존 FP32 layer의 출력값과 비교함    
  - 최종적으로 두 출력값의 분포의 차이가 가장 적은 threshold 값을 선택하여 최적의 α를 결정하고, 이를 통해 FP32 layer를 INT8 layer로 변환하는데 이러한 일련의 과정을 **Calibration**이라고 함    

&nbsp;

## 1. Kullback-Leibler Divergence   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/29.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **정보의 손실 측정**   
  - Calibration이 FP32 layer와 INT8 layer의 출력값 분포의 차이가 가작 적은 α를 찾는 것이 목표라면 이 분포의 차이는 **Kullback-Leibler Divergence**를 통해 측정함    
  - **Kullback-Leibler Divergence**   
    - relative entropy 혹은 information divergence 라고도 하며, 두 확률 분포의 차이를 측정하는 함수    
      - 특히 어떤 확률 분포에 대하여 그것을 근사하는 다른 확률 분포를 생설할 때, 어느 정도의 손실이 발생하는 지를 측정하는데에 사용함     
    - 위 자료를 보면 두 확률 분포 $$p, q$$에 대해 KL divergence를 적용하면 두 확률 분포가 보유한 정보의 차이가 얼마나 나느지 산출할 수 있음     
  - Quantization에서는 FP32 layer의 출력값 분포인 $$P$$와 INT8 layer의 출력값 분포인 $$Q$$의 차이를 측정, 정보 손실의 정도를 파악하는데 KL divergence를 사용함   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/30.png" width="20%" height="20%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **KL divergence 측정 공식**   
  - 위 수식은 이산확률분포 $$P, Q$$에 대하여 KL divergence를 측정하는 공식   
  - Calibration 과정은 $$P$$에 대하여 가장 작은 KL divergence 값을 가지는 분포 $$Q$$와 그때의 α를 찾는 것     

&nbsp;

## 2. Post Train Quantization    


