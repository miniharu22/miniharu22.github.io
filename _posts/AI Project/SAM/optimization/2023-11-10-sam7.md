---
layout : single
title: "[Compression] Quantization"
categories: 
  - Post Training Quantization SAM
toc: true
toc_sticky: true
use_math: true
---

모델 경량화 기법 중 하나인 Quantization에 대해서 정리    

## 0. About Quantization    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/21.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Quantization**   
  - **Qauntization(양자화)**는 아날로그 신호를 이산 신호로 근사하는 과정을 의미하는데, 컴퓨터는 양자화를 통해 자연 내의 아날로그 신호를 0과 1의 데이터로 저장함   
  - Deep Learing에서 양자화는 Nueral Network의 parameter와 feature map을 **FP32** data type에서 **INT8** data type으로 근사하는 과정을 의미하며, 이를 통해 네트워크의 크기 압축은 물론 연산 속도의 향상을 꾀할 수 있음     
  - 일반적으로 딥러닝 네트워크는 FP32 type으로 구현을 하는데, 위 자료는 Data type 별로 표현 가능한 숫자의 범위 및 최소 숫자 크기를 보여줌    
    - FP32 type은 $$\text{-10}^{38} ~ \text{10}^{38}$$의 숫자를 표현하는 동시에, 높은 정확도로 소수를 표현할 수 있음    
    - 따라서 딥러닝 네트워크에서 사용되는 숫자를 충분히 표현할 수 있다는 장점을 지님   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/22.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **INT8 type의 장점**   
  - 위 자료는 FP32 type 대비 INT8 type을 사용할 경우 얻게 되는 에너지 및 Chip area 측면에서의 이점을 보여줌     
  - FP32 type은 32bit를 사용하여 데이터를 저장하지만, 이를 INT8 type으로 바꾼다면 사용 전력과 chip area도 줄어듦    
  - 또한 일반적으로 Integer 연산은 floating 연산에 비해 매우 빠르기 때문에 추론 속도 향상을 기대할 수 있음    
  - 따라서 딥러닝에서는 parameter/tensor/feature map 표현에 사용되는 FP32 type을 INT8 type으로 바꾸려는 Quantization 연구가 활발히 진행되고 있음    

&nbsp;

- **Inference Optimization**   
  - 딥러닝 네트워크에서 양자화는 학습 과정보다는 추론(Inferenece) 과정의 최적화를 위해 사용됨, 따라서 이미 학습이 완료된 모델에 대해서 양자화를 수행함       
  - 즉, 양자화는 후처리 작업에 속하며 parameter/tensor/feature map에 대해 calibration, fine-tuning을 수행함     

&nbsp;

- **양자화의 부작용**   
  - 딥러닝 네트워크를 낮은 정확도의 data type으로 변환하는 것은 필연적으로 네트워크 성능에 이슈를 야기함    
  - 데이터의 표현 가능한 범위 및 정확도가 바뀌기 때문에 신경망의 parameter 값이 조금씩은 변하기 때문에 결과적으로 추론 정확도 또한 변할 수 있다는 위험성도 따름   
  - 따라서, 현재의 딥러닝 네트워크 양자화는 추론 정확도의 하락을 최대한 방지하는 것을 목표로 연구되고 있음   

&nbsp;

## 1. Base Idea    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/23.png" width="20%" height="20%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **양자화의 기본 아이디어**   
  - 위 수식에서 $$F$$를 FP32 type의 수, $$Q$$를 INT8 type의 수라고 가정하면, 임의의 $$F, Q$$에 대하여 관계성을 나타내는 상수인 α, β를 찾을 수 있음   
    - 이때, α, β를 각각 **Scaling Factor**와 **Bias**라고 정의함   
  - 위 수식을 사용하면 FP32 type의 $$F$$를 INT8 type의 $$Q$$로 변환할 수 있음   
    - 예를 들어 α=0.01, β=0.0 이라 가정하면 $$F$$=0.1을 $$Q$$=10으로 변환가능함   
    - **TensorRT**의 경우, Quantization을 구현할 때, Bias는 0으로 설정하고 α만을 이용함    

&nbsp;

- **α sharing**   
  - 딥러닝 네트워크의 모든 parameter와 tensor에 대해 α를 따로 저장한다면, 오히려 양자화 이전보다 더 많은 메모리를 사용하게 됨     
  - 따라서 일반적으로 딥러닝 네트워크의 1개의 layer 혹은 tensor마다 하나의 α 값을 공유하는 것이 원칙임     

&nbsp;

<div align="center">
  <img src="/assets/images/sam/24.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
  <br>
  <img src="/assets/images/sam/25.png" width="20%" height="20%" alt=""/>
</div>

&nbsp;

- **양자화 과정**   
  - 위 자료는 딥러닝 네트워크의 1개 layer에 대한 parameter quantization을 보여줌   
  - 자료 속 X 표시들은 parameter를 의미하며, FP32(-max ~ max) 사이의 값들을 INT8(-127 ~ 127)까지의 수로 변환하는 것을 확인 가능함    
    - 이때, α는 위 수식으로 계산 가능함    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/26.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **정보 손실**   
  - 문제는 딥러닝 네트워크에서 paramter의 max 값은 매우 큰 수가 될 수 있는데 특히 해당 값이 outlier일 경우 더 큰 문제가 됨    
    - 예를 들어 parameter들이 `[1.0, 2.0, 3.0, 4.0, 5.0, 12700.0]`일 때, α = 100이므로, 따라서 양자화된 parameter들은 `[0, 0, 0, 0, 0, 127]`이 됨    
    - 해당 값들을 다시 Dequantize할 시, `[0.0, 0.0, 0.0, 0.0, 0.0, 12700.0]`가 되는데 결과적으로 많은 정보들이 손실되는 것을 확인 가능함    
  - 이를 방지하기 위해 TensorRT에서는 위 자료에서처럼 threshold를 두어 threshold보다 큰 값에 대해서는 127 혹은 -127로 saturate시킴   
    - 예를 들어 Threshold를 5로 설정할 경우, `[1.0, 2.0, 3.0, 4.0, 5.0, 12700.0]` → `[25, 50, 76, 101, 127, 127]`가 됨    
    - 이를 다시 Dequantize할 시, `[0.98, 1.97, 2.99, 3.98, 5.0, 5.0]`이 되는데 threshold가 없을 때에 비해 정보 손실이 덜함을 알 수 있음    

&nbsp;


