---
layout : single
title: "[Paper Review] Segment-Anything Model(SAM)"
categories: 
  - Post Training Quantization SAM
toc: true
toc_sticky: true
use_math: true
---

Meta AI에서 발표한 이미지 객체 분할 모델인 Segment-Anything Model에 대해 정리    

[논문 링크](https://ai.meta.com/research/publications/segment-anything/)  

- April 5 2023
- Meta AI
- [Alexander Kirillov](https://alexander-kirillov.github.io/)     

## 0. Introduction    

&nbsp;

- **Segment-Anything Model**    
  - 모든 분야에서 광범위하게 사용할 수 있는 Image segmentation model    
    - 이를 위해 광범위한 대용량 데이터셋(SA-1B)을 새롭게 만들고, 이를 학습시켜 powerful generalization을 적용     
  - 본 논문에서 강조하는 SAM의 요소에는 세 가지가 존재    
    - **Prmopt**   
      - Promptable segmentation    
      - Training sample들에 대해서 prompt들을 simulation한 후, 모델의 prediction과 GT를 비교   
    - **Model**    
      - prompt & image를 Input으로 받아 실시간으로 mask를 prediction    
    - **Data**    
      - diverse & large-scale data로 모델을 학습     
  - 학습된 모델의 성능을 평가하기 위해 추가적인 훈련 없이 pre-trained model을 새로운 task에 적용할 수 있는 Zero-shot transfer에 대해서도 실험을 진행    
  - **추론(Inference) 과정**   
    - prompt가 주어진 경우에는 prompt encoder를 통해서 해당 prompt의 embedding과 image embedding을 종합하여 최종 mask를 예측    
    - prompt가 주어지지 않은 경우에는 전체 이미지를 grid로 나누어 grid의 point들을 prompt로 제공하여 전체 이미지에 대해서 최종 mask를 예측   

&nbsp;

## 1. Segment Anything Task      
### 1-1. Prompt in NLP   

&nbsp;

- **Prompt**   
  - prompt는 NLP에서 처음 도입된 개념    
  - Chatgpt와 같은 Generative Model의 경우 사용자가 특정한 task를 요구함   
    - **예시 : classification**    
      - 예를 들어 "이 기사는 스포츠와 관련된 거야? 혹은 정치에 관련된 거야?" 라고 물어보면 이는 해당 기사의 카테고리를 '분류'하는 문제가 됨    
      - 이때, "(주어)가 (분류1)일까 혹은 (분류2)일까" 라는 prompt와 함께 학습된 경우, 위에 해당하는 질문이 분류에 해당하는 문제임을 알 수 있음    
    - **예시 : guidance**   
      - 때로는 질문에 대한 대답을 하기 위한 prompt도 존재하는데, 예를 들어 "한국의 현 대통령은 누구야?" 라고 질문한 경우, "한국의 현 대통령은 [정답]입니다" 라고 대답함   
      - 이처럼 prompt는 어떤 질문을 했을 때, 매끄러운 답변을 하는 guidance를 만들어주는 역할도 수행함     
  - 위 예시들처럼 task에 대한 guide를 제공하는 역할을 하는 prompt와 함께 학습하게 되면, 적은 양의 labeled data로도 좋은 성능을 보이며 generalization에 뛰어나기 때문에 다양한 downstream task에 대해 쉽게 학습할 수 있도록 도움을 줌    


&nbsp;

### 1-2. Segment Anything Task with Prompt Engineering   

&nbsp;

<div align="center">
  <img src="/assets/images/sam/1.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Task**   
  - SAM에서의 prompt는 다양한 것이 될 수 있음   
  - 위 자료의 (a)에서 볼 수 있듯이 foreground와 background를 구분하는 기준이 되거나 러프한 bbox(bounding box)나 mask가 될 수도 있음 (타겟에 대한 글이어도 상관없음)
  - 이처럼 mask를 생성하고자 하는 대상에 대한 정보를 담은 어떤 것이든 prompt가 될 수 있으며 이를 통해 다양한 downstream에 대해 zero-shot transfer가 가능함    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/2.png" width="70%" height="70%" alt=""/>
  <p><em>Ambiguous prompt</em></p>
</div>

&nbsp;


- **Pre-training**   
  - prompt와 image를 함께 받아와서 대용량 데이터셋으로 valid한 mask를 생성해내는 pre-training을 수행함     
  - 이때, 'ambiguous'한 prompt에 대해서도 학습했다고 설명되어 있는데, 이렇게 ambiguous prompt와 함께 학습시켰을 때 어떠한 prompt가 오더라도 valid한 mask를 생성해낼 수 있음    
  - **ambiguous prompt**   
    - 위 자료를 예시로 들면 셔츠를 입고 있는 사람에서 셔츠에 prompt point가 찍혔다고 가정    
    - 그러면 해당 point가 사람 자체를 뜻하는 것인지, 아니면 셔츠만 타겟하는 것인지 알 수가 없기 때문에 이를 'ambiguous'라고 칭함     

&nbsp;

- **Zero-shot transfer**   
  - pre-training을 통해 어떠한 prompt가 오더라도 valid한 mask를 생성할 수 있는 모델이 있기 때문에, zero-shot transfer가 가능함    
  - 예를 들어 고양이에 bounding box가 쳐져 있다면, 이를 통해 별다른 학습 없이 고양이를 segmentation하는 것이 가능함    

&nbsp;

- **related tasks**   
  - segmentation에는 여러 종류가 있는데 예를 들면, edge detection, interactive segmentation, foreground segmentation 등이 있음    
  - SAM은 prompt engineering을 통해 다양한 task에 적용할 수 있는 동시에 현존하는 다양한 object detector와 결합하여 사용할 수 있다는 점에서 확장 가능성이 크다고 평가됨    

&nbsp;

- **Data Engine**   
  - prompting은 powerful한 방법이지만, 이미지의 prompt에 대해서 mask GT를 만드는 것도 human cost가 듦, 따라서 fully human annotaed dataset을 사용하기 보다는 data engine을 통해 GT를 생성함    
  - 처음에는 사람이 생성한 GT로 학습을 수행(assited-manual), 그 후에는 사람이 생성한 것과 generate된 mask를 함께 학습(semi-automatic), 최종적으로는 foreground에 대해서 point가 주어진 prompt로 생성된 mask를 통해서만 학습을 수행    


&nbsp;

## 2. Segment Anything Model    

&nbsp;

<div align="center">
  <img src="/assets/images/sam/3.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **모델 구성**   
  - **Image Encoder**   
    - scalability하면서도 powerful한 MAE pretrained ViT 모델 사용    
  - **Prompt Encoder**   
    - **Sparse**    
      - points, boxes, text가 존재    
      - point와 box의 경우 positional encoding을 합쳐서 임베딩을 구축    
      - text의 경우 CLIP의 text encoder를 활용    
    - **Dense**   
      - mask가 존재    
      - 해당 경우에는 일반적인 Conv 연산을 통해서 임베딩을 생성 후, 이미지와 element-wise로 합침    
  - **Mask Decoder**   
    - Image embedding과 prompt embedding을 받아서 mask를 생성    
    - Transformer Decoder 구조를 따랐으며 head에 dynamic mask prediction이 가능하도록 수정을 가함    
    - 전체 임베딩을 업데이트하기 위해 prompt-to-image와 image-to-prompt, 양방향으로 self-attention과 cross-attention을 수행     

&nbsp;

- **모델 설정**   
  - **Resolving Ambiguity**    
    - ambiguous prompt가 주어졌을 때의 성능을 향상시키기 위해서 1개의 prompt에 대해 3개의 prediction mask를 생성(whole, part, subpart)    
    - backpropagation 수행시에는 mask들 중에서 minimum loass를 가지고 있는 것에 대해서만 시행    
  - **Efficiency**   
    - 미리 계산된 image embedding에 대해서 prompt encoder와 mask decoder가 돌아가는 것은 50ms 이내에 가능    
    - 이는 real-time interactive prompting이 가능한 수준    
  - **Losses & Training**   
    - Focal Loss와 Dice Loss를 Linear하게 합쳐서 사용    

&nbsp;

## 3. Segment Anything Data Engine   

&nbsp;  

- **SA-1B의 데이터 엔진**   
  - 본 논문에서는 SA-1B의 데이터 엔진을 **"model-in-the-loop"** 라는 표현으로 설명, 이는 총 세 가지 단계로 진행되는데, 단계를 거칠수록 automatic해짐   
  - **Assisted-manual stage**     
    - segmentation tool을 사용하여 몇 번의 클릭으로 breif한 segmentation mask를 얻은 다음, 사람이 세부 수정하는 식으로 진행됨     
  - **Semi-autimatic stage**   
    - 사람이 직접 라벨링한 mask GT와 모델이 예측한 mask를 함께 섞어서 쓰는 단계   
    - 정확도를 유지시키면서도 human cost를 줄여가는 단계    
  - **Fully automatic stage**   
    - 모든 것을 모델이 예측한 mask로 학습하는 단계로 이를 통해 '1B'라는 거대한 이미지 데이터셋에 대해 높은 정확도를 가진 segmentation mask를 사람의 노동력 없이 만들어 낼 수 있었음    
    - 이때, prompt의 경우 32x32의 grid point들을 주었기 때문에, 모델이 예측한 mask를 쓰는 것이 ambiguity-aware한 model을 만드는데에도 도움이 되었다고 함    
      - 이는 grid point가 subpart에 있으면 subpart한 mask를 내놓을 것이고, whole part의 경우, 전체를 내놓기 때문     
    - Post-processing 단계를 거쳐서 조금 더 mask를 정교화시켰는데, confidence score가 높고 probability map을 고려하여 stable한 mask를 선택했으며 NMS까지 거쳐서 최종 mask를 획득    

&nbsp;


