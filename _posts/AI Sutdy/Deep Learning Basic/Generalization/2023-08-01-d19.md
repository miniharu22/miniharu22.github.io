---
layout : single
title: "[Generalization] Batch Normalization"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

배치 정규화(Batch Normalization) 기법에 대해서 정리   

## 0. Batch Normalization  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/40.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Normalization(정규화)**  
  - 모든 데이터들의 Scale을 동등하게 변환하여 각 feature 값들이 동등한 중요도를 가지도록 하는 작업    
  - 위 자료에서 확인할 수 있듯이, 정규화 이전에는 데이터들이 0~20에 몰려서 분포되어 있는 반면, 정규화 이후에는 0~1 사이의 값들로 일정한 분포를 가지게 됨   

&nbsp;

<div align="center">
  <img src="/assets/images/DL/39.png" width="30%" height="30%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Batch Normalization(배치 정규화)**  
  - 활성화 함수에 data를 입력할 때, data가 특정 범위에 편향되어 있을 경우, 예를 들면 음수 값에 치중된다면 Vanishing gradient가 발생할 수 있음  
  - 인공신경망을 학습할 때, 각 layer에 들어가는 input data를 정규화하여 Vanishing gradient를 방지하게끔 처리하는 것이 **Batch Normalization**   
    - Batch Dimension에서 Layer의 각 activation을 정규화하여 Input data의 편향성을 줄이는 것을 의미    
    - 일반적으로 각 층의 활성화 함수를 통과하기 전에 Batch Normalization이 수행됨   

&nbsp;

## 1. Batch Normalization Implementation  
### 1-1. Load MNIST Dataset   

```python 
from torch import nn, optim, cuda
from torch.utils import data
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import time

# Training settings
batch_size = 64
device = 'cuda' if cuda.is_available() else 'cpu'   # Use GPU if available, otherwise CPU

# MNIST Dataset
train_dataset = datasets.MNIST(root='./mnist_data/',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='./mnist_data/',
                              train=False,
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
train_loader = data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)

sample_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)
```

&nbsp;

### 1-2. Neural Network Model with Batch Normalization  

```python
class Net(nn.Module):
    def __init__(self, use_batchnorm=False):
        super(Net, self).__init__()
        self.use_batchnorm = use_batchnorm

        self.hidden_layers = nn.ModuleList()
        self.batchnorms = nn.ModuleList()
        
        layer_sizes = [784] + [64] * 10

        for i in range(len(layer_sizes) - 1):
            self.hidden_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))
            self.batchnorms.append(
                nn.BatchNorm1d(layer_sizes[i + 1]) if use_batchnorm else nn.Identity()
            )

        self.output = nn.Linear(64, 10)

    def forward(self, x, return_features=False):
        x = x.view(-1, 784)
        features = []

        for layer, bn in zip(self.hidden_layers, self.batchnorms):
            x = F.relu(bn(layer(x)))
            if return_features:
                features.append(x)

        out = self.output(x)
        if return_features:
            return out, features
        return out
```

&nbsp;

### 1-3. Train function  

```python
# Training function
def train(model, optimizer, criterion, epoch):
    model.train()   # Set the model to training mode
    for batch_idx, (data, target) in enumerate(train_loader):
        # Move data and target to the device
        data, target = data.to(device), target.to(device)   
        optimizer.zero_grad()               # Zero the gradients
        output = model(data)                # Forward pass
        loss = criterion(output, target)    # Compute loss
        loss.backward()                     # Backward pass
        optimizer.step()                    # Update weights
        if batch_idx % 10 == 0:
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, 
                batch_idx * len(data), 
                len(train_loader.dataset),
                100. * batch_idx / len(train_loader), 
                loss.item()))
```

&nbsp;

### 1-4. Test funtion  

```python
# Testing function
def test(model, criterion):
    model.eval()    # Set model to evaluation mode
    test_loss = 0   # Initialize test loss
    correct = 0     # Initialize correct predictions counter
    for data, target in test_loader:
        # Move data and target to the device
        data, target = data.to(device), target.to(device)
        output = model(data)                            # Forward pass
        test_loss += criterion(output, target).item()   # Accumulate loss

        # Get the index of the max log-probability
        pred = output.data.max(1, keepdim=True)[1]      
        # Count correct predictions
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    # Average the test loss
    test_loss /= len(test_loader.dataset)
    print(f'===========================\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({100. * correct / len(test_loader.dataset):.0f}%)')
```

&nbsp;

### 1-5. Feature Visulization function  

```python
# Feature Visulization function  
def visualize_features(model, image_tensor, label, title_prefix):
    model.eval()
    with torch.no_grad():
        _, features = model(image_tensor.unsqueeze(0).to(device), return_features=True)

    plt.figure(figsize=(15, 3))
    for i, feat in enumerate(features):
        vec = feat[0].cpu().numpy()
        side = int(len(vec) ** 0.5)
        img = vec[:side * side].reshape(side, side)
        plt.subplot(1, len(features), i + 1)
        plt.imshow(img, cmap='gray')
        plt.title(f"{title_prefix}\nLayer {i+1}")
        plt.axis('off')
    plt.suptitle(f"Input Label: {label}", fontsize=14)
    plt.tight_layout()
    plt.show()
```

&nbsp;

### 1-6. Train & Test for Comparison w/ & w/o B.N

```python
# Main Execution
if __name__ == '__main__':
    # Initialize models, optimizers, and loss function
    model_bn = Net(use_batchnorm=True).to(device)
    model_no_bn = Net(use_batchnorm=False).to(device)

    criterion = nn.CrossEntropyLoss()

    optimizer_bn = torch.optim.Adam(model_bn.parameters(), 
                                    lr=0.001, 
                                    betas=(0.9, 0.999),
                                    eps=1e-08,
                                    weight_decay=0,
                                    amsgrad=False)
    
    optimizer_no_bn = torch.optim.Adam(model_no_bn.parameters(), 
                                        lr=0.001, 
                                        betas=(0.9, 0.999),
                                        eps=1e-08,
                                        weight_decay=0,
                                        amsgrad=False)

    for epoch in range(1, 10):  
        print(f"\n--- Epoch {epoch} [With BatchNorm] ---")
        train(model_bn, optimizer_bn, criterion, epoch)
        test(model_bn, criterion)

        print(f"\n--- Epoch {epoch} [No BatchNorm] ---")
        train(model_no_bn, optimizer_no_bn, criterion, epoch)
        test(model_no_bn, criterion)
```

**학습 및 추론 결과**   
--- Epoch 1 [With BatchNorm] ---  
Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.345695  
Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.212653  
Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 1.861607  
...   
Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.035466  
===========================  
Test set: Average loss: 0.0012, Accuracy: 9783/10000 (98%)  
--- Epoch 1 [No BatchNorm] ---  
Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.286790  
Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.285217  
Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.306041  
...  
Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.159812  
===========================  
Test set: Average loss: 0.0025, Accuracy: 9598/10000 (96%)  
{: .notice}  

&nbsp;

### 1-7. Feature Check   

```python
sample_img, sample_label = next(iter(sample_loader))
print(f"\nSelected Test Image Label: {sample_label.item()}")
visualize_features(model_bn, sample_img[0], sample_label.item(), "W/ BatchNorm")
visualize_features(model_no_bn, sample_img[0], sample_label.item(), "w/o BatchNorm")
```

<div align="left">
    <strong>Selected Test Image Label: 6</strong>
    <br>
  <img src="/assets/images/DL/43.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

&nbsp;

