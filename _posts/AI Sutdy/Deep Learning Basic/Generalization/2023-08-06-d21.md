---
layout : single
title: "[Generalization] Drop Out"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Drop Out 기법에 대해서 정리  

## 0. Drop Out  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/46.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Drop Out**     
  - FC Layer에서 0부터 1 사이의 확률로 뉴런을 제거하는 기법   
  - 어떤 특정한 Feature만을 과도하게 집중하여 학습함으로써 발생하는 과적합(Overfitting)을 방지하기 위해 사용됨   
    - 결과적으로 특정 Feature에 편향된 학습 결과를 얻지 않도록 모델의 학습 경로를 임의로 유도하는 과정   
    - 제거되는 뉴런의 종류와 개수는 Drop out rate에 따라 랜덤하게 결정되며, 이 또한 hyperparameter에 속함   

&nbsp;

## 1. Drop out Implementation   
### 1-1. Load CIFAR-10 Dataset & Data Augmentation   

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 64
num_epochs = 10
k_folds = 5

transform_plain = transforms.Compose([
    transforms.ToTensor(),
])

transform_augmented = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),
    transforms.ToTensor(),
])

train_dataset_aug   = datasets.CIFAR10(root='./cifar10', train=True, transform=transform_augmented, download=True)
test_dataset        = datasets.CIFAR10(root='./cifar10', train=False, transform=transform_plain)

test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

&nbsp;

### 1-2. CNN Class with Drop out   

```python
class CNN(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer

        self.fc1 = nn.Linear(8 * 8 * 16, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.pool(x)

        x = self.conv2(x)
        x = F.relu(x)
        x = self.pool(x)

        x = x.view(-1, 8 * 8 * 16)

        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)  # 🔸 Dropout after fc1

        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout(x)  # 🔸 Dropout after fc2

        x = self.fc3(x)
        x = F.log_softmax(x, dim=1)

        return x
```

&nbsp;

### 1-3. Train function   

```python
# Training function
def train(model, loader, optimizer, criterion, epoch):
    model.train()   # Set the model to training mode

    train_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(loader):
        # Move data and target to the device
        data, target = data.to(device), target.to(device)   
        optimizer.zero_grad()               # Zero the gradients
        output = model(data)                # Forward pass
        loss = criterion(output, target)    # Compute loss
        loss.backward()                     # Backward pass
        optimizer.step()                    # Update weights

        train_loss += loss.item() * data.size(0)
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
        total += target.size(0)

        if batch_idx % 10 == 0:
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, 
                batch_idx * len(data), 
                len(loader.dataset),
                100. * batch_idx / len(loader), 
                loss.item()))
        
    avg_loss = train_loss / total
    accuracy = correct / total
    print(f'Train set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({100. * accuracy:.2f}%)')
```

&nbsp;

### 1-4. Validation function  

```python
# Validation function 
def validate(model, loader, criterion):
    model.eval()
    val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item() * data.size(0)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += data.size(0)
            
    val_loss /= total
    val_acc = correct / total
    print(f'Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{total} '
          f'({100. * val_acc:.0f}%)')
```

&nbsp;

### 1-5. Train & Validation & Test  

```python
if __name__ == '__main__':
    # K-Fold Cross-Validation
    # Create K-Fold cross-validator with shuffling 
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42) 

    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset_aug)):
        print(f'\n======================')
        print(f'Fold {fold + 1}/{k_folds}')
        print(f'======================')

        # Subset samplers
        train_subset = Subset(train_dataset_aug, train_idx)
        val_subset = Subset(train_dataset_aug, val_idx)

        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

        # Model, Optimizer, Loss
        model = CNNNet().to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(),
                                        lr=1e-4, 
                                        betas=(0.9, 0.999),
                                        eps=1e-08,
                                        weight_decay=0,
                                        amsgrad=False)
        

        for epoch in range(num_epochs):
            print(f'\n--- Epoch {epoch + 1} ---')
            train(model, train_loader, optimizer, criterion, epoch)
            validate(model, val_loader, criterion)

    # Optional final test
    print("\n\n Final Evaluation on Test Set")
    model.eval()
    test_correct = 0
    test_total = 0
    test_loss = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item() * data.size(0)
            pred = output.argmax(dim=1)
            test_correct += pred.eq(target).sum().item()
            test_total += data.size(0)

    test_loss /= test_total
    test_acc = test_correct / test_total
    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_correct}/{test_total} ({100. * test_acc:.2f}%)')
```

**학습 및 추론 결과**     
Fold 1/5  
======================  
--- Epoch 1 ---  
Train Epoch: 0 | Batch Status: 0/40000 (0%) | Loss: 2.377940  
Train Epoch: 0 | Batch Status: 640/40000 (2%) | Loss: 2.372963  
Train Epoch: 0 | Batch Status: 1280/40000 (3%) | Loss: 2.325651  
Train Epoch: 0 | Batch Status: 1920/40000 (5%) | Loss: 2.334165  
...  
Train Epoch: 9 | Batch Status: 39680/40000 (99%) | Loss: 0.988446  
Train set: Average loss: 1.3171, Accuracy: 21124/40000 (52.81%)  
Validation set: Average loss: 1.2445, Accuracy: 5457/10000 (55%)  
Final Evaluation on Test Set   
Test set: Average loss: 1.1681, Accuracy: 5829/10000 (58.29%)   
{: .notice}   

- ResNet을 사용했을 때 약 80% 이상의 accuracy를 보였던 반면, CNN model은 60%에도 못미침  
  - 결국 CNN은 층이 깊어질수록 기울기 소실이 심화되지만, ResNet은 Skip connection을 통해 이를 해소할 수 있기 때문    
  - 그리고 ResNet의 Non-linearity가 더 복잡하기 때문에 표현력 측면에서도 우수할 수 밖에 없음    

&nbsp;

