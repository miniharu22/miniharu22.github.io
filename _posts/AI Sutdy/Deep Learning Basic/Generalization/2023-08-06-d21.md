---
layout : single
title: "[Generalization] Drop Out"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Drop Out ê¸°ë²•ì— ëŒ€í•´ì„œ ì •ë¦¬  

## 0. Drop Out  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/46.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Drop Out**     
  - FC Layerì—ì„œ 0ë¶€í„° 1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë‰´ëŸ°ì„ ì œê±°í•˜ëŠ” ê¸°ë²•   
  - ì–´ë–¤ íŠ¹ì •í•œ Featureë§Œì„ ê³¼ë„í•˜ê²Œ ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë°œìƒí•˜ëŠ” ê³¼ì í•©(Overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë¨   
    - ê²°ê³¼ì ìœ¼ë¡œ íŠ¹ì • Featureì— í¸í–¥ëœ í•™ìŠµ ê²°ê³¼ë¥¼ ì–»ì§€ ì•Šë„ë¡ ëª¨ë¸ì˜ í•™ìŠµ ê²½ë¡œë¥¼ ì„ì˜ë¡œ ìœ ë„í•˜ëŠ” ê³¼ì •   
    - ì œê±°ë˜ëŠ” ë‰´ëŸ°ì˜ ì¢…ë¥˜ì™€ ê°œìˆ˜ëŠ” Drop out rateì— ë”°ë¼ ëœë¤í•˜ê²Œ ê²°ì •ë˜ë©°, ì´ ë˜í•œ hyperparameterì— ì†í•¨   

&nbsp;

## 1. Drop out Implementation   
### 1-1. Load CIFAR-10 Dataset & Data Augmentation   

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 64
num_epochs = 10
k_folds = 5

transform_plain = transforms.Compose([
    transforms.ToTensor(),
])

transform_augmented = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),
    transforms.ToTensor(),
])

train_dataset_aug   = datasets.CIFAR10(root='./cifar10', train=True, transform=transform_augmented, download=True)
test_dataset        = datasets.CIFAR10(root='./cifar10', train=False, transform=transform_plain)

test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

&nbsp;

### 1-2. CNN Class with Drop out   

```python
class CNN(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer

        self.fc1 = nn.Linear(8 * 8 * 16, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.pool(x)

        x = self.conv2(x)
        x = F.relu(x)
        x = self.pool(x)

        x = x.view(-1, 8 * 8 * 16)

        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)  # ğŸ”¸ Dropout after fc1

        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout(x)  # ğŸ”¸ Dropout after fc2

        x = self.fc3(x)
        x = F.log_softmax(x, dim=1)

        return x
```

&nbsp;

### 1-3. Train function   

```python
# Training function
def train(model, loader, optimizer, criterion, epoch):
    model.train()   # Set the model to training mode

    train_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(loader):
        # Move data and target to the device
        data, target = data.to(device), target.to(device)   
        optimizer.zero_grad()               # Zero the gradients
        output = model(data)                # Forward pass
        loss = criterion(output, target)    # Compute loss
        loss.backward()                     # Backward pass
        optimizer.step()                    # Update weights

        train_loss += loss.item() * data.size(0)
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
        total += target.size(0)

        if batch_idx % 10 == 0:
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, 
                batch_idx * len(data), 
                len(loader.dataset),
                100. * batch_idx / len(loader), 
                loss.item()))
        
    avg_loss = train_loss / total
    accuracy = correct / total
    print(f'Train set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({100. * accuracy:.2f}%)')
```

&nbsp;

### 1-4. Validation function  

```python
# Validation function 
def validate(model, loader, criterion):
    model.eval()
    val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item() * data.size(0)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += data.size(0)
            
    val_loss /= total
    val_acc = correct / total
    print(f'Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{total} '
          f'({100. * val_acc:.0f}%)')
```

&nbsp;

### 1-5. Train & Validation & Test  

```python
if __name__ == '__main__':
    # K-Fold Cross-Validation
    # Create K-Fold cross-validator with shuffling 
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42) 

    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset_aug)):
        print(f'\n======================')
        print(f'Fold {fold + 1}/{k_folds}')
        print(f'======================')

        # Subset samplers
        train_subset = Subset(train_dataset_aug, train_idx)
        val_subset = Subset(train_dataset_aug, val_idx)

        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

        # Model, Optimizer, Loss
        model = CNNNet().to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(),
                                        lr=1e-4, 
                                        betas=(0.9, 0.999),
                                        eps=1e-08,
                                        weight_decay=0,
                                        amsgrad=False)
        

        for epoch in range(num_epochs):
            print(f'\n--- Epoch {epoch + 1} ---')
            train(model, train_loader, optimizer, criterion, epoch)
            validate(model, val_loader, criterion)

    # Optional final test
    print("\n\n Final Evaluation on Test Set")
    model.eval()
    test_correct = 0
    test_total = 0
    test_loss = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item() * data.size(0)
            pred = output.argmax(dim=1)
            test_correct += pred.eq(target).sum().item()
            test_total += data.size(0)

    test_loss /= test_total
    test_acc = test_correct / test_total
    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_correct}/{test_total} ({100. * test_acc:.2f}%)')
```

**í•™ìŠµ ë° ì¶”ë¡  ê²°ê³¼**     
Fold 1/5  
======================  
--- Epoch 1 ---  
Train Epoch: 0 | Batch Status: 0/40000 (0%) | Loss: 2.377940  
Train Epoch: 0 | Batch Status: 640/40000 (2%) | Loss: 2.372963  
Train Epoch: 0 | Batch Status: 1280/40000 (3%) | Loss: 2.325651  
Train Epoch: 0 | Batch Status: 1920/40000 (5%) | Loss: 2.334165  
...  
Train Epoch: 9 | Batch Status: 39680/40000 (99%) | Loss: 0.988446  
Train set: Average loss: 1.3171, Accuracy: 21124/40000 (52.81%)  
Validation set: Average loss: 1.2445, Accuracy: 5457/10000 (55%)  
Final Evaluation on Test Set   
Test set: Average loss: 1.1681, Accuracy: 5829/10000 (58.29%)   
{: .notice}   

- ResNetì„ ì‚¬ìš©í–ˆì„ ë•Œ ì•½ 80% ì´ìƒì˜ accuracyë¥¼ ë³´ì˜€ë˜ ë°˜ë©´, CNN modelì€ 60%ì—ë„ ëª»ë¯¸ì¹¨  
  - ê²°êµ­ CNNì€ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸° ì†Œì‹¤ì´ ì‹¬í™”ë˜ì§€ë§Œ, ResNetì€ Skip connectionì„ í†µí•´ ì´ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸    
  - ê·¸ë¦¬ê³  ResNetì˜ Non-linearityê°€ ë” ë³µì¡í•˜ê¸° ë•Œë¬¸ì— í‘œí˜„ë ¥ ì¸¡ë©´ì—ì„œë„ ìš°ìˆ˜í•  ìˆ˜ ë°–ì— ì—†ìŒ    

&nbsp;

