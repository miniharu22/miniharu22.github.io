---
layout : single
title: "[Generalization] Vanishing Gradient & ReLU"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

기울기 소실(Vanishing Gradient)와 ReLU Activation에 대해서 정리   

## 0. Vanishing Gradient

&nbsp;

<div align="center">
  <img src="/assets/images/DL/36.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Vanishing Gradient(기울기 소실)**  
  - Layer가 많은 신경망에서 역전파를 수행할 때, 입력층에 가까울수록 Gradient가 소실되어 weight가 업데이트되지 못하는 이슈   
  - 주로 Sigmoid Activation을 사용했을 때 발생   
    - Sigmoid는 최대 기울기가 1/4이기 때문에, 편미분이 반복될 수록 gradient 값이 감소하기 때문   

&nbsp;

## 1. ReLU  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/37.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ReLU(Rectified Linear Unit)**  
  - 기울기 소실을 방지하기 위해 Sigmoid의 대체제로 제시된 활성화 함수  
  - 도출된 값이 0 이상일 경우, 해당 값을 그대로 사용하기 때문에 Vanishing gradient를 방지할 수 있음  
  - **Linear function을 사용하지 않는 이유**   
    - $$y = x$$ 처럼 Linear function을 쓰면 이 또한 기울기 소실을 방지할 수는 있음  
    - 다만, 이러면 [Non-linear](https://miniharu22.github.io/deep%20learning%20basic/d14/)를 모델에 반영할 수 없기 때문에 모델의 표현력이 떨어짐    

&nbsp;

<div align="center">
  <img src="/assets/images/DL/38.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ReLU의 한계**  
  - 입력 값이 0 이하일 경우, 출력이 0이 된다는 단점에 의해 오히려 gradient가 0으로 계산되어 weight가 업데이트되지 않을 수도 있음  
  - 이를 해결하기 위해 $$x /leq 0$$에서도 Linear function을 취하는 ReLU의 변형함수들이 제시됨   

&nbsp;

- **ReLU의 종류**  
  - **Learky ReLU**  
    - ReLU에 의해 gradient가 0으로 계산되는 것 자체를 방지하기 위해 $$x \leq 0 $$에서도 값을 취해주는 Activation  
  - **Parametric ReLU**  
    - $$x \leq 0$$에서의 Linear factor 자체를 weight와 bias처럼 모델이 학습하게끔 하는 Activation    

&nbsp;

## 2. ReLU vs Sigmoid   
### 2-1. Load MNIST Dataset  

```python
from torch import nn, optim, cuda
from torch.utils import data
from torchvision import datasets, transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
import time

# Training settings
batch_size = 64
device = 'cuda' if cuda.is_available() else 'cpu'   # Use GPU if available, otherwise CPU

# MNIST Dataset
train_dataset = datasets.MNIST(root='./mnist_data/',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='./mnist_data/',
                              train=False,
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
train_loader = data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
```

&nbsp;

### 2-2. Neural Network Model with Different Activation  

```python
# Define the neural network model
class MLP(nn.Module):
    def __init__(self, activation_fn):
        super(MLP, self).__init__()

        self.activation_fn = activation_fn

        # Define fully connected layers
        self.l1 = nn.Linear(784, 520)  # Input layer (28x28=784) to hidden layer
        self.l2 = nn.Linear(520, 320)  # Hidden layer
        self.l3 = nn.Linear(320, 240)  # Hidden layer
        self.l4 = nn.Linear(240, 120)  # Hidden layer
        self.l5 = nn.Linear(120, 10)   # Output layer (10 classes)

    def forward(self, x):
        x = x.view(-1, 784)
        x = self.activation_fn(self.l1(x))
        x = self.activation_fn(self.l2(x))
        x = self.activation_fn(self.l3(x))
        x = self.activation_fn(self.l4(x))
        return self.l5(x)
```

&nbsp;

### 2-3. Train Function  

```python
# Save the model with different activation functions
model_relu = MLP(F.relu).to(device)
model_sig = MLP(F.sigmoid).to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()

optimizer_relu = torch.optim.Adam(model_relu.parameters(),  
                             lr=0.001, 
                             betas=(0.9, 0.999),
                             eps=1e-08,
                             weight_decay=0,
                             amsgrad=False)

optimizer_sig = torch.optim.Adam(model_sig.parameters(),   
                             lr=0.001, 
                             betas=(0.9, 0.999),
                             eps=1e-08,
                             weight_decay=0,
                             amsgrad=False)


# Training function
def train(epoch, model, optimizer, activation):
    model.train()   # Set the model to training mode
    for batch_idx, (data, target) in enumerate(train_loader):
        # Move data and target to the device
        data, target = data.to(device), target.to(device)   
        optimizer.zero_grad()               # Zero the gradients
        output = model(data)                # Forward pass
        loss = criterion(output, target)    # Compute loss
        loss.backward()                     # Backward pass
        optimizer.step()                    # Update weights
        
        if batch_idx % 10 == 0:
            print('[{}] Train Epoch: {} | Batch: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                activation,
                epoch, 
                batch_idx * len(data), 
                len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item()))
```

&nbsp;

### 2-4. Test Function  
  
```python
# Testing function
def test(model, activation):
    model.eval()    # Set model to evaluation mode
    test_loss = 0   # Initialize test loss
    correct = 0     # Initialize correct predictions counter

    with torch.no_grad():  # Disable gradient calculation
        for data, target in test_loader:
            # Move data and target to the device
            data, target = data.to(device), target.to(device)
            output = model(data)                            # Forward pass
            test_loss += criterion(output, target).item()   # Accumulate loss

            # Get the index of the max log-probability
            pred = output.data.max(1, keepdim=True)[1]      
            # Count correct predictions
            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()

    # Average the test loss
    test_loss /= len(test_loader.dataset)

    print(f'\n[{activation}] Test set: Average loss: {test_loss:.4f}, '
          f'Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({100. * correct / len(test_loader.dataset):.2f}%)\n')
```

&nbsp;

### 2-5. Main Function  

```python
# Main function
if __name__ == '__main__':
    since = time.time()

    for epoch in range(1, 10):
        print(f"\n====== Epoch {epoch} ======")

        # ReLU Model Test & Evaluation
        epoch_start = time.time()
        train(epoch, model_relu, optimizer_relu, 'ReLU')
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'[ReLU] Training time: {m:.0f}m {s:.0f}s')

        test(model_relu, 'ReLU')
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'[ReLU] Testing time: {m:.0f}m {s:.0f}s')

    for epoch in range(1, 10):
        print(f"\n====== Epoch {epoch} ======")

        # Sigmoid Model Test & Evaluation
        epoch_start = time.time()
        train(epoch, model_sig, optimizer_sig, 'Sigmoid')
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'[Sigmoid] Training time: {m:.0f}m {s:.0f}s')

        test(model_sig, 'Sigmoid')
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'[Sigmoid] Testing time: {m:.0f}m {s:.0f}s')

    m, s = divmod(time.time() - since, 60)
    print(f'\nTotal Time: {m:.0f}m {s:.0f}s')
    print(f'Models were trained on {device}!')
```

**학습 및 추론 결과**  
====== Epoch 1 ======
[ReLU] Train Epoch: 1 | Batch: 0/60000 (0%) | Loss: 2.309318  
[ReLU] Train Epoch: 1 | Batch: 640/60000 (1%) | Loss: 2.030342  
[ReLU] Train Epoch: 1 | Batch: 1280/60000 (2%) | Loss: 1.341074  
[ReLU] Train Epoch: 1 | Batch: 1920/60000 (3%) | Loss: 1.065203  
...  
[ReLU] Train Epoch: 9 | Batch: 59520/60000 (99%) | Loss: 0.001054   
[ReLU] Training time: 0m 13s   
[ReLU] Test set: Average loss: 0.0011, Accuracy: 9838/10000 (98.38%)  
[ReLU] Testing time: 0m 14s  
====== Epoch 1 ======  
[Sigmoid] Train Epoch: 1 | Batch: 0/60000 (0%) | Loss: 2.488977  
[Sigmoid] Train Epoch: 1 | Batch: 640/60000 (1%) | Loss: 2.298376  
[Sigmoid] Train Epoch: 1 | Batch: 1280/60000 (2%) | Loss: 2.312181  
[Sigmoid] Train Epoch: 1 | Batch: 1920/60000 (3%) | Loss: 2.317724  
...  
[Sigmoid] Train Epoch: 9 | Batch: 59520/60000 (99%) | Loss: 0.006473  
[Sigmoid] Training time: 0m 10s  
[Sigmoid] Test set: Average loss: 0.0013, Accuracy: 9789/10000 (97.89%)  
[Sigmoid] Testing time: 0m 10s  
Total Time: 4m 9s  
{: .notice}  

&nbsp;


