---
layout : single
title: "[ANN] Non-linear Activation"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

인공 신경망에서 Non-linear 활성화 함수가 사용되는 이유에 대해서 정리  

## 0. Non-linear Activation function

&nbsp;

<div align="center">
  <img src="/assets/images/DL/27.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Non-linearity**  
  - **Q.1)** 만약, MLP의 Layer를 Linear-activation만으로 사용해도 깊게 만든다면 복잡한 함수를 사용할 수 있는가?  
    - Linear activation만 쓴다면 아무리 깊게 만들어도 1개의 FC Layer를 쓴 것과 유사한 표현력을 가지게 되기 때문에 DNN을 구현하기 힘듦   
    - 따라서 Non-linear activation이 필요한 것   
  - **Q.2)** Linear → Non-linear → Linear → Non-linear 순으로 작성한다면 표현력이 높아지는가?  
    - Linear activation만 쓰는 것보다는 표현력이 높아지겠지만, 결국 Non-Linear → Non-Linear 두 개의 Layer를 쓰는 것과 동일한 표현력을 얻음    

&nbsp;

- **Linear function을 쓰는 경우**  
  - Linear Regression에서 출력층(Output layer)를 작성해야 할 때  
    - -$$\infty$$ ~ $$\infty$$의 범위를 얻기 위함   
  - 정보 소실을 방지하기 위해   
    - node 수가 적은 은닉층에서 ReLU를 사용할 경우, input $$\leq$$ 0일 때 정보 소실이 발생할 수 있기 때문에 Linear function을 activation으로 사용함   