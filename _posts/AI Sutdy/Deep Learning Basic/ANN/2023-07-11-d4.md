---
layout : single
title: "[ANN] Linear Regression"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

선형 회귀(Linear Regression)에 대해서 정리  

**Dataset :** [test.csv](https://www.kaggle.com/datasets/andonians/random-linear-regression?resource=download)   

## 0. About Linear Regression

&nbsp;

<div align="left">
  <img src="/assets/images/DL/8.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Linear Regression**  
  - 입력과 출력 간의 관계(함수)를 선형으로 놓고 알아내는 것  
    - 처음 보는 입력에 대해서도 적절한 출력을 얻기 위함  
  - Deep Learning에서는 $$y=ax+b$$ 라는 식에서 a(wight)와 b(bias)를 구하는 과정을 의미  
    - 이를 알아내기 위한 지표가 바로 **Loss**  

&nbsp;

- **Loss & MSE**
  - loss의 최소화는 원하는 출력을 얻을 수 있도록 최적의 a와 b를 구하는 것을 의미  
  - **Mean Square Error(MSE)**  
    - loss를 결정 짓는 지표로 예측값과 실제값의 차이를 error라고 정의할 때, 각 error의 제곱들의 합을 평균으로 계산한 값을 의미    
    - 제곱을 하는 이유는 error를 합했을 때, 부호의 차이로 인한 miss를 없애기 위함  
    - outliner가 존재할 경우, 제곱이 아닌 절댓값으로 산출하는 **MAE(Mean Absolute Error)** 방식이 유리할 수도 있음  

&nbsp;

## 1. Linear Regression Implementation
### 1-1. Load Dataset  

```python
import torch
import numpy as np
import torch.nn as nn
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable

# Load Dataset
class MyDataset(Dataset):

    # Initialize data, dowload, etc
    def __init__(self, file_path):
        # Download & Read data  
        data = pd.read_csv(file_path)

        # Convert Numpu array to Pytorch Tensor  
        # Reshape into 2D Tensor
        self.y_data = torch.from_numpy(data['y'].values).unsqueeze(dim=1).float()
        self.x_data = torch.from_numpy(data['x'].values).unsqueeze(dim=1).float()

    def __len__(self):
        # return the data length
        return len(self.x_data)
    
    def __getitem__(self, idx):
        # return one item on the index
        x = self.x_data[idx]
        y = self.y_data[idx]
        return x, y
```

&nbsp;

### 1-2. Linear Regression Model

```python
# Simple Linear Regression Model
class LinearRegressinModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1) # input_dim & output_dim = 1

    def forward(self, x):
        return self.linear(x)
```

&nbsp;

### 1-3. Model Training

```python
# Training
def main():
    # Hyperparameters
    batch_size = 1
    lr = 0.00001
    epochs = 500
    num_workers = 0
    dataset = MyDataset("C:/Users/isang/OneDrive/Desktop/DL/test.csv")
    train_loader = DataLoader(dataset= dataset,
                              batch_size= batch_size,
                              shuffle= True,
                              num_workers= num_workers
                              )
    model = LinearRegressinModel()
    optimizer = torch.optim.SGD(model.parameters(), lr = lr)
    criterion = nn.MSELoss()

    for epoch in range(epochs):

        for i, data in enumerate(train_loader):

            # get data
            x, y = data
            x, y = Variable(x), Variable(y)

            # forward pass
            y_pred = model(x)

            # compute loss  
            loss = criterion(y_pred, y)

            if epoch % 10 == 0:
                print(f"Epoch : {epoch}/{epochs} loss : {loss.item():.6f}")
            
            # Zero graidents, perform a backward pass, update the weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

if __name__ == '__main__':
    main()
```

**학습 결과**  
Epoch : 0/500 loss : 2.466708  
Epoch : 0/500 loss : 1.342183  
Epoch : 0/500 loss : 42.742828  
Epoch : 0/500 loss : 11.051977  
Epoch : 0/500 loss : 11.152569  
Epoch : 0/500 loss : 26.613474  
Epoch : 0/500 loss : 36.954918  
Epoch : 0/500 loss : 6.626553  
Epoch : 0/500 loss : 20.493076  
Epoch : 0/500 loss : 2.776015  
...  
Epoch : 490/500 loss : 25.067795  
Epoch : 490/500 loss : 2.360079  
Epoch : 490/500 loss : 3.934187  
Epoch : 490/500 loss : 0.627566  
Epoch : 490/500 loss : 3.128083  
Epoch : 490/500 loss : 1.564580  
Epoch : 490/500 loss : 54.358444  
Epoch : 490/500 loss : 0.053648  
Epoch : 490/500 loss : 11.908297  
Epoch : 490/500 loss : 12.062190  
{: .notice}

- **결과 분석**  
  - epoch을 500까지 설정했음에도 불구하고 loss가 일정하게 감소하지 않은 것을 확인 가능  
  - 다만, loss를 줄이는 것이 목적이 아닌 Linear Regression model을 구현하는 것이 목적이므로 추가적인 parameter tuning은 진행하지 않음  


&nbsp;