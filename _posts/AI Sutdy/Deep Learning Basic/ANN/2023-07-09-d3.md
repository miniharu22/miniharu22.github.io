---
layout : single
title: "[ANN] Aritificial Neural Network"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

인공 신경망(Aritificial Neural Network)에 대해서 정리  

## 0. About Aritificial Neural Network

&nbsp;

<div align="left">
  <img src="/assets/images/DL/4.jpg" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Artificial Neural**
  - Node(Unit)과 Edge(Connection)으로 이루어져 있음  
  - Weight(가중치)를 곱하고 Bias를 더한 후, Activation Function(활성화 함수)에 입력   
    - 주어진 Input에 대해 원하는 Output이 나올 수 있도록 weight와 bias를 잘 설정해야 함  
    - AI가 학습한다는 것은 적절한 weight와 bias를 스스로 선택하는 것을 의미  

&nbsp;

<div align="left">
  <img src="/assets/images/DL/7.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Artificial Neural Network**  
  - weight-sum-activation을 반복, 즉 인공 신경으로 구성된 Network를 의미  
  - Layer의 종류  
    - Input Layer  
    - Hidden Layer  
    - Output Layer  
  - **FC(Fully-Connected) Layer** : node끼리 전부 연결된 layer   
  - **MLP(Multi-Layer Perceptron)**  
    - 모든 Layer가 FC Layer인 신경망을 의미  
    - Perceptron은 unit step function을 활성화 함수로 사용하는 인공 신경을 의미  
    - 다만, MLP는 활성화 함수와 상관없이 FC Layer로 이루어진 인공 신경망을 의미   
    - Hidden Layer가 한 층 이상은 존재해야 함  

&nbsp;

## 1. ANN Implementation  
### 1-1. Define Neural Network class 

```python
import numpy
import scipy.special

# Nueral Network class definition  
class NeuralNetwork :
    
    # Initialize the neural network  
    def __init__():
        pass

    # Train the neural network  
    def train():
        pass

    # Query to neural network
    def query():
        pass
```

- 신경망은 적어도 다음 세 가지 기능을 가져야 함  
  - **초기화(init)** : input, hidden, output node의 수 설정  
  - **학습(train)** : 학습 데이터들을 통해 학습하고 이에 따라 weight를 업데이트  
  - **질의(query)** : input을 받아 연산한 후 output node에서 답을 전달  

&nbsp;

### 1-2. Initialize Neural Network 

```python
import numpy
import scipy.special

# Nueral Network class definition  
class NeuralNetwork :
    
    # Initialize the neural network  
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        
        # set number of nodes in each Input, Hidden, Output layer  
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes

        # Weight matrix  
        # wih(input_hidden), who(hidden_output)
        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))

        self.lr = learningrate # Learning rate

        # Action Function : Sigmoid 
        self.activation_function = lambda x: scipy.special.expit(x)

        pass

    # Train the neural network  
    def train():
        pass

    # Query to neural network
    def query():
        pass
```

- 신경망의 형태와 크기를 정의하기 위해 각 layer의 node 수를 설정  
  - 신경망 내에서 직접 정의하기보다는 매개변수를 사용하는 것이 범용적임  

- **weight 설정**  
  - 가중치는 input-hidden & hidden-output, 이렇게 두개의 행렬로 나눠서 표현  
  - 가중치가 -0.5~0.5 범위에서 임의의 값으로 초기화되도록 `numpy.random`을 사용  
  - 또한 더 정교하게 초기화하기 위해 0을 중심으로 $$\frac{1}{\sqrt{Number of Input node}}$$의 표준편차를 가지는 정규분포를 사용   
    - `numpy.random.normal(정규분포의 중심, 표준편차, numpy 행렬)`   

&nbsp;

### 1-3. Query to Neural Network 

```python
import numpy
import scipy.special

# Nueral Network class definition  
class NeuralNetwork :
    
    # Initialize the neural network  
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        
        # set number of nodes in each Input, Hidden, Output layer  
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes

        # Weight matrix  
        # wih(input_hidden), who(hidden_output)
        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))

        self.lr = learningrate # Learning rate

        # Action Function : Sigmoid 
        self.activation_function = lambda x: scipy.special.expit(x)

        pass
    
    # Train the neural network  
    def train():
        pass

    # Query to neural network
    def query(self, inputs_list):
        # Convert Input list to 2-Dimension Matrix
        inputs = numpy.array(inputs_list, ndim=2).T

        # Calculate the signal entering the hidden layer
        # x_hidden = w_hidden x I  (x : weight matrix, I : Input value Matrix)
        hidden_inputs = numpy.dot(self.wih, inputs)

        # Calculate the signal leaving the hidden layer
        hidden_outputs = self.activation_function(hidden_inputs)

        # Calculate the signal entering the Final Output layer
        final_inputs = numpy.dot(self.wih, hidden_outputs)

        # Calculate the signal leaving the Final Output layer
        final_outputs = self.activation_function(final_inputs)

        return final_outputs
```

- **Query**  
  - 신경망으로 들어오는 input을 받아 output을 반환하는 작업을 input/hidden/output layer를 거쳐 수행     
  - hidden layer로 들어노는 신호를 계산하기 전에 Input_list를 `numpy.array()`를 사용하여 2차원의 행렬로 변환  
  - 각 노드의 weight를 연산하고 활성화 함수를 적용하기 위해서 행렬을 이용   


&nbsp;

### 1-4. Train Neural Network 

 ```python
import numpy
import scipy.special

# Nueral Network class definition  
class NeuralNetwork :
    
    # Initialize the neural network  
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        
        # set number of nodes in each Input, Hidden, Output layer  
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes

        # Weight matrix  
        # wih(input_hidden), who(hidden_output)
        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))

        self.lr = learningrate # Learning rate

        # Action Function : Sigmoid 
        self.activation_function = lambda x: scipy.special.expit(x)

        pass
    
    # Train the neural network  
    def train(self, inputs_list, targets_list):
        # Convert Input list to 2-Dimension Matrix   
        inputs = numpy.array(inputs_list, ndmin=2).T
        targets = numpy.array(targets_list, ndmin=2).T

        # Calculate the signal entering the hidden layer
        hidden_inputs = numpy.dot(self.wih, inputs)
        # Calculate the signal leaving the hidden layer
        hidden_outputs = self.activation_function(hidden_inputs)

        # Calculate the signal entering the Final Output layer
        final_inputs = numpy.dot(self.wih, hidden_outputs)
        # Calculate the signal leaving the Final Output layer
        final_outputs = self.activation_function(final_inputs)

        # output layer error = actual value - calculated value
        output_errors = targets - final_outputs
        # hidden layer error is calculated by recombining the errors of the layers
            # seperated by the weights
        hidden_errors = numpy.dot(self.who.T, output_errors)

        # Update weights between hidden and output layers
        self.who += self.lr*numpy.dot((output_errors*final_outputs*(1.0-final_outputs)), numpy.transpose(hidden_outputs))
        # Update weights between input and hidden layers
        self.wih += self.lr*numpy.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)), numpy.transpose(inputs))

        pass

    # Query to neural network
    def query(self, inputs_list):
        # Convert Input list to 2-Dimension Matrix
        inputs = numpy.array(inputs_list, ndim=2).T

        # Calculate the signal entering the hidden layer
        # x_hidden = w_hidden x I  (x : weight matrix, I : Input value Matrix)
        hidden_inputs = numpy.dot(self.wih, inputs)

        # Calculate the signal leaving the hidden layer
        hidden_outputs = self.activation_function(hidden_inputs)

        # Calculate the signal entering the Final Output layer
        final_inputs = numpy.dot(self.wih, hidden_outputs)

        # Calculate the signal leaving the Final Output layer
        final_outputs = self.activation_function(final_inputs)

        return final_outputs
```

- **학습의 단계**  
  - 주어진 학습 데이터에 대해 결과 값을 계산해내는 단계  
    - `query()`에서 했던 작업과 다르지 않으며 출력 값을 계산해냄    
  - 계산한 결과 값을 실제의 값과 비교하고 이 차이를 이용해 가중치를 업데이트하는 단계  
    - 가중치를 어떻게 업데이트해야 하는지 알려주기 위해 error를 역전파  

- **가중치 업데이트 과정**    
  - `train()`은 `query()`와 달리 계산 값과 실제 값 간의 오차에 기반하기에 `target_list`라는 매개변수를 입력받아야 함  
  - output layer error는 (실제 값 행렬 - 계산 값 행렬)로 산출됨  
  - hidden layer error는 역전파를 통해 전달되며, 이를 위해서는 연결 노드의 가중치에 따라 error를 나눠서 전달하고 각각의 hidden layer의 노드에 대해 이를 재조합  
  - 가중치 업데이트 수식 : $$\Delta W_{jk} = \alpha * E_k * \text{sigmoid}(O_k) * (\text{1-sigmoid}(O_k)) * O_{j_T}$$  
    - 현재(j)와 다음(k) 표시를 활용해 노드 j와 다음 계층의 노드 k 간의 가중치 업데이트를 표현  
    - `self.lr` : Learning rate($$\alpha$$)
    - `output_errors` : $$E_k$$ 
    - `final_outputs*(1.0-final_outputs)` : $$\text{sigmoid}(O_k)$$  
    - `1.0-final_outputs` : $$\text{1-sigmoid}(O_k)$$  
    - `numpy.transpose(hidden_outputs)` : $$O_{j_T}$$

 &nbsp;