---
layout : single
title: "[Optimizer] Stochastic Gradient Descent"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

확률적 경사 하강법(SGD, Stochastic Gradient Descent)에 대해서 정리  

## 0. About Stochastic Gradient Descent

&nbsp;

<div align="left">
  <img src="/assets/images/DL/13.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **확률적 경사 하강법(Stochastic Gradient Descent)**  
  - 기존 경사 하강법은 전체 train data를 고려하여 error를 계산시키기 때문에 연산량이 많다는 점이 단점  
  - 확률적 경사 하강법은 전체 train data에서 **1개씩 sampling**한 후, 해당 data들을 input으로 넣어 error를 연산시키고, 이 작업을 반복적으로 수행하는 방식을 의미    
  - 즉, 기존 경사 하강법 대비 **빠르게 Loss를 수렴시킬 수 있다**는 장점이 있음  
  - 다만, SGD에서도 Local minimum으로 수렴될 수 있는 것은 여전함(물론 GD보다 확률은 적음)   

&nbsp;

- **SGD에서 Gradient의 방향이 계속 바뀌는 이유**  
  - 경사 하강법을 설명하면서 모든 Loss function에서 Gradient는 항상 최적점을 향해 수렴한다고 설명했지만, 위 자료를 보면 SGD에서는 Gradient의 방향이 계속 바뀌는 것을 알 수 있음  
  - 이는 sampling된 Data들이 계속 변함으로써 Gradient의 수렴방향이 계속 바뀌기 때문    

&nbsp;

## 1. SGD Implementation
### 1-1. Optimizer class definition

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet

# Optimizer classes  
# Gradient Descent class
class GD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# Stochastic Gradient Descent class
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

- **GD vs SGD**  
  - GD와 SGD의 코드는 사실 동일, 두 기법의 차이는 Train data에 대한 샘플링 유무로 나뉘기 때문  

&nbsp;

### 1-2. Test setting  

```python
# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 1

# Experiment setup
optimizers = {
    'SGD' : SGD(lr = 0.01),
    'GD' : GD(lr = 0.01)
}

networks = {}
train_loss = {}

# Initialize networks & loss tracking  
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784,
        hidden_size_list=[100, 100, 100, 100],
        output_size=10
    )
    train_loss[key] = []
```

&nbsp;

### 1-3. Training

```python
# Training 
# Training loop

for epoch in range(2000):
    # Sample a mini-batch for SGD
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in optimizers.keys():
        # Use full dataset for GD, mini-batch for SGD
        if key == 'GD':
            x_in = x_train
            t_in = t_train
        else:
            x_in = x_batch
            t_in = t_batch

        # Compute gradients and update parameters
        grads = networks[key].gradient(x_in, t_in)
        optimizers[key].update(networks[key].params, grads)

        # Compute and record loss
        loss = networks[key].loss(x_in, t_in)
        train_loss[key].append(loss)

    # Print loss every 100 iterations
    if epoch % 100 == 0:
        print("========== epoch: " + str(epoch) + " ==========")
        for key in optimizers.keys():
            x_input = x_train if key == 'GD' else x_batch
            t_input = t_train if key == 'GD' else t_batch
            loss = networks[key].loss(x_input, t_input)
            print(f"{key}: {loss}")
```

**학습 결과**    
========== epoch: 0 ==========  
SGD: 0.45266236887822703  
GD: 2.3487775372305943  
========== epoch: 100 ==========  
SGD: 0.609032316873912  
GD: 1.615244045600307  
========== epoch: 200 ==========  
SGD: 0.39671596048427166  
GD: 0.8443309802697119  
========== epoch: 300 ==========  
SGD: 0.39703247841763695  
GD: 0.5690785173986177  
========== epoch: 400 ==========  
SGD: 0.04526158142602381  
GD: 0.4579273407541614  
========== epoch: 500 ==========  
SGD: 0.12401093014165664  
GD: 0.39999565135980003  
========== epoch: 600 ==========  
SGD: 0.17398543164863065  
GD: 0.36391879109970016  
========== epoch: 700 ==========  
SGD: 0.004902041479539472  
GD: 0.33834627509666054  
========== epoch: 800 ==========  
...  
GD: 0.2235118565214074  
========== epoch: 1900 ==========  
SGD: 0.0727139583809098  
GD: 0.21798220044172587  
{: .notice}  

&nbsp;

### 1-4. Visulization  

```python
# === 3. Plot training loss ===

markers = {"SGD": "o", "GD": "x"}
x = np.arange(2000)

for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.ylim(0, 1)
plt.legend()
plt.title("Comparison of GD and SGD on MNIST")
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/18.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- **결과 분석**  
  - 일반 경사 하강법과 비교하면 SGD에서는 loss가 큰 진폭을 보이는 등, 불안정한 것을 알 수 있음  
    - 이는 결국 하나의 data만을 샘플링하기 때문  
  - 다만 loss의 수렴 속도를 비교하면 비록 안정성은 떨어질지언정, SGD가 훨씬 빠른 것을 확인 가능  

&nbsp;  


