---
layout : single
title: "[Optimizer] Mini-batch SGD & Hyperparameter"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Mini-batch SGD 기법과 hyperparameter에 대해서 정리  

## 0. About Mini-batch SGD

- **Mini-batch SGD**  
  - SGD는 1개의 Data를 샘플링한다는 점에서 속도는 빠를지언정 정확도가 떨어진다는 점이 지적됨  
  - 따라서 여러 개의 data를 샘플링하여 적절한 속도와 정확성을 챙기는 **mini-batch SGD** 방식이 도입됨, 여기서 샘플링하는 data의 갯수를 **Batch size**라고 정의  
  - 물론 이러면 일반적인 SGD 대비 연산 속도가 저하되겠지만, **GPU**는 병렬 연산을 가능케 하므로 여러 데이터를 샘플링하더라도 속도는 빠름     

&nbsp;

<div align="left">
  <img src="/assets/images/DL/14.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Batch size 크기**
  - 다만 GPU만 믿고 batch size를 무작정 키우면 안되고 최대 8k 정도까지만 키워야 함  
  - 이는 8K 이상의 batch size는 error를 키우기 때문  
  - 따라서, batch size를 키우면서도 small batch size와 유사한 성능을 얻기 위해 다음과 같은 방법이 제시됨  
    - **Learning rate 증가**    
      - Batch size를 증가시킨 비율만큼 **Learing rate**($$\alpha$$) 또한 동등하게 증가시키기      
      - 이는 SGD에서 가중치의 업데이트는 gradient의 평균을 고려하기 때문에 learning rate는 $$\frac{\alpha}{\text{batch size}}$$와 같이 batch size가 일종의 factor로 처리됨   
      - 따라서 batch size를 증가시키면서 동일한 성능을 얻기 위해서는 learning rate 또한 동일한 비율로 설정하는 것이 옳기 때문  
    - **Learning rate warm-up**  
      - Weight Initialization 이후, 초기값만으로는 모델이 불안정할 수 밖에 없는데 여기서 learing rate가 너무 높다면 불안정성이 심화됨  
      - 따라서, 학습 초기에는 learning rate를 서서히 증가시키고, 이후에는 원하는대로 스케줄링하는 기법이 바로 **Learing rate warm-up**   

&nbsp;

## 1. Hyperparameter  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/21.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Parameter vs Hyperparameter**  
  - **Parameter**  
    - AI가 스스로 알아내는 변수  
    - weight, bias, etc...  
  - **Hyperparameter**  
    - 사람이 직접 지정해줘야 하는 변수   
    - Initial weight  
    - learning rate & learning rate scheduling  
    - model architecture (number of layer & node, activation function, ...)  
    - loss function   
    - optimizer   

&nbsp;

## 2. Mini-batch SGD Implementation
### 2-1. Optimizer class definition

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet

# Optimizer classes  
# Gradient Descent class
class GD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# Stochastic Gradient Descent class
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# Mini-batch Stochastic Gradient Descent class
class mini_SGD:
    def __init__(self, lr=0.64):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

- **GD vs SGD vs Mini-batch**  
  - GD, SGD, Mini-batch 기법은 결국 train data의 샘플링 사이즈, 즉 Batch size 차이로 나뉘기 때문에 클래스 자체는 동일함   

&nbsp;

### 2-2. Test setting  

```python
# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size1 = 1
batch_size2 = 64

# Experiment setup
optimizers = {
    'SGD' : SGD(lr = 0.01),
    'GD' : GD(lr = 0.01),
    'mini_SGD' : mini_SGD(lr = 0.64)
}

networks = {}
train_loss = {}

# Initialize networks & loss tracking  
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784,
        hidden_size_list=[100, 100, 100, 100],
        output_size=10
    )
    train_loss[key] = []
```

&nbsp;

### 2-3. Training

```python
# Training 
# Training loop
for epoch in range(2000):
    # Sample a data for SGD
    batch_mask1 = np.random.choice(train_size, batch_size1)
    x_batch1 = x_train[batch_mask1]
    t_batch1 = t_train[batch_mask1]

    # Sample a mini-batch for SGD
    batch_mask2 = np.random.choice(train_size, batch_size2)
    x_batch2 = x_train[batch_mask2]
    t_batch2 = t_train[batch_mask2]


    for key in optimizers.keys():
        # Use full dataset for GD, mini-batch for SGD
        if key == 'GD':
            x_input = x_train
            t_input = t_train
        elif key == 'SGD':
            x_input = x_batch1
            t_input = t_batch1
        else:
            x_input = x_batch2
            t_input = t_batch2            

        # Compute gradients and update parameters
        grads = networks[key].gradient(x_input, t_input)
        optimizers[key].update(networks[key].params, grads)

        # Compute and record loss
        loss = networks[key].loss(x_input, t_input)
        train_loss[key].append(loss)

    # Print loss every 100 iterations
    if epoch % 100 == 0:
        print("========== epoch: " + str(epoch) + " ==========")
        for key in optimizers.keys():
            if key == 'GD':
                x_input = x_train
                t_input = t_train
            elif key == 'SGD':
                x_input = x_batch1
                t_input = t_batch1
            else:  # e.g., 'mini_SGD' or others
                x_input = x_batch2
                t_input = t_batch2
                
            loss = networks[key].loss(x_input, t_input)
            print(f"{key}: {loss}")
```

**학습 결과**    
========== epoch: 0 ==========  
SGD: 1.7643527774063241  
GD: 2.3269226487857395  
mini_SGD: 2.1679845365751085  
========== epoch: 100 ==========  
SGD: 0.4834347191547657   
GD: 1.3821736271929763   
mini_SGD: 1.1617592180558782  
========== epoch: 200 ==========  
SGD: 0.13033927171819498  
GD: 0.7186215995539207  
mini_SGD: 1.3091636292643918  
========== epoch: 300 ==========  
SGD: 0.05374343687986144  
GD: 0.5254953636533944  
mini_SGD: 0.3667281319515562  
========== epoch: 400 ==========  
SGD: 0.048141543263268305  
GD: 0.43930708446787514  
mini_SGD: 0.1631074287430223  
========== epoch: 500 ==========  
SGD: 0.021797788506586705  
GD: 0.3900666298336589  
mini_SGD: 0.27290195022614444  
========== epoch: 600 ==========  
...  
========== epoch: 1900 ==========  
SGD: 0.021375136408035084  
GD: 0.21924774534164443  
mini_SGD: 0.10397562630628239  
{: .notice}  

&nbsp;

### 2-4. Visulization  

```python
# === 3. Plot training loss ===

markers = {"SGD": "o", "GD": "x", "mini_SGD" :"v"}
x = np.arange(2000)

for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.ylim(0, 1)
plt.legend()
plt.title("Comparison of GD, SGD, mini-batch on MNIST")
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/19.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- **결과 분석**  
  - mini-batch 방식 또한 결국 train data를 샘플링하기 때문에 일정 수준의 noise는 보이지만, 기존 SGD 방식보다는 훨씬 안정적인 것을 알 수 있음  
  - 또한 수렴 속도 자체는 SGD와 거의 유사한 것을 확인 가능   

&nbsp;  