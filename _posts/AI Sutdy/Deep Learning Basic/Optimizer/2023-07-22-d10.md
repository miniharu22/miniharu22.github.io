---
layout : single
title: "[Optimizer] RMSprop"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

RMSprop 기법에 대해서 정리   

## 0. About RMSprop

&nbsp;

<div align="center">
  <img src="/assets/images/DL/22.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

- **RMSprop**  
  - 만약 Gradient의 수렴 방향이 weight나 bias, 둘 중 하나의 parameter로 치중될 경우, 학습 효율이 감소될 수 있음   
  - 따라서 각 parameter에 적합한 factor를 곱해줌으로써 효율을 끌어올리는 기법이 바로 **RMSprop**으로, learning rate를 각 parameter 별로 다르게 준 것과 같음    
  - 기존 기법들과 비교하면 Local minimum과 같은 Sattle point에 수렴하는 것을 방지하는 효과를 얻을 수 있음   

&nbsp;

- **RMSprop의 특징**  
  - **지수 이동 평균(Exponenential Moving Average)**     
    - RMSprop은 지수 이동 평균을 사용하여 이전 gradient의 제곱값의 평균을 계산함  
    - 이를 통해, 최근 gradient 값이 더 많이 반영되면서 gradient가 급격하게 감소하는 것을 방지   
  - **Learning rate 조정**  
    - RMSprop은 gradient 제곱의 이동 평균을 계산 후, 이를 learning rate로 나눠줌  
    - 결과적으로 learning rate를 조절하는 동시에, 각 parameter 별로 적절한 크기의 업데이트를 가능케 함     

&nbsp;

## 1. RMSprop Implementation  
### 1-1. Optimizer class definition

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet

# Optimizer classes  
# Mini-batch Stochastic Gradient Descent class
class mini_SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# RMSprop class
class RMSprop:
    def __init__(self, lr=0.01, decay_rate=0.99, epsilon=1e-8):
        self.lr = lr
        self.decay_rate = decay_rate
        self.epsilon = epsilon
        self.h = {}  # Squared gradient moving average

    def update(self, params, grads):
        if not self.h:
            for key in params:
                self.h[key] = np.zeros_like(grads[key])  # Initialize h for each param

        for key in params:
            # 1. Update moving average of squared gradients
            self.h[key] = self.decay_rate * self.h[key] + (1 - self.decay_rate) * (grads[key] ** 2)

            # 2. Apply adaptive learning rate scaling
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)
```

&nbsp;

### 1-2. Test setting  

```python
# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128

# Experiment setup
optimizers = {
    'mini_SGD': mini_SGD(lr=0.01),
    'RMSprop': RMSprop(lr=0.01)
}

networks = {}
train_loss = {}

# Initialize networks & loss tracking  
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784,
        hidden_size_list=[100, 100, 100, 100],
        output_size=10
    )
    train_loss[key] = []
```

&nbsp;

### 1-3. Training

```python
# Training 
# Training loop
for epoch in range(2000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in optimizers.keys():
        # Compute gradients and update parameters
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)

        # Compute and record loss
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)

    # Print loss every 100 iterations
    if epoch % 100 == 0:
        print("========== epoch: " + str(epoch) + " ==========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))
```

**학습 결과**    
========== epoch: 0 ==========  
mini_SGD:2.332518099693904  
RMSprop:14.103333682088529  
========== epoch: 100 ==========  
mini_SGD:1.4819117170212908  
RMSprop:0.35215638925055376  
========== epoch: 200 ==========  
mini_SGD:0.8457056487959888  
RMSprop:0.2849038528208391  
========== epoch: 300 ==========  
mini_SGD:0.5472828001258622  
RMSprop:0.14686246199895692  
========== epoch: 400 ==========  
mini_SGD:0.44652453782848434  
RMSprop:0.19425152494021541  
========== epoch: 500 ==========  
mini_SGD:0.44051698910220527  
RMSprop:0.2893813609714834  
========== epoch: 600 ==========  
mini_SGD:0.45843686876138906  
RMSprop:0.2529967362061332  
========== epoch: 700 ==========  
mini_SGD:0.36714068281719286  
RMSprop:0.1437336777190843  
========== epoch: 800 ==========  
...  
RMSprop:0.04748028253503912  
========== epoch: 1900 ==========  
mini_SGD:0.1208810183640465  
RMSprop:0.10396147324118468  
{: .notice}  


&nbsp;

### 1-4. Visulization 

```python
markers = {"mini_SGD" :"v", "RMSprop" : "o"}
x = np.arange(2000)

for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.ylim(0, 1)
plt.legend()
plt.title("Comparison of mini-batch and RMSprop on MNIST")
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/23.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- **결과 분석**  
  - Mini-batch SGD 기법과 비교한 결과, loss의 수렴 속도에 있어서 RMSprop 방식이 더 빠른 것을 확인 가능   

&nbsp;