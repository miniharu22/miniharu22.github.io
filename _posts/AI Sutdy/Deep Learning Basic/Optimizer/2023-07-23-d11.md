---
layout : single
title: "[Optimizer] Adam"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Adam(Adaptive Moment Estimation) 기법에 대해서 정리   

## 0. About Adam

&nbsp;

<div align="center">
  <img src="/assets/images/DL/24.gif" width="50%" height="50%" alt=""/>
  <p><em>Adadelta = Adam <br>수렴속도가 제일 빠름</em></p>
</div>

&nbsp;

- **Adam(Adaptive Moment Estimation)**  
  - [Momentum](https://miniharu22.github.io/deep%20learning%20basic/d9/) 기법과 [RMSprop](https://miniharu22.github.io/deep%20learning%20basic/d10/) 기법을 합친 최적화 알고리즘   
  - **주요 특징**  
    - **지수 이동 평균을 이용한 gradient 추정**  
      - Adam은 gradient와 gradient 제곱값의 이동 평균을 추정  
      - 이를 통해 이전 gradient 값을 보존하면서도 새로운 gradient의 영향을 반영   
    - **Hyperparmaeter 조정**  
      - Momentum의 지수 감쇠율과 RMSprop의 지수 감쇠율, 이 두 가지 hyperparameter를 사용하여 학습률을 효율적으로 조정   

&nbsp;

- **Adam의 작동 방식**  
  - **Gradient 계산** : parameter에 대한 loss function의 gradient를 계산   
  - **1차 moment 및 이동된 제곱 평균 추정** : 1차 moment(평균)과 이동된 gradient의 제곱값의 추정값을 계산   
  - **momentum 업데이트** : 계산한 값을 바탕으로 parameter를 갱신   
  - **Hyperparameter 보정** : learning rate의 보정을 수행   


&nbsp;

## 1. Adam Implementation
### 1-1. Optimizer class definition

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet

# Optimizer classes  
# Mini-batch Stochastic Gradient Descent class
class mini_SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# Momentum class
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

# RMSprop class
class RMSprop:
    def __init__(self, lr=0.01, decay_rate=0.99, epsilon=1e-8):
        self.lr = lr
        self.decay_rate = decay_rate
        self.epsilon = epsilon
        self.h = {}  # Squared gradient moving average

    def update(self, params, grads):
        if not self.h:
            for key in params:
                self.h[key] = np.zeros_like(grads[key])  # Initialize h for each param

        for key in params:
            # 1. Update moving average of squared gradients
            self.h[key] = self.decay_rate * self.h[key] + (1 - self.decay_rate) * (grads[key] ** 2)

            # 2. Apply adaptive learning rate scaling
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)

# Adam class
class Adam:
    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        for key in params.keys():
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
```

&nbsp;

### 1-2. Test setting   

```python
# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128

# Experiment setup
optimizers = {
    'mini_SGD': mini_SGD(lr=0.01),
    'Momentum' : Momentum(lr=0.01),
    'RMSprop': RMSprop(lr=0.01),
    'Adam' : Adam(lr=0.01)
}

networks = {}
train_loss = {}

# Initialize networks & loss tracking  
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784,
        hidden_size_list=[100, 100, 100, 100],
        output_size=10
    )
    train_loss[key] = []
```

&nbsp;

### 1-3. Training

```python
# Training 
# Training loop
for epoch in range(2000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in optimizers.keys():
        # Compute gradients and update parameters
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)

        # Compute and record loss
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)

    # Print loss every 100 iterations
    if epoch % 100 == 0:
        print("========== epoch: " + str(epoch) + " ==========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))
```

**학습 결과**  
========== epoch: 0 ==========  
mini_SGD:2.3335679551499573  
Momentum:2.2773881841653134  
RMSprop:14.103333682088527  
Adam:2.027689666220193  
========== epoch: 100 ==========  
mini_SGD:1.6013415080204536  
Momentum:0.40709877638476677  
RMSprop:0.42517081322119454  
Adam:0.17977216111125643  
========== epoch: 200 ==========  
mini_SGD:0.8424331910800772  
Momentum:0.26847611851353537  
RMSprop:0.4412998276197071  
Adam:0.1632469073037573  
========== epoch: 300 ==========  
mini_SGD:0.5880332942212605  
Momentum:0.2839815897267923  
RMSprop:0.23526150627025627  
Adam:0.2234951367074387  
========== epoch: 400 ==========  
mini_SGD:0.4498237138995269  
Momentum:0.12326941876084176  
RMSprop:0.11296463908966792  
Adam:0.06602046706593602  
...  
========== epoch: 1900 ==========    
mini_SGD:0.15030458452632098  
Momentum:0.03870928877486682  
RMSprop:0.05027398398543748  
Adam:0.09601655629381965  
{: .notice}

&nbsp;

### 1-4. Visulization 

```python
markers = {"mini_SGD" :"v", "Momentum" :  "p", "RMSprop" : "o", "Adam" : "s"}
x = np.arange(2000)

for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.ylim(0, 1)
plt.legend()
plt.title("Comparison of Optimizers on MNIST")
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/24.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- **결과 분석**  
  - mini-batch SGD 방식 대비 Momentum, RMSprop, Adam 알고리즘 모두 수렴 속도가 월등히 빠른 것을 육안으로 알 수 있음   
  - 그중에서도 Adam 알고리즘이 제일 빠르면서도 Noise도 적은 것을 확인 가능하며, 이는 Momentum 기법의 학습 경로 단축과 RMSprop 기법의 learning rate 보정, 두 장점을 동시에 가지기에 가능하다고 판단  


&nbsp;