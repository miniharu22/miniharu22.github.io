---
layout : single
title: "[Optimizer] Weight Initialization"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

가중치 초기화(Weight Initialization)에 대해서 정리  

## 0. About Weight Initialization

&nbsp;

<div align="left">
  <img src="/assets/images/DL/11.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **가중치 초기화(Weight Initialization)**    
  - Gradient Descent를 따라서 최적점을 찾아내려고 하더라도 Loss function이 위와 같다면, 초기 가중치 값에 따라 도달하는 최저점이 달라질 수 있음, 즉 성능에 영향을 끼침  
  - **가중치 초기화가 필요한 이유**    
    - 가중치가 매우 높은 값으로 초기화되는 경우   
      - layer의 출력 값이 상당히 높아져 sigmoid와 같은 활성화 함수를 적용할 때, 1에 가까운 값이 매핑됨  
      - sigmoid 값이 1에 가까워질 수록 기울기가 0에 가까워지고, 경사 하강 속도가 감소함으로써 학습에 많은 시간이 소요됨  
    - 가중치가 매우 작은 값으로 초기화되는 경우  
      - layer의 출력 값이 상당히 낮아짐  
      - sigmoid에서 출력 값이 작아질 수록 0에 가까운 값이 매핑되고, 이 또한 기울기가 0에 가까워져, 학습에 많은 시간이 소요됨  
    - Gradient vanishing 관련    
      - sigmoid 도함수의 경우, 최대값이 0.25로 layer가 깊어질수록 gradient가 감소함  
      - 이때, 제대로 초기화되지 않는 가중치를 사용하여 layer 출력 값이 0이나 1에 치우칠 경우, 학습이 제대로 안되는 상황이 초래될 수 있음    

&nbsp;

- **가중치 초기화의 종류**  
  - **LeCun**  
    - $$w$$ ~ $$U(-\sqrt{\frac{3}{N_{in}}}, \sqrt{\frac{3}{N_{in}}})$$  
    - $$w$$ ~ $$N(0, \frac{1}{N_{in}})$$   
  - **Kaiming**(ReLU 사용하는 신경망)   
    - $$w$$ ~ $$U(-\sqrt{\frac{6}{N_{in}}}, \sqrt{\frac{6}{N_{in}}})$$  
    - $$w$$ ~ $$N(0, \frac{2}{N_{in}})$$    
  - **Xavier**(sigmoid/tanh 사용하는 신경망)   
    - $$w$$ ~ $$U(-\sqrt{\frac{6}{(N_{in}+N_{out})}}, \sqrt{\frac{6}{(N_{in}+N_{out})}})$$  
    - $$w$$ ~ $$N(0, \frac{2}{(N_{in}+N_{out})})$$     

&nbsp;

- $$N_{in}$$ & $$N_{out}$$  
  - Input & Output Node의 갯수  
  - **$$N_{in}$$을 사용하는 이유** 
    - Input node가 너무 많은 경우, 가중치 데이터가 너무 많기 때문에 분산이 급증함  
    - 따라서, $$N_{in}$$을 일종의 Factor로 사용함으로써 분산을 조절  
  - **$$N_{out}$$을 사용하는 이유**  
    - Output node가 너무 많은 경우, Backpropagation 진행 시 gradient의 분산이 너무 커지기 때문  

&nbsp;

## 1. Weight Initialization Test
### 1-1. Load & Read MNIST Dataset

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt

from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet
from common.optimizer import SGD

# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128
```

&nbsp;

### 1-2. Test Setting

```python
# Test Setting
weight_init_types = {'Kaiming' : 'relu', 'Xavier' : 'sigmoid'}
opitimizer = SGD(lr = 0.01)

networks = {}
train_loss = {}

for key, weight_type in weight_init_types.items():
    networks[key] = MultiLayerNet(input_size= 784,
                                  hidden_size_list = [100, 100, 100, 100],
                                  output_size = 10,
                                  weight_init_std = weight_type
                                  )
    train_loss[key] = []
```

&nbsp;

### 1-3. Training

```python
# Training
for epoch in range(2000):
    # Randomly choose 128 samples
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in weight_init_types.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        opitimizer.update(networks[key].params, grads)

        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)

    if epoch % 100 == 0:
        print("===========" + "epoch:" + str(epoch) + "===========")
        for key in weight_init_types.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))
```

**학습 결과**  
===========epoch:0===========  
Kaiming:2.3805283307972043  
Xavier:2.296443205884054  
===========epoch:100===========  
Kaiming:1.5724395765540966  
Xavier:2.237045273371177  
===========epoch:200===========  
Kaiming:0.8682435147897237  
Xavier:2.0962867259376923  
===========epoch:300===========  
Kaiming:0.4309799426312949  
Xavier:1.7666654259991341  
===========epoch:400===========  
Kaiming:0.28007971722427477  
Xavier:1.2558682740996272  
===========epoch:500===========  
Kaiming:0.324564382033237  
Xavier:0.9061021317099933  
===========epoch:600===========  
Kaiming:0.29895460102218824  
Xavier:0.5994891935948107  
===========epoch:700===========  
Kaiming:0.19119296830454718  
Xavier:0.4144122021198333  
===========epoch:800===========  
...  
Xavier:0.30335061750979847  
===========epoch:1900===========  
Kaiming:0.18697297036457527  
Xavier:0.28984603895713706   
{:. notice}


&nbsp;

### 1-4. Plotting Results

```python
# Visualization
markers = {'Kaiming' : 'o', 'Xavier' : 's'}
x = np.arange(2000)
for key in weight_init_types.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 2.5)
plt.legend()
plt.show()
```
<div align="left">
    <strong>학습 결과</strong>
  <img src="/assets/images/DL/12.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- Kaiming 초기값을 사용하였을 때, 수렴이 더 빠른 것을 확인 가능   

&nbsp;
