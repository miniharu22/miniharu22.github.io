---
layout : single
title: "[Optimizer] Gradient Descent"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

경사 하강법(Gradient Descent)에 대해서 정리  

## 0. About Gradient Descent  

&nbsp;

<div align="left">
  <img src="/assets/images/DL/9.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **경사 하강법(Graient Descent)**  
  - node의 수가 많을 경우, Linear Regression만으로 최적의 a(weight)와 b(bias)를 찾기에는 연산량이 너무 많다는 점이 한계  
  - 따라서 현재 a,b 위치에서 Loss를 줄이는 방향으로 Gradient를 이용하는 것이 Gradient Descent의 핵심  
  - **Gradient**  
    - $$x^2+y^2$$ 그래프를 보면 Gradient는 항상 가장 가파른 방향으로 향함  
    - 즉, 현재 위치에서 Loss의 Gradient 값을 구한 다음 그 **반대 방향**으로 간다면 Loss를 최소화할 수 있는 a와 b로 수렴하게 됨   
  - **Learnig rate**($$\alpha$$)    
    - 다만, Gradient의 크기가 너무 클 때는 수렴이 아닌 진동을 할 수 있으므로 이를 보정할 factor가 필요, 이것이 바로 Learning rate  
    - Learning rate는 처음부터 끝까지 상수로 설정하거나 이 또한 스케줄링을 하기도 함  
  - Gradient Descent를 진행할 Intial weight의 경우, 랜덤하게 설정하되 되도록 0 근처로 설정함에 주의  

&nbsp;

- **경사 하강법의 한계**  
  - Gradient Descent는 모든 데이터를 고려하여 방향을 선택하기 때문에 정확도는 높을지언정, 연산량이 너무 많아 느리다는 것이 단점  
  - 또한, 극점이 여러개 존재하는 Loss Function의 경우 **Global minimum**이 아닌 **Local minimum**에 수렴할 수도 있음  
    - Global minimum : Loss function 전체에서 가장 최적의 loss를 산출하는 a와 b  
    - Local minimum : 복수의 극점이 존재할 때, 각 극점의 좌표에 해당하는 a와 b  

&nbsp;

## 1. Gradient Descent Implement  

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.random.rand(100)
Y = 0.2 * X * 0.5  

# Visulation Function
def plot_prediction(pred, y):
    plt.figure(figsize = (8, 6))
    plt.scatter(X, y)
    plt.scatter(X, pred)
    plt.show()

# Random number for weight(a) & bias(b) near zero point
a = np.random.uniform(-1, 1)
b = np.random.uniform(-1, 1)

# Learning rate
lr = 0.7

for epoch in range(100):
    Y_pred = a * X + b

    # Train termination condition with MAE
    error = np.abs(Y_pred - Y).mean()
    if error < 0.001 :
        break  

    # Gradient descent 
    # Compute gradient
    a_grad = lr * ((Y_pred - Y) * X).mean()
    b_grad = lr * (Y_pred - Y).mean()

    # Update a, b
    a = a - a_grad
    b = b - b_grad

    if epoch % 10 == 0:
        Y_pred = a * X + b
        plot_prediction(Y_pred, Y)
```

<div align="left">
    <strong>학습 결과</strong>
  <img src="/assets/images/DL/10.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- 임의의 Linear Function을 설정했을 때, epoch가 증가할 수록 Gradient Descent에 따라 a와 b가 수렴하는 것을 육안으로 확인 가능  

&nbsp;