---
layout : single
title: "[Optimizer] Momentum"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Momentum(관성) 기법에 대해서 정리   

## 0. About Momentum  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/16.png" width="40%" height="40%" alt=""/>
  <p><em>SGD & mini-batch SGD 학습 경로</em></p>
</div>

- **SGD의 한계**   
  - SGD와 mini-batch SGD의 경우, 위와 같이 gradient가 등고선(contour)의 접선 방향으로 이동하기 때문에 수렴하기까지 불필요한 시간을 소요하게 됨   
  - 이를 해결하기 위한 것이 **관성(momentum)**을 이용한 **Momentum** 알고리즘   

&nbsp;

<div align="left">
  <img src="/assets/images/DL/17.jpg" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Momentum**  
  - Momentum 알고리즘은 Gradient vector에 **관성을 적용한 속도 vector**를 더함으로써 다음 Step의 weight를 계산함    
  - 관성이 적용함으로써 gradient의 진행 방향은 급격하게 변하지 않는 동시에 전체적인 학습 경로가 매끄러워지고 가속도가 생겨 학습이 빨라진다는 장점이 있음   
  - 또한 관성에 의해 Local minumum에 빠지더라도 탈출할 수 있다는 장점을 가지고 있음  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/17.png" width="40%" height="40%" alt=""/>
  <p><em>Momentum 학습 경로</em></p>
</div>

&nbsp;

## 1. Momentum Implementation
### 1-1. Optimizer class definition

```python
import os, sys
sys.path.append("C:/Users/isang/OneDrive/Desktop/DL/deep-learning-from-scratch-master")

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.util import smooth_curve
from common.multi_layer_net import MultiLayerNet

# Optimizer classes  
# Mini-batch Stochastic Gradient Descent class
class mini_SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

# Momentum class
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]
```


&nbsp;

### 1-2. Test setting  

```python
# Load & Read MNIST Dataset
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128

# Experiment setup
optimizers = {
    'mini_SGD' : mini_SGD(),
    'Momentum' : Momentum()
}

networks = {}
train_loss = {}

# Initialize networks & loss tracking  
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784,
        hidden_size_list=[100, 100, 100, 100],
        output_size=10
    )
    train_loss[key] = []
```

&nbsp;

### 1-3. Training

```python
# Training 
# Training loop
for epoch in range(2000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    for key in optimizers.keys():
        # Compute gradients and update parameters
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)

        # Compute and record loss
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)

    # Print loss every 100 iterations
    if epoch % 100 == 0:
        print("========== epoch: " + str(epoch) + " ==========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))
```

**학습 결과**    
========== epoch: 0 ==========  
mini_SGD:2.2792778156892943  
Momentum:2.3836884618127834  
========== epoch: 100 ==========  
mini_SGD:1.532620051342606  
Momentum:0.3789838258353871  
========== epoch: 200 ==========  
mini_SGD:0.8649606016627037  
Momentum:0.3770450561149059  
========== epoch: 300 ==========  
mini_SGD:0.5498115344366721  
Momentum:0.2533083851005702  
========== epoch: 400 ==========  
mini_SGD:0.3805648885983156  
Momentum:0.1532302560305952  
========== epoch: 500 ==========  
mini_SGD:0.3892171661703986  
Momentum:0.17489084363941576  
========== epoch: 600 ==========  
mini_SGD:0.4728652134655976  
Momentum:0.14668838014481392  
========== epoch: 700 ==========  
mini_SGD:0.25117838277079185  
Momentum:0.11960211016264141  
========== epoch: 800 ==========  
...  
Momentum:0.03500280356602503  
========== epoch: 1900 ==========  
mini_SGD:0.15136658731676103  
Momentum:0.03215767284007858  
{: .notice}  

&nbsp;

### 1-4. Visulization  

```python
# === 3. Plot training loss ===

markers = {"mini_SGD" :"v", "Momentum" : "o"}
x = np.arange(2000)

for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)

plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.ylim(0, 1)
plt.legend()
plt.title("Comparison of mini-batch and Momentum on MNIST")
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/20.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

- **결과 분석**  
  - Mini-batch 방식과 비교하면 Noise는 그대로더라도 loss의 수렴속도는 Momentum이 월등히 높은 것을 육안으로 확인 가능  
  - 결국, 학습 경로를 단축시키는 것이 효율적이라는 것을 알 수 있음   



&nbsp;  