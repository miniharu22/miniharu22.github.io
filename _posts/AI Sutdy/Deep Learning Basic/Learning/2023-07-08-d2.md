---
layout : single
title: "[Learning] Reinforcement Learning"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

강화 학습(Reinforcement Learning)에 대해서 정리  

## 0. About Reinforcement Learning  

- **Reinforcement Learning**    
  - 동물의 학습 능력을 모방한 것으로, 특정 상태(state)에서 어떤 행동(action)을 취하는 것이 최적인지를 학습하는 기법  
  - **강화 학습 용어**  
    - `Agent` : 강화학습 문제 상황에서 학습 혹은 행동의 주체가 되는 시스템  
    - `Action` : Agent의 선택  
    - `Reward`  
      - Agent가 action을 취했을 때 받게 되는 보상으로서, 다음과 같은 종류가 있음   
      - `Positive Reward` : 특정 행동을 유발시키기 위함  
      - `Negative Reward` : 특정 행동을 금지시키기 위함    
    - `Environment` : Agent와 상호작용하는 환경   
    - `State`  
      - Agent가 필요한 구체적 정보를 의미    
      - Agent가 action을 취하면, 그에 따라 환경이 agent의 상태를 변화시킴  
    - `Episode` : Agent가 한 번의 학습을 진행/종료하는 횟수   
    - `Q-function`  
      - State와 Action에 대한 함수  
      - 특정 State에서 특정 Action을 취했을 때의 기대치를 산출   
  - **Deep Reinforcement Learning 용어**  
    - `Q-learning` : 딥러닝 모델을 통해 Q-function을 학습하는 기법   
    - `Exploration` : Agent가 현재 학습한 지식으로 행동을 하지 않고, 모르는 방향으로 나아가서 정보를 얻기 위해 행동하는 것을 의미   
    - `Discount Factor` : Agent가 얻을 reward에 factor를 적용함으로써 최단경로로 학습해나갈 수 있게 조정하는 기법  

&nbsp;

## 1. Reinforcement Learning with CartPole Game
### 1-1. ANN Structure

```python
from torch import nn
from torch.distributions import Categorical
import torch
import torch.optim as optim
import gym
import numpy as np

# Dicount Factor
gamma = 0.99

class Pi(nn.Module):
  def __init__(self, in_dim, out_dim):
    super(Pi, self).__init__()
    layers = [
        nn.Linear(in_dim, 64),
        nn.ReLU(),
        nn.Linear(64, out_dim),
        ]
    self.model = nn.Sequential(*layers)
    self.onpolicy_reset()
    self.train() 
  
  # Initialize log_probs & rewards
  def onpolicy_reset(self):
    self.log_probs = []
    self.rewards = []
  
  # Calculate Propagation based on the state of Agent
  def forward(self, x):
    pdparam = self.model(x)
    return pdparam

  # By taking the state as a factor, call forward(), calculate it
    # and Select the action as a sample
  # log_probs according to the action are calulated and stored
  def act(self, state):
    x = torch.from_numpy(state.astype(np.float32))  # Convert to numpy to tensor
    pdparam = self.forward(x)                       # Forward pass      
    pd = Categorical(logits=pdparam)                # Probability Distribution
    action = pd.sample()                            # Sampling one action from pd
    log_prob = pd.log_prob(action)                  # Calculate log_probs
    self.log_probs.append(log_prob)                 # Store log_probs
    return action.item()
```

- `Pi` 설명  
  - 64개의 unit으로 구성된 Linear-ReLU-Linear Layer 구축  
  - `onpolicy_reset` : training 이후, `log_probs`, `rewards` 초기화  
  - `forward` : Agent의 state를 바탕으로 순전파 계산 수행  
  - `act`  
    - state를 인자로 받는 동시에, `forward` 함수를 호출하여 계산 후, action을 sampling  
    - action에 따른 `log_prob`를 계산하여 저장    
    - `log_prob` : 임의의 확률분포에서 특정 행동이 선택될 로그 확률값  

&nbsp;

### 1-2. Train code  

```python
def train(pi, optimizer):
  # Inner gradient-ascent loop of REINFORCE algorithm
  T = len(pi.rewards)                  # Total number of rewards received durning the episode
  rets = np.empty(T, dtype=np.float32) # Array to store the returns
  future_ret = 0.0                     # Intialize Future return
  
  # Compute the returns efficiently
  for t in reversed(range(T)):
    future_ret = pi.rewards[t] + gamma * future_ret
    rets[t] = future_ret

  # Convert to Pytorch tensor  
  rets = torch.tensor(rets)
  log_probs = torch.stack(pi.log_probs)

  # Calculate Loss Sum
  loss = - log_probs * rets # gradient term; Negative for maximizing
  loss = torch.sum(loss)

  # Backpropagation & Optimization
  optimizer.zero_grad()     # Clear previous gradients
  loss.backward()           # Compute gradients via backpropagation
  optimizer.step()          # Update policy parameters using gradients
  return loss
```

- `train` 설명  
  - `pi`에 계산되어 있는 reward를 가져와서 `gamma`를 통해 discounted reward를 산출  
  - `reward`와 `log_prob`를 곱한 후, negative함으로써 loss sum을 계산  
  - 계산된 loss를 바탕으로 `loss.backward()`를 계산, optimizer를 불러와 최적화를 진행  

&nbsp;

### 1-3. Reinforcement Learning

```python
def main():
  # Initialize Environment & Network
  env = gym.make('CartPole-v0')           # Call the Environment
  in_dim = env.observation_space.shape[0] # 4
  out_dim = env.action_space.n            # 2
  pi = Pi(in_dim, out_dim)                # policy pi_theta for REINFORCE
  optimizer = optim.Adam(pi.parameters(), lr=0.01)
  
  # Episode Loop
  for epi in range(300):
    state, _ = env.reset()
    # Time Step Loop
    for t in range(200): 
      action = pi.act(state) # Action Sampling
      next_state, reward, terminated, truncated, _ = env.step(action) # Calculate Next State & Reward
      done = terminated or truncated
      pi.rewards.append(reward) # Store Reward 
      env.render()
      state = next_state
      if done:
        break
    loss = train(pi, optimizer) # train per episode
    total_reward = sum(pi.rewards)
    solved = total_reward > 195.0
    pi.onpolicy_reset() # onpolicy: clear memory after training
    
    print(f'Episode {epi}, loss: {loss}, \
    total_reward: {total_reward}, solved: {solved}')
    
if __name__ == '__main__':
  main()
```

**학습 결과**  
Episode 0, loss: 42.46593475341797,     total_reward: 11.0, solved: False  
Episode 1, loss: 150.3479766845703,     total_reward: 21.0, solved: False  
Episode 2, loss: 189.4750518798828,     total_reward: 24.0, solved: False  
...   
Episode 57, loss: 5590.64404296875,     total_reward: 200.0, solved: True  
Episode 58, loss: 5795.24267578125,     total_reward: 200.0, solved: True  
...  
Episode 163, loss: 4125.0048828125,     total_reward: 176.0, solved: False   
Episode 164, loss: 3814.0244140625,     total_reward: 167.0, solved: False  
Episode 165, loss: 5419.02880859375,     total_reward: 200.0, solved: True  
Episode 166, loss: 4674.5634765625,     total_reward: 191.0, solved: False  
Episode 167, loss: 4295.33837890625,     total_reward: 200.0, solved: True  
Episode 168, loss: 5320.81591796875,     total_reward: 200.0, solved: True  
Episode 169, loss: 4535.9794921875,     total_reward: 182.0, solved: False  
Episode 170, loss: 4258.580078125,     total_reward: 200.0, solved: True  
Episode 171, loss: 5429.326171875,     total_reward: 200.0, solved: True  
Episode 172, loss: 5349.14501953125,     total_reward: 200.0, solved: True  
Episode 173, loss: 4476.59326171875,     total_reward: 200.0, solved: True  
Episode 174, loss: 4815.4189453125,     total_reward: 200.0, solved: True  
Episode 175, loss: 5528.533203125,     total_reward: 200.0, solved: True  
Episode 176, loss: 3424.606689453125,     total_reward: 165.0, solved: False  
Episode 177, loss: 4820.150390625,     total_reward: 200.0, solved: True  
Episode 178, loss: 4406.384765625,     total_reward: 200.0, solved: True  
...  
Episode 297, loss: 2011.51025390625,     total_reward: 133.0, solved: False  
Episode 298, loss: 1618.895751953125,     total_reward: 123.0, solved: False  
Episode 299, loss: 2287.964111328125,     total_reward: 132.0, solved: False  
{: .notice}  


- `main` 설명  
  - `gym` 라이브러리를 통해 `CartPole-vo`을 수행할 environment 호출  
  - ANN 구조와 opitimizer 세팅 진행  
    - Loop 구조  
      - 매 episode마다 state는 초기화되고, timestep 200번을 돌 때까지 실패하지 않았다면 성공으로 간주  
      - 매 timestep마다 `Pi()`을 통해 action을 샘플링하며, 이에 따른 다음 state와 reward를 구하여 저장  
      - timestep이 종료되면, `train()`을 통해 loss를 구하고 업데이트  
      - `Pi()`에서 도출한 reward를 더하여 최종 성공여부를 판단  
      - 다시 episode 시작할 때는 모든 `log_prob`와 `reward`를 초기화  

- **결과 분석**  
  - 최초로 성공한 Episode는 57번째  
    - 우연히 성공한 것이기에, 이후로는 다시 실패했지만, reward 향상에 기여했을 것으로 예상  
    - 이후 300번의 Episode 동안, 165번째부터 total_reward가 200에 수렴하기 시작한 것을 확인 가능  

&nbsp;