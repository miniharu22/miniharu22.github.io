---
layout : single
title: "[Multiclass Classification] Softmax Regression"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

다중 분류(Multiclass Classification)에 특화된 Softmax Regression에 대해서 정리   

## 0. Softmax Regression

&nbsp;

<div align="center">
  <img src="/assets/images/DL/35.png" width="50%" height="50%" alt=""/>
  <p><em>Softmax Function</em></p>
</div>

&nbsp;

- **Softmax Regression**  
  - Softmax Regression은 Sigmoid 함수가 아닌 Softmax 함수를 Activation으로 사용하는 것으로 Multi-Class Classification(다중 분류)에 특화됨  
  - **Sigmoid의 문제점**  
    - 값이 조금만 커지거나 작아져도 0 또는 1에 수렴하기에, 다중 분류에는 사용하기 힘듦   
    - 다중 분류의 경우, 각각의 node에 대해 상대적인 평가가 불가능함   
  - **Softmax의 장점**  
    - 값이 한 쪽으로 수렴되지 않음  
    - 출력 값이 확률 분포를 따르기 때문에 모든 node에 대해 상대적인 평가가 가능함  
  - Softmax Regression 또한 Linear Combination을 통해 얻은 Logit을 Softmax 함수를 통해 확률들로 변환한 것이므로 Linear Regression의 연장선으로 해석 가능   

&nbsp;

- **Cross Entropy**  
  - Logistic Regression에서는 BCE Loss를 사용했다면, Softmax Regression에서는 Cross Entropy Loss를 사용   
  - 입력값에 상응하는 출력값이 나오게끔 분포 차이를 나타내는 특징을 가지고 있음   
  - 해당 방식이 이진 분류에 사용된다면, 결국 BCE(Binary Cross Entropy)와 동일함   

&nbsp;

## 1. Softmax Regression Implementation  
### 1-1. Load MNIST Dataset   

```python
from torch import nn, optim, cuda
from torch.utils import data
from torchvision import datasets, transforms

import torch
import torch.nn as nn
import torch.nn.functional as F
import time

# Training settings
batch_size = 64
device = 'cuda' if cuda.is_available() else 'cpu'   # Use GPU if available, otherwise CPU

# MNIST Dataset
train_dataset = datasets.MNIST(root='./mnist_data/',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='./mnist_data/',
                              train=False,
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
train_loader = data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
```

&nbsp;
 
### 1-2. Define Neural Network model

```python
# Define the neural network model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Define fully connected layers
        self.l1 = nn.Linear(784, 520)  # Input layer (28x28=784) to hidden layer
        self.l2 = nn.Linear(520, 320)  # Hidden layer
        self.l3 = nn.Linear(320, 240)  # Hidden layer
        self.l4 = nn.Linear(240, 120)  # Hidden layer
        self.l5 = nn.Linear(120, 10)   # Output layer (10 classes)


    def forward(self, x):
        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)
        x = F.relu(self.l1(x))  # Apply ReLU activation
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        return self.l5(x)       # Output logits
```

&nbsp;
 
### 1-3. Train function

```python
# Initialize model, loss function, and optimizer
model = Net()
model.to(device) # Move model to the appropriate device (CPU/GPU)
criterion = nn.CrossEntropyLoss()   # Loss function for multi-class classification
optimizer = torch.optim.Adam(model.parameters(),    # Adam optimizer
                             lr=0.01, 
                             betas=(0.9, 0.999),
                             eps=1e-08,
                             weight_decay=0,
                             amsgrad=False)

# Training function
def train(epoch):
    model.train()   # Set the model to training mode
    for batch_idx, (data, target) in enumerate(train_loader):
        # Move data and target to the device
        data, target = data.to(device), target.to(device)   
        optimizer.zero_grad()               # Zero the gradients
        output = model(data)                # Forward pass
        loss = criterion(output, target)    # Compute loss
        loss.backward()                     # Backward pass
        optimizer.step()                    # Update weights
        if batch_idx % 10 == 0:
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
```

&nbsp;
 
### 1-4. Test function  

```python
# Testing function
def test():
    model.eval()    # Set model to evaluation mode
    test_loss = 0   # Initialize test loss
    correct = 0     # Initialize correct predictions counter
    for data, target in test_loader:
        # Move data and target to the device
        data, target = data.to(device), target.to(device)
        output = model(data)                            # Forward pass
        test_loss += criterion(output, target).item()   # Accumulate loss

        # Get the index of the max log-probability
        pred = output.data.max(1, keepdim=True)[1]      
        # Count correct predictions
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    # Average the test loss
    test_loss /= len(test_loader.dataset)
    print(f'===========================\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({100. * correct / len(test_loader.dataset):.0f}%)')
```

&nbsp;
 
### 1-5. Main function  

```python
# Main function
if __name__ == '__main__':
    since = time.time()
    for epoch in range(1, 10):
        epoch_start = time.time()
        train(epoch)
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'Training time: {m:.0f}m {s:.0f}s')
        test()
        m, s = divmod(time.time() - epoch_start, 60)
        print(f'Testing time: {m:.0f}m {s:.0f}s')

    m, s = divmod(time.time() - since, 60)
    print(f'Total Time: {m:.0f}m {s:.0f}s\nModel was trained on {device}!')
```

**학습 및 추론 결과**  
Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 0.025043  
Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 0.100890  
Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 0.102560  
Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 0.128611  
Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 0.395046  
Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 0.259228  
Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 0.291979  
Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 0.158741  
Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 0.162068  
Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 0.082402  
Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 0.181836  
Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 0.126667  
Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 0.131128  
Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 0.041252  
Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 0.134402  
Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 0.036284  
Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 0.256561  
Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 0.100930  
Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 0.136768  
Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 0.059977  
Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 0.096708  
Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 0.034465  
Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 0.214865  
Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 0.128551  
Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 0.098118  
...  
Test set: Average loss: 0.0025, Accuracy: 9650/10000 (96%)  
Testing time: 0m 8s  
Total Time: 1m 16s  
Model was trained on cpu!  
{: .notice}  

&nbsp;



 