---
layout : single
title: "[Binary Classification] Sigmoid & Logistic Regression"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

Binary Classifiaction(이진 분류)에서 Sigmoid 함수와 Logistic Regression(로지스틱 회귀)의 개념을 정리   

## 0. Sigmoid

&nbsp;

<div align="center">
  <img src="/assets/images/DL/30.png" width="50%" height="50%" alt=""/>
  <p><em>Unit-step Function</em></p>
</div>

&nbsp;

- **이진 분류에서 Unit-step function의 한계**  
  - 분류 경계가 선형적이지만, 미분이 불가능하다는 단점이 존재   
    - 즉, 경사 하강법을 사용 못함  
  - Step Function의 특성 상, 분류가 너무 극단적  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/29.png" width="50%" height="50%" alt=""/>
  <p><em>Sigmoid Funtion</em></p>
</div>

&nbsp;

- **Sigmoid**  
  - Unit Step function의 단점을 해소하기 위해 연속적으로 변형한 함수  
  - 전 구간 미분이 가능하기에 경사 하강법을 사용 가능하며, 유연한 분류가 가능하다는 장점이 존재  
  - 수식 : $$\frac{1}{1+e{-x}}$$  

&nbsp;

## 1. Logistic Regression

&nbsp;

<div align="center">
  <img src="/assets/images/DL/32.png" width="50%" height="50%" alt=""/>
  <p><em>BCE Loss</em></p>
</div>

&nbsp;

- **BCE(Binary Cross Entropy) Loss**
  - Linear Regression에서 MSE(Mean Square Loss)가 활용되었다면, BCE(Binary Cross Entropy)는 이진 분류에서 주로 활용되는 Loss  
  - 특정 객체에 대해 AI가 예측한 결과가 정답일 확률을 $$p$$라고 정의  
    - 다만 각각의 시행은 독립시행이므로 확률을 계속 곱해야 하는데, 이러면 underflow가 발생할 수 있으므로 log를 취해서 곱셈을 덧셈으로 변환     

&nbsp;

- **MSE vs BCE**  
  - MSE($$(q_1-1)^2$$) 대신에 BCE($$-\text{log}q_1$$)을 쓰는 이유는 뭘까?  
  - MSE와 비교하면 BCE가 훨씬 sensitive함  
  - MSE는 Non-convex한 반면, BCE는 Convex함  
    - **Convex** : Local Minumum에 수렴하지 않음   
    - 다만, 이는 Single node에 대해서만 해당될 뿐, FC Layer와 같은 Multi-node에서는 BCE 또한 Non-convex함, 대신 MSE보다는 덜함  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/33.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Logistic Regression(로지스틱 회귀)**
  - Logistic Regression은 이진 분류(Binary Classification)에 사용되지만, 구조적으로는 선형 회귀(Linear Regression)의 확장 형태임  
  - 이는 입력에 선형 결합(linear combination)을 적용해 얻은 **Logit** 값을, **Sigmoid 함수**를 통해 확률로 변환하기 때문   

&nbsp;

## 2. Logistic Regression Implementation
### 2-1. Create Dataset

```python
import torch
import torch.nn as nn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification

features, targets = make_classification(    
                                        n_samples = 300,
                                        n_features = 2,
                                        n_informative = 1,
                                        n_redundant = 0,
                                        n_clusters_per_class = 1,
                                        random_state = 1
                                        )

plt.style.use('fivethirtyeight')
plt.scatter(features[:,0], features[:,1], c=targets)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

<div align="left">
    <strong>출력 결과</strong>
  <img src="/assets/images/DL/34.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>
{: .notice} 

&nbsp;

### 2-2. Dataset Split

```python
inputs = torch.from_numpy(features).float()
targets = torch.from_numpy(targets).float().unsqueeze(1)

# Split the dataset into train and test
train_inputs = inputs[:200]
train_targets = targets[:200]
test_inputs = inputs[200:]
test_targets = targets[200:]
```

&nbsp;

### 2-3. Simple Logistic Regression model with Sigmoid

```python
class LogisticRegression(nn.Module):
    def __init__(self):
        super(LogisticRegression, self).__init__()
        
        # Define the linear layer with 2 input features and 1 output feature
        self.linear = nn.Linear(2, 1)
        
        # Define the sigmoid activation function
        self.sigmoid = nn.Sigmoid()
    
    # Define the forward pass of the model
    def forward(self, x):
        out = self.linear(x) # Pass the input through the linear layer
        out = self.sigmoid(out) # Apply the sigmoid activation function
        return out
```

&nbsp;

### 2-4. Training

```python
model = LogisticRegression()
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), 
                             lr=0.01, 
                             betas=(0.9, 0.999),
                             eps=1e-08,
                             weight_decay=0,
                             amsgrad=False)


# Iterate over epochs
for epoch in range(1000):
    # Zero the gradients
    optimizer.zero_grad()
    # Forward pass
    outputs = model(train_inputs)
    # Calculate loss
    loss = criterion(outputs, train_targets)
    # Backward pass
    loss.backward()
    # Update parameters
    optimizer.step()

    if (epoch+1) % 100 == 0:
        print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}')
```

**학습 결과**   
Epoch: 100, Loss: 0.2572  
Epoch: 200, Loss: 0.1440  
Epoch: 300, Loss: 0.0921  
Epoch: 400, Loss: 0.0645  
Epoch: 500, Loss: 0.0479  
Epoch: 600, Loss: 0.0372   
Epoch: 700, Loss: 0.0298  
Epoch: 800, Loss: 0.0245  
Epoch: 900, Loss: 0.0205  
Epoch: 1000, Loss: 0.0174  
{: .notice}

&nbsp;

### 2-5. Test with Predict data

```python
with torch.no_grad():
    # Forward pass on the test set
    outputs = model(test_inputs)

    # Convert the outputs to binary predictions
    predictions = (outputs >= 0.5).float()

    # Compute the accuracy
    accuracy = ((predictions == test_targets).sum() / test_targets.shape[0]).item()

    # Print the accuracy
    print('Test Accuracy: {:.4f}'.format(accuracy))
```

**Test 결과**     
Test Accuracy: 1.0000  
{: .notice}   

&nbsp;