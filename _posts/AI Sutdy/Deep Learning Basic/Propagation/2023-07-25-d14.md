---
layout : single
title: "[Propagation] Backpropagation"
categories: 
  - Deep Learning Basic
toc: true
toc_sticky: true
use_math: true
---

역전파(Backpropagation)과 순전파(Foward propagation)에 대해서 정리  

## 0. Backpropagation  

&nbsp;

<div align="center">
  <img src="/assets/images/DL/28.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Backpropagation(역전파)**  
  - 모델의 가중치를 학습하기 위한 과정으로, loss function의 gradient를 계산하는 과정을 의미  
  - 역전파를 통해 계산한 gradient를 기반으로 최적화 알고리즘을 이용하여 모델의 파라미터를 업데이트할 수 있음   
  - **역전파 과정**   
    - **순전파**  
      - 데이터를 모델에 입력하여 예측값을 계산   
    - **Loss function 계산**  
      - 예측값과 실제 값 사이의 오차를 계산하는 loss function을 정의  
      - 순전파로 얻은 예측값과 실제 값으로 loss function을 계산   
    - **Gradient 계산**  
      - loss function의 값에서 Input layer의 gradient를 계산  
      - 해당 gradient는 loss function 값에 대한 Input layer weight의 편미분  
    - **역전파**  
      - Gradient를 이전 layer로 전파   
      - 각 layer에서는 해당 layer의 입력과 gradient를 이용하여 weight와 bias에 대한 편미분 값을 계산   
    - **Weight update**  
      - 계산된 편미분 값을 사용하여 최적화 알고리즘을 통해 weight와 bias를 업데이트  
      - learning rate와 같은 hyperparameter를 사용하여 weight updata의 크기를 조절 가능  
    - **반복**  
      - 업데이트된 weight를 바탕으로 다시 순전파를 수행하여 새로운 예측값을 계산  
      - 역전파를 수행하여 새로운 gradient를 계산    
      - 해당 과정을 지정된 epoch 횟수만큼 반복    

&nbsp;

- **Foward propagation(순전파)를 먼저 해야 하는 이유**  
  - 역전파를 수행하기 위해서는 결국 weight와 bias에 대한 loss function의 출력값을 알고 있어야 함  
  - 따라서 순전파를 먼저 수행하여 각 파라미터에 대한 수치를 구해 놓고 역전파를 통해 gradient를 얻어내야 함    

&nbsp;

## 1. Backpropagation Implementation
### 1-1. Data & Model definition

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Example data
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# Simple LinearRegression model with Forward Propagation  
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1,1) # Input : 1, Output : 1

    def forward(self, x):
        return self.linear(x)
```

&nbsp;

### 1-2. Training

```python
# Train code  
# Generate model
model = LinearRegressionModel()
# Set Optimizer with Adam
optimizer = optim.Adam(model.parameters(), lr = 0.01)
# Loss function  
criterion = nn.MSELoss()

# Training loop
for epoch in range(1000):
    # Foward Propagation
    outputs = model(x_train)
    loss = criterion(outputs, y_train)

    # Backpropagation
    optimizer.zero_grad()   # Initialize gradient
    loss.backward()         # Calculate gradient
    optimizer.step()        # Update gradient

    if (epoch+1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{1000}], Loss: {loss.item():.6f}")
```

**학습 결과**  
Epoch [100/1000], Loss: 17.822374  
Epoch [200/1000], Loss: 3.998354  
Epoch [300/1000], Loss: 0.747344  
Epoch [400/1000], Loss: 0.254126  
Epoch [500/1000], Loss: 0.192581  
Epoch [600/1000], Loss: 0.171242  
Epoch [700/1000], Loss: 0.152221  
Epoch [800/1000], Loss: 0.133719  
Epoch [900/1000], Loss: 0.116020  
Epoch [1000/1000], Loss: 0.099409  
{: .notice}

&nbsp;