---
layout : single
title: "[VGGNet] VGGNet-19 Implementation"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

VGGNet 논문 구현   

[참고 논문 링크](https://arxiv.org/abs/1409.1556)  

- September 4 2014 
- [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan,+K), [Andrew Zisserman](https://arxiv.org/search/cs?searchtype=author&query=Zisserman,+A)    

## 0. dataloader.py   

```python
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

def load_dataset():

    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])
                                    
    trainset = torchvision.datasets.CIFAR10(root='/data',
                                            train=True,
                                            download=True,
                                            transform=transform)

    testset = torchvision.datasets.CIFAR10(root='/data',
                                        train=False,
                                        download=True,
                                        transform=transform)
                                        
    trainloader = DataLoader(trainset,
                            batch_size=256,
                            shuffle=True,
                            num_workers=2)
    testloader = DataLoader(testset,
                            batch_size=100,
                            shuffle=True,
                            num_workers=2)
    
    return trainloader, testloader
```

&nbsp;

## 1. model.py

```python
import torch.nn as nn
from torch.nn.modules.dropout import Dropout

# VGG model definition
cfg = {
    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}

class VGG(nn.Module):
    def __init__(self, vgg_name, num_classes=10):
        super(VGG, self).__init__()
        # Initialize the VGG model with the specified configuration
        self.features = self._make_layers(cfg[vgg_name])

        self.fc_layer = nn.Sequential(
            # Cifar-10 Size = 32x32  
            nn.Linear(512*1*1, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),

            nn.Linear(4096, 1000),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),

            nn.Linear(1000, num_classes)
            )
    
    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.fc_layer(out)

        return out   
    
    def _make_layers(self, cfg):
        layers = []
        in_channels = 3
        for x in cfg:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), 
                        nn.BatchNorm2d(x),   
                        nn.ReLU(inplace=True)]
                in_channels = x

        return nn.Sequential(*layers)
```

&nbsp;

## 2. train.py   

```python
import numpy as np 
import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.tensorboard import SummaryWriter

from model import VGG
from dataloader import load_dataset
from tqdm import tqdm

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data,target) in enumerate(train_loader):
        target = target.type(torch.LongTensor)
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 30 == 0:
            print(f"{batch_idx*len(data)}/{len(train_loader.dataset)}")

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target, reduction='mean').item()
            writer.add_scalar("Test Loss", test_loss, epoch)
            pred = output.argmax(1)
            correct += float((pred == target).sum())
            writer.add_scalar("Test Accuracy", correct, epoch)
            
        test_loss /= len(test_loader.dataset)
        correct /= len(test_loader.dataset)
        return test_loss, correct
        writer.close()

if __name__ == "__main__":

    num_epochs = 20
    learning_rate = 0.0001

    use_cuda = torch.cuda.is_available()
    print("use_cuda : ", use_cuda)
    device = torch.device("cuda:0" if use_cuda else "cpu")

    trainloader, testloader = load_dataset()

    model = VGG('VGG19').to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    writer = SummaryWriter("./vggnet/tensorboard") 

    for epoch in tqdm(range(1, num_epochs + 1)):
        train(model, device, trainloader, optimizer, epoch)
        test_loss, test_accuracy = test(model, device, testloader)
        writer.add_scalar("Test Loss", test_loss, epoch)
        writer.add_scalar("Test Accuracy", test_accuracy, epoch)
        print(f"Processing Result = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
        writer.close()

                                 
    print(f"Result of VGGNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
```

**학습 결과**   
use_cuda :  True   
0%|          | 0/20 [00:00<?, ?it/s]0/50000   
7680/50000   
15360/50000  
23040/50000  
30720/50000  
...  
38400/50000  
46080/50000   
100%|██████████| 20/20 [09:36<00:00, 28.85s/it]Processing Result = Epoch : 20  Loss : 0.011424077636003494   Accuracy : 0.7577   
Result of VGGNet = Epoch : 20   Loss : 0.011424077636003494   Accuracy : 0.7577   
{: .notice}   

&nbsp;

