---
layout : single
title: "[SENet] SENet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

Squeeze-and-Excitation Networks   

[참고 논문 링크](https://arxiv.org/abs/1709.01507)  

- September 5 2017    
- [Jie Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+J), [Li Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen,+L), [Samuel Albanie](https://arxiv.org/search/cs?searchtype=author&query=Albanie,+S), [Gang Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+G), [Enhua Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+E)    


## 0. About SENet    

&nbsp;

- **SENet**   
  - 기존의 모델들이 Depth나 Layer의 관계를 수정하면서 성능을 향상시켜 왔다면, SENet은 채널들의 관계를 토대로 성능을 향상시키는 방법을 제시    
  - Layer의 구조를 변경시키는 것이 아닌 채널 간의 특징을 파악하고 채널 사이의 상호 종속 관계들을 명시적으로 모델링하면서 그에 맞게 재조정함     
  - 특히, 새로운 Layer 구조를 만드는게 아닌 채널 간의 관계를 변경시키는 것이기에 다른 SOTA 모델(예를 들면 ResNet)에 추가하는 방식으로 응용 가능함    
    - 또한 SE Block을 추가한다고 해서 computational cost는 크게 증가하지 않으면서 모델의 performance는 크게 향상됨    

&nbsp;

## 1. Channel Weighting      

&nbsp;

<div align="center">
  <img src="/assets/images/Net/34.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **채널별 중요도 적용**    
  - SENet에서는 중요한 채널에는 더 큰 가중치를, 안 중요한 채널에는 작은 가중치를 주는게 핵심    
  - 위 자료와 같이 CNN의 추출한 fmap을 채널별로 분리 후, 각각의 중요도를 계산해서 채널별로 곱함   
    - 이렇게 중요도를 곱해서 새롭게 계산된 채널들은 정보의 중요도에 비례하여 그 값이 상이해짐, 이를 다시 쌓아서 원래의 fmap size로 만들어줌     

&nbsp;

<div align="center">
  <img src="/assets/images/Net/35.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **채널별 중요도 계산 방법**    
  - 기본적으로 fmap의 정보를 채널별로 나눈 후, 각 채널별 정보에 대해 중요도를 계산하여 하나의 값으로 표현해야 함     
  - **정보 압축**    
    - fmap의 tensor가 W x H x C 라면 각 채널별 정보의 size는 W x H 일텐데 이는 너무 크므로 정보를 압축해줄 필요가 있음     
    - 각 채널별 정보를 하나의 값으로 압축한다면 해당 정보에서 위치 정보는 소실되고, 물체 정보만 남았을 거임    
  - **물체 정보에 대한 중요도 계산**    
    - 압축 후 각 채널별로 물체 정보만 남았다면 연산을 통해 중요도를 계산해야함    
    - 그렇게 나온 채널별 중요도 값은 해당 채널 정보가 얼마나 중요한지를 나타내는 지표가 됨     

&nbsp;

<div align="center">
  <img src="/assets/images/Net/36.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **SENet**    
  - 상술한 과정들은 논문에서는 위 그림과 같이 표현함    
  - 첫번째 연산인 $$F_{tr}$$은 transform을 의미하며 이는 일반적인 CNN 연산을 뜻함   
  - 이제 W x H x C size의 fmap에 대해 channel weighting을 수행할 것인데 본 논문에서는 해당 과정을 **Squeeze(정보 압축)**와 **Excitation(중요도 계산)**으로 나눠서 설명함     


&nbsp;

## 2. Squeeze    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/37.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Squeeze 단계**    
  - fmap의 채널별 정보를 '하나의 값'으로 압축해주는 단계    
  - W x H size의 정보를 어느 한 개의 값으로 표현하기 위해 **평균값**을 사용함, 즉 평균값으로 해당 채널을 대표하겠다는 의미이며, 이를 **Global Average Pooling**이라고 함   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/38.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Global Average Pooling**    
  - 위 자료는 Global Average Pooling의 수식    
  - $$u$$는 Input feature map, $$i$$와 $$j$$는 위치 좌표를 의미    
  - 즉 위 수식은 Squeeze 연산의 Output fmap ($$z$$)의 $$c$$번째 채널값은 Input fmap의 $$c$$번째 채널의 평균값과 동일함을 내포함      

&nbsp;

- **평균값을 사용하는 이유**    
  - W x H size 정보의 평균값은 정보의 위치에 대해서는 불변임    
  - 즉, 어떤 객체가 왼쪽에 있든 오른쪽에 있든 평균값으로 표현하면 동일한 종류인 이상 같은 값이 나오게 됨    
  - 결국 Squeeze 기능은 채널별 정보에서 위치 정보는 버리고 물체 정보만 남기는 역할로 해석 가능함    

&nbsp;

## 3. Excitation   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/39.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **중요도 계산 과정**    
  - 위 자료는 채널별 압축 정보(평균값)을 사용하여 채널별 중요도를 계산하는 과정을 묘사   
  - SENet에서는 해당 과정을 FC → ReLU → FC → Sigmoid 순서로 구성하여 처리함   
    - Fully Connected 연산만으로는 비선형성이 표현이 안되기 때문에 ReLU를 사용   
    - 최종 결과값을 0~1 사이의 값으로 표현하기 위해 Sigmoid로 사용    
    - 최종값이 0~1 사이의 값으로 나와야 채널별 중요도, 즉 **Attention Score** 사용 가능함   

&nbsp;

## 4. Entire Process in SENet   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/40.png" width="50%" height="50%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **전체 동작 과정**   
  - Input fmap이 SE Block으로 진입, 해당 fmap은 채널별로 분리되어 각 Attention Score로 나오게 됨    
  - 계산된 채널별 Attention Score는 다시 원래의 fmap에 곱해져 정보의 중요도만큼의 선택과 집중을 거치게 됨    

&nbsp;


