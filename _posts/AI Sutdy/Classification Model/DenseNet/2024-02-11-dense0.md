---
layout : single
title: "[DenseNet] DenseNet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

Densely Connected Convolutional Networks   

[참고 논문 링크](https://arxiv.org/abs/1608.06993)  

- Auguset 25 2016    
- [Gao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+G), [Zhuang Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+Z), [ Laurens van der Maaten](https://arxiv.org/search/cs?searchtype=author&query=van+der+Maaten,+L), [Kilian Q. Weinberger](https://arxiv.org/search/cs?searchtype=author&query=Weinberger,+K+Q)   

## 0. About DenseNet   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/26.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **DenseNet 특징**   
  - ResNet 이후로 모델들(Highway Network, FractalNet)을 살펴보면 I/O Layer에 Skip connection 구조를 차용한 경우가 많음    
    - 이를 통해 Vanishing gradient와 gradient exploding 문제가 해결되는 동시에 모델의 depth에 대한 제한이 풀렸기 때문   
    - DenseNet 또한 Skip connection을 통해 성능 향상을 이룸   
  - DenseNet은 하나의 stage 내에 있는 block들에 대해 모두 입력으로 사용하는데, 위 자료는 한 block 내의 모든 layer들을 보여줌    
    - 기존의 network의 깊이가 $$L$$이라고 가정하면, $$L$$개의 connection을 가졌지만, DenseNet은 $$L(L+1)/2$$ 개의 connection을 가짐   
    - 해당 구조 때문에 DenseNet은 거의 모든 정보가 network 내에서 모두 사용될 수 있음   
  - 또한 모든 layer와 연결되어 있기 때문에 global state를 확인할 수 있게 되었고 그 결과, feature map의 재사용이 가능함    
  - DenseNet의 구조는 기존 network 대비 적은 파라미터를 요구하고 필요없는 feature map을 다시 학습시킬 필요가 없음    

&nbsp;

## 1. ResNet 1202    

&nbsp;

- **About ResNet 1202**  
  - 기존의 ResNet 구조를 변형시켜 1202개의 layer까지 쌓은 모델로 ResNet 1202는 학습 도중에 무작위로 일부 layer를 버리는 구조를 사용함    
    - 이론 상으로는 layer를 버리게 되면 성능이 감소해야 하지만, 오히려 성능이 향상됨  
    - 이를 통해 모든 layer는 무조건 필요한 것이 아님과 동시에 network에 중복되는 layer가 존재함을 증빙됨    
  - 이로 인해 정보의 흐름 속에서 불필요한 layer로 인한 손실이 발생함    

&nbsp;

## 2. ResNet vs DenseNet   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/27.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ResNet**   
  - 위 식은 ResNet의 skip connection 식으로 $$X_n$$을 n번째 layer의 output이라고 가정할 때, N번째 layer는 N-1번째 layer와 연결됨    
  - 이때, add 연산으로 layer를 구성하므로 정보의 흐름에 방해가 됨    
  
&nbsp;

<div align="center">
  <img src="/assets/images/Net/28.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **DenseNet**   
  - 위 식은 DenseNet의 skip connection 식으로 n번째 layer는 0~n-1번째 layer 모두와 연결됨   
  - 이때, [...]는 Concatenate 연산을 의미   

&nbsp;

- **Add vs Concatenate**   
  - Add 연산의 경우, 두 결과의 값을 그냥 더하는 것으로 즉 input들에 대해서 채널과 이미지의 size가 같아야 함   
    - Ex) (400x400x3) + (400x400x3) → (400x400x3)   
  - Concatenate 연산의 경우, 두 이미지를 붙이는 연산이므로 즉 input들에 대해서 채널의 size는 상관 없이 이미지의 size만 맞으면 됨    
    - Ex) (400x400x3) + (400x400x15) → (400x400x18)   
  - 만약 add 연산 대신 concatenate 연산을 통해 skip connection을 구현하는 경우, add 연산은 input들의 값을 더해주는 방식이어서 feature들을 제대로 보존하지 못하지만, concatenate 연산은 채널들을 그대로 붙여주기 때문에 input들의 특징을 보존 가능함   

&nbsp;

## 3. DenseNet Architecture   

&nbsp;

- **Composition Function**   
  - 본 논문에서 $$H()$$는 Batch.Norm → ReLU → 3x3 Conv로 구성됨   
  - 해당 연산은 basic한 DenseNet에서만 사용   

&nbsp;

- **Pooling Layer**   
  - concatenate 연산은 채널의 수는 달라도 되지만, feature map의 size는 동일해야 함   
  - 본 논문에서는 transition layer를 구성하여 down sampling layer를 구현함   
  - Transition layer는 Dense Block 사이에 존재하며 다음과 같이 구성되어 있음   
    - **Conv Layer** : Batch.Norm → ReLU → 1x1 Conv 연산을 수행   
    - **Pooling Layer** : 2x2 Avg pooling 연산을 수행    

&nbsp;

- **Growth rate**   
  - $$H()$$ 연산이 K개의 feature map을 만들어내면 L번째 layer는 k0+k(L-1)개의 Input을 가짐   
    - 이때, k0는 첫 input layer의 입력 채널    
  - 여기서 k는 growth rate를 의미하며 이 또한 hyper parameter에 해당됨   
  - DenseNet은 이를 통해 좁은 layer를 가지고 있음에도 불구하고 높은 성능을 달성함   

&nbsp;

- **Bottleneck Layers**  
  - 비록 모든 layer가 k개의 feature map output을 낸다고 하여도 input은 더 많음   
  - 3x3 Conv 이전에 진행되는 1x1 Conv를 Bottleneck layer라고 하며 해당 layer는 3x3 Conv의 Input feature map의 size를 줄이는 역할을 수행   
    - 이를 통해 연산 효율성이 증가함   
  - 만약 $$H()$$ 함수를 Batch.Norm → ReLU → 1x1 Conv → Batch.Norm → ReLU → 3x3 Conv으로 정의한다면 이를 DenseNet-B라고 칭함   
    - 이때, 1x1 Conv 연산의 output feature map의 수는 4k에 해당함   

&nbsp;

- **Compression**   
  - 모델의 compactness를 향상하기 위해 Transition layer의 feature map의 수를 감소시킬 수도 있음    
  - Dense Block의 결과가 m개의 feature map을 가지고 있을 때, transition layer의 결과는 floor(θ*m)개의 feature map을 가짐   
    - 이때, θ = 1이라면 transition layer를 지난 feature map의 수는 변하지 않음   
    - θ < 1인 경우를 DenseNet-C라고 칭함   
    - Bottleneck 구조와 θ < 1을 동시에 만족한다면 DenseNet-BC라고 칭함   
  - 본 논문에서는 θ = 0.5로 지정하여 실험을 진행   

&nbsp;

## 4. More Detail about Architecture   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/29.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **DenseNet 상세 구조**   
  - 본 논문에서 Basic DenseNet은 3개의 Dense Block들을 사용하며 각 block들은 같은 수의 layer를 보유   
    - 3개의 Dense bolck들의 feature map size는 각각 32x32, 16x16, 8x8   
  - Basic DenseNet에서 첫 Dense block에 들어가기 전의 output 채널은 16이고, DenseNet-BC에서는 growth rate의 두 배에 해당하는 수임   
  - 모든 3x3 Conv 연산은 zero padding된 input들을 사용   
  - Transition block에서는 1x1 Conv 연산 이후, 2x2 Avg pooling을 진행   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/26.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Growth rate & 채널**   
  - 위 자료를 통해 채널 수에 대해 설명하면 다음과 같음   
    - 위 사진에서는 Input → x0 → h1 → x1 → h2 → x2 → h3 → x3 → h4 → x4 → transition 순서로 진행됨   
    - 이때, input과 x의 채널을 보면 각각 2 → 4 → 4 → 4 → 4 →4 로 나타남   
    - 각 $$H()$$의 입력을 보면 다음과 같음   
      - x0 = H(input) → 입력 채널의 수는 2   
      - x1 = H([input,x0]) → 입력 채널의 수는 6    
      - x2 = H([input,x0,x1]) → 입력 채널의 수는 10      
      - x3 = H([input,x0,x1,x2]) → 입력 채널의 수는 14      
      - x4 = H([input,x0,x1,x2,x3]) → 입력 채널의 수는 18         
  - 즉, 입력 채널의 수는 k0+k(L-1)에 해당됨   

&nbsp;

