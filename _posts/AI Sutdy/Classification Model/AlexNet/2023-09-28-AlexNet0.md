---
layout : single
title: "[AlexNet] AlexNet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

ImageNet classification with deep convolutional neural networks   

[참고 논문 링크](https://dl.acm.org/doi/10.1145/3065386)    

- [NeurIPS](https://neurips.cc/)   
- 24 May 2017 
- [Alex Krizhevsky](https://dl.acm.org/profile/81488667660), [Ilya Sutskever](https://dl.acm.org/profile/81388600728), [Geoffrey E. Hinton](https://dl.acm.org/profile/81100505762)   

## 0. About AlexNet     

&nbsp;

<div align="center">
  <img src="/assets/images/Net/30.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **AlexNet의 특징**   
  - 기존 논문들과는 달리 학습 속도를 가속시키면서 overfitting을 방지하고자 함   
  - 사용된 방법들은 다음과 같음   
    - ReLU Activation을 통해 Non-linear transformation을 사용   
    - GPU를 활용하여 병렬 연산을 수행    
    - Local Response Normalization을 사용하여 성능을 개선    
    - FC Layer에 Drop out 메소드 사용    

&nbsp;

## 1. AlexNet Mechanism   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/1.jpg" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ReLU Function**   
  - 기존의 tanh, sigmoid 등의 활성화 함수는 gradient descent를 사용해 학습하는 동안, 기울기가 소실되는 문제가 발생    
  - ReLU 함수를 사용할 경우, Vanishing gradient를 방지할 수 있는 동시에 모델의 표현력을 상승시킬 수 있음    

&nbsp;

- **Local Response Normalization**   
  - ReLU만으로도 충분한 학습이 되기도 하고 정규화가 꼭 요구되지는 않지만, 추가적인 성능 향상을 위해 정규화를 사용함   
  - 본 논문에서는 Local Normalization을 통해 Generalization을 구현하였으나, 모델 구현에서는 적용하지 않음   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/32.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Overlapping pooling**   
  - 기존의 Pooling layer들은 Pooling을 거칠 때 겹치는 부분이 없도록 연산을 수행함   
  - Pooling layer의 ZxZ size filter의 중간 좌표로부터 S 픽셀만큼 떨어진 위치로 filter의 center point를 옮길 때, S = Z 라고 가정한다면 이는 기존의 CNN에서 흔하게 사용되는 Pooling 연산과 다를게 없음    
    - 하지만, S < Z 라면, filter가 겹치게 되면서 Pooling 연산을 진행하게 됨    
    - 본 논문에서는 S = 2, Z = 3으로 실험을 진행하였고, 그 결과 Acc가 상승하는 동시에 overfitting이 덜 발생함    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/2.jpg" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Drop out**    
  - 학습을 수행하면서 특정 feature에 대해 학습 편향이 이루어져 다른 feature에 대해 제대로 학습되지 못하는 경우가 발생, 이는 overfitting으로 이어짐    
  - 이를 방지하기 위해 본 논문에서는 무작위로 특정 feature를 비활성화시켜 편향성을 개선하였으며, Drop out rate는 0.5로 설정하고 이는 처음 두 개의 FC Layer에 적용됨    

&nbsp;

## 2. AlexNet Architecture  

&nbsp;

<div align="center">
  <img src="/assets/images/Net/31.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

<div align="center">
  <img src="/assets/images/Net/24.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **AlexNet 구조**   
  - AlexNet의 구조는 총 8개의 Layer로 구성   
    - 1~5번째 layer는 Conv layer, 나머지 3개는 FC Layer로 구성   
  - 논문에서는 입력 이미지 size를 224x224x3이라고 하지만, 실제로는 227x227x3임에 주의   
  - 또한 논문에서는 두 개의 GPU를 병렬 연결하여 학습을 수행했지만, 현재의 GPU는 성능이 충분하므로 구현이 다름에 주의    

&nbsp;

- **AlexNet 구조 특징**   
  - 마지막 FC Layer는 1000개의 Class에 대한 Classification을 하기 위한 layer로 Softmax 함수를 적용   
  - Max Pooling Layer는 1/2/5 Layer의 뒤에 적용됨   
  - 모든 Conv Layer와 FC Layer 뒤에는 ReLU Activation이 적용됨    

&nbsp;

