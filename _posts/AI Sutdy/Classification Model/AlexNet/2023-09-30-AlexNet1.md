---
layout : single
title: "[AlexNet] AlexNet Implementation"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

AlexNet 논문 구현    

[참고 논문 링크](https://dl.acm.org/doi/10.1145/3065386)  

- 2012  
- [Alex Krizhevsky](https://dl.acm.org/profile/81488667660), [Ilya Sutskever](https://dl.acm.org/profile/81388600728), [Geoffrey E. Hinton](https://dl.acm.org/profile/81100505762)   

## 0. dataloader.py  

```python
import torch
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

import numpy as np 

def load_dataset():

    # Basic transform for CIFAR-10 dataset
    basic_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
    ])

    train_set = torchvision.datasets.CIFAR10(root='./cifar10', 
                                             train=True, download=True, 
                                             transform=basic_transform)
    test_set = torchvision.datasets.CIFAR10(root='./cifar10', 
                                            train=False, download=True, 
                                            transform=basic_transform)

    # Calculate mean/standard deviation per RGB channel
    train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_set]
    train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_set]
    train_meanR = np.mean([m[0] for m in train_meanRGB])
    train_meanG = np.mean([m[1] for m in train_meanRGB])
    train_meanB = np.mean([m[2] for m in train_meanRGB])
    train_stdR = np.mean([s[0] for s in train_stdRGB])
    train_stdG = np.mean([s[1] for s in train_stdRGB])
    train_stdB = np.mean([s[2] for s in train_stdRGB])

    # Normaize for RGB 3channel
    normalize_transform = transforms.Normalize(
        mean=[train_meanR, train_meanG, train_meanB],
        std=[ train_stdR,  train_stdG,  train_stdB]
    )

    train_transformer = transforms.Compose([
        transforms.Resize((32, 32)),    
        transforms.ToTensor(),
        normalize_transform
    ])
    test_transformer = train_transformer

    train_set.transform = train_transformer
    test_set.transform = test_transformer

    batch_size = 128
    trainloader = DataLoader(train_set, batch_size=batch_size, 
                             shuffle=True,  num_workers=2)
    testloader  = DataLoader(test_set,  batch_size=batch_size, 
                             shuffle=False, num_workers=2)

    return trainloader, testloader
```

&nbsp;

## 1. model.py   

```python
import torch.nn as nn

class AlexNet(nn.Module):
    def __init__(self, num_classes=10):
        super(AlexNet, self).__init__()
        # Input size : 227 x 227 x 3
        # Conv layer
        self.net = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0), # 55 x 55 x 96
            nn.ReLU(inplace=True),
            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2), # 27 x 27 x 96

            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2), # 27 x 27 x 256
            nn.ReLU(),
            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2), # 13 x 13 x 256

            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1), # 13 x 13 x 384
            nn.ReLU(),

            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1), # 13 x 13 x 384
            nn.ReLU(),

            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1), # 13 x 13 x 256
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2), # 6 x 6 x 256
        )

        # fc layer
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5, inplace=True),
            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),
            nn.ReLU(),
            nn.Dropout(p=0.5, inplace=True),
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(),
            nn.Linear(in_features=4096, out_features=num_classes),
        )

        # weight initialization
        self.init_weight()

    # define weight initialization function
    def init_weight(self):
        for layer in self.net:
            if isinstance(layer, nn.Conv2d):
                nn.init.normal_(layer.weight, mean=0, std=0.01)
                nn.init.constant_(layer.bias, 0)
        # in paper, initialize bias to 1 for conv2, 4, 5 layer
        nn.init.constant_(self.net[4].bias, 1)
        nn.init.constant_(self.net[10].bias, 1)
        nn.init.constant_(self.net[12].bias, 1)
    
    def forward(self,x):
        x = self.net(x)
        x = x.view(-1, 256 * 6* 6)
        x = self.classifier(x)
        return x
```

&nbsp;

## 2. train.py  

```python
import numpy as np 
import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.tensorboard import SummaryWriter

from model import AlexNet
from dataloader import load_dataset
from tqdm import tqdm

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data,target) in enumerate(train_loader):
        target = target.type(torch.LongTensor)
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 30 == 0:
            print(f"{batch_idx*len(data)}/{len(train_loader.dataset)}")

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target, reduction='mean').item()
            writer.add_scalar("Test Loss", test_loss, epoch)
            pred = output.argmax(1)
            correct += float((pred == target).sum())
            writer.add_scalar("Test Accuracy", correct, epoch)
            
        test_loss /= len(test_loader.dataset)
        correct /= len(test_loader.dataset)
        return test_loss, correct
        writer.close()

if __name__ == "__main__":

    num_epochs = 20
    learning_rate = 0.0001

    use_cuda = torch.cuda.is_available()
    print("use_cuda : ", use_cuda)
    device = torch.device("cuda:0" if use_cuda else "cpu")

    trainloader, testloader = load_dataset()

    model = AlexNet().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    writer = SummaryWriter("./Alexnet/tensorboard") 

    for epoch in tqdm(range(1, num_epochs + 1)):
        train(model, device, trainloader, optimizer, epoch)
        test_loss, test_accuracy = test(model, device, testloader)
        writer.add_scalar("Test Loss", test_loss, epoch)
        writer.add_scalar("Test Accuracy", test_accuracy, epoch)
        print(f"Processing Result = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
        writer.close()

                                 
    print(f"Result of AlexNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
```

**학습 결과**  
use_cuda :  True   
0%|          | 0/20 [00:00<?, ?it/s]0/50000   
3840/50000  
7680/50000  
11520/50000  
15360/50000  
19200/50000  
...  
46080/50000  
31200/50000    
100%|██████████| 20/20 [05:33<00:00, 16.66s/it]Processing Result = Epoch : 20   Loss : 0.004719122672080993   Accuracy : 0.8045   
Result of AlexNet = Epoch : 20   Loss : 0.004719122672080993   Accuracy : 0.8045    
{: .notice}   

&nbsp;


