---
layout : single
title: "[InceptionNet] InceptionNet V1. GoogLeNet Implementation"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

InceptionNet V1. GoogLeNet 논문 구현   

[참고 논문 링크](https://arxiv.org/abs/1409.4842)  

- September 17 2014 
- [Christian Szegedy](https://arxiv.org/search/cs?searchtype=author&query=Szegedy,+C), [Wei Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+W), [Yangqing Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+Y), [Pierre Sermanet](https://arxiv.org/search/cs?searchtype=author&query=Sermanet,+P), [Scott Reed](https://arxiv.org/search/cs?searchtype=author&query=Reed,+S), [Dragomir Anguelo](https://arxiv.org/search/cs?searchtype=author&query=Anguelov,+D), [Dumitru Erhan](https://arxiv.org/search/cs?searchtype=author&query=Erhan,+D), [Vincent Vanhoucke](https://arxiv.org/search/cs?searchtype=author&query=Vanhoucke,+V), [Andrew Rabinovich](https://arxiv.org/search/cs?searchtype=author&query=Rabinovich,+A)   

## 0. dataloader.py   

```python
import torch
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms

from torch.utils.data import DataLoader
import numpy as np


def load_dataset():
    train_set = torchvision.datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transforms.ToTensor())
    test_set = torchvision.datasets.CIFAR10(root='./cifar10', train=False, download=True, transform=transforms.ToTensor())

    batch_size = 128

    train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_set]
    train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_set]

    train_meanR = np.mean([m[0] for m in train_meanRGB])
    train_meanG = np.mean([m[1] for m in train_meanRGB])
    train_meanB = np.mean([m[2] for m in train_meanRGB])
    train_stdR = np.mean([s[0] for s in train_stdRGB])
    train_stdG = np.mean([s[1] for s in train_stdRGB])
    train_stdB = np.mean([s[2] for s in train_stdRGB])

    train_transformer = transforms.Compose([transforms.Resize((224,224)),
                                        transforms.ToTensor(),
                                        transforms.Normalize(mean=[train_meanR, train_meanG, train_meanB], 
                                                             std=[train_stdR, train_stdG, train_stdB])])

    train_set.transform = train_transformer
    test_set.transform = train_transformer

    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)
    testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=2)

    return trainloader, testloader
```

&nbsp;

## 1. model.py   
### 1-1. Conv Block module    

```python
import torch
import torch.nn as nn
from torch import Tensor
from typing import Optional

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(ConvBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs) 
        self.batchnorm = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        x = self.batchnorm(x)
        x = self.relu(x)
        
        return x
```

&nbsp;

### 1-2. Inception module   

```python
class Inception(nn.Module):
    def __init__(self, in_channels, n1, n3_reduce, n3, n5_reduce, n5, pool_proj):
        super().__init__()
        self.branch1 = ConvBlock(in_channels, n1, kernel_size=1, stride=1, padding=0)

        self.branch2 = nn.Sequential(
          ConvBlock(in_channels, n3_reduce, kernel_size=1, stride=1, padding=0),
          ConvBlock(n3_reduce, n3, kernel_size=3, stride=1, padding=1)
        )
        
        self.branch3 = nn.Sequential(
          ConvBlock(in_channels, n5_reduce, kernel_size=1, stride=1, padding=0),
          ConvBlock(n5_reduce, n5, kernel_size=5, stride=1, padding=2)
        )

        self.branch4 = nn.Sequential(
          nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
          ConvBlock(in_channels, pool_proj, kernel_size=1, stride=1, padding=0)
        )
        
    def forward(self, x):
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        x4 = self.branch4(x)

        return torch.cat([x1, x2, x3, x4], dim=1)
```

&nbsp;

### 1-3. InceptionAux Module   

```python
class InceptionAux(nn.Module):
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.avg_conv = nn.Sequential(
          nn.AvgPool2d(kernel_size=5, stride=3),
          ConvBlock(in_channels, 128, kernel_size=1, stride=1, padding=0)
        )

        self.fc = nn.Sequential(
          nn.Linear(2048, 1024),
          nn.ReLU(),
          nn.Dropout(p=0.7),
          nn.Linear(1024, num_classes)
        )

    def forward(self, x):
        x = self.avg_conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        
        return x
```

### 1-4. GooLeNet Model   

```python
class GoogLeNet(nn.Module):
    def __init__(self, aux_logits=True, num_classes=10):
        super().__init__()

        self.aux_logits = aux_logits

        self.conv1 = ConvBlock(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)
        self.conv2 = ConvBlock(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)
        self.conv3 = ConvBlock(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)
        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)
        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)
        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)
        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)
        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)
        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)
        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)
        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)
        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)
        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)
        self.dropout = nn.Dropout(p=0.4)
        self.linear = nn.Linear(1024, num_classes)

        if self.aux_logits:
            self.aux1 = InceptionAux(512, num_classes)
            self.aux2 = InceptionAux(528, num_classes)
        else:
            self.aux1 = None
            self.aux2 = None
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.maxpool2(x)
        x = self.a3(x)
        x = self.b3(x)
        x = self.maxpool3(x)
        x = self.a4(x)
        
        aux1 = None
        if self.aux_logits and self.training:
            aux1 = self.aux1(x)

        x = self.b4(x)
        x = self.c4(x)
        x = self.d4(x)
        
        aux2 = None
        if self.aux_logits and self.training:
            aux2 = self.aux2(x)

        x = self.e4(x)
        x = self.maxpool4(x)
        x = self.a5(x)
        x = self.b5(x)
        x = self.avgpool(x)
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        x = self.dropout(x)

        if self.aux_logits and self.training:
            return [x, aux1, aux2]
        else:
            return x
```

&nbsp;

## 2. train.py

```python
import numpy as np 
import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.tensorboard import SummaryWriter

from model import GoogLeNet
from dataloader import load_dataset
from tqdm import tqdm

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data,target) in enumerate(train_loader):
        target = target.type(torch.LongTensor)
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 30 == 0:
            print(f"{batch_idx*len(data)}/{len(train_loader.dataset)}")

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target, reduction='mean').item()
            writer.add_scalar("Test Loss", test_loss, epoch)
            pred = output.argmax(1)
            correct += float((pred == target).sum())
            writer.add_scalar("Test Accuracy", correct, epoch)
            
        test_loss /= len(test_loader.dataset)
        correct /= len(test_loader.dataset)
        return test_loss, correct
        writer.close()

if __name__ == "__main__":

    num_epochs = 20
    learning_rate = 0.0001

    use_cuda = torch.cuda.is_available()
    print("use_cuda : ", use_cuda)
    device = torch.device("cuda:0" if use_cuda else "cpu")

    trainloader, testloader = load_dataset()

    model = GoogLeNet().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    writer = SummaryWriter("./googlenet/tensorboard") 

    for epoch in tqdm(range(1, num_epochs + 1)):
        train(model, device, trainloader, optimizer, epoch)
        test_loss, test_accuracy = test(model, device, testloader)
        writer.add_scalar("Test Loss", test_loss, epoch)
        writer.add_scalar("Test Accuracy", test_accuracy, epoch)
        print(f"Processing Result = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
        writer.close()

                                 
    print(f"Result of GoogLeNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}")
```

**학습 결과**   
use_cuda :  True   
0%|          | 0/20 [00:00<?, ?it/s]0/50000   
3840/50000   
7680/50000   
11520/50000   
15360/50000  
19200/50000  
23040/50000  
...  
42240/50000   
46080/50000  
31200/50000  
100%|██████████| 20/20 [1:16:37<00:00, 229.90s/it]Processing Result = Epoch : 20   Loss : 0.004977113389968872   Accuracy : 0.8405  
Result of GoogLeNet = Epoch : 20   Loss : 0.004977113389968872   Accuracy : 0.8405   
{: .notice}  

&nbsp;

