---
layout : single
title: "[ResNet] ResNet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

Deep Residual Learning for Image Recognition   

[참고 논문 링크](https://arxiv.org/abs/1512.03385)  

- [CVPR](https://cvpr.thecvf.com/)   
- 10 December 2015    
- [Kaiming He](https://arxiv.org/search/cs?searchtype=author&query=He,+K), [Xiangyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+X), [Shaoqing Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren,+S), [Jian Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+J)   

## 0. Introduction   

&nbsp;

- **Model Depth의 문제점**   
  - 모델의 Layer 수가 증가할수록 학습할 수 있는 Feature가 달라지기에 모델의 깊이는 중요한 요소  
  - 다만, 모델의 깊이가 깊어질수록 Vanishg Gradient 등의 이슈가 발생할 수 있음   
    - 이를 해결하여 converge가 되었다고 해도 overfitting에 의해 성능이 악화되는 경우도 있음   
    - 또한 모델의 Layer가 추가되면서 Training error가 커지는 이슈도 존재    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/11.png" width="70%" height="70%" alt=""/>
  <p><em>Skip Connection</em></p>
</div>

&nbsp;

- **Skip Conncection**   
  - 상기한 문제를 해결하기 위해 본 논문에서는 **Deep Residual Learning Framework**를 이용하고자 함   
  - **Residual Mapping**을 시도하는 것으로, 원래의 Underlying Mapping에 Input인 $$x$$를 더하는 것으로 이를 **Skip Connection**이라고 정의   
  - $$x$$는 Indentity Mapping을 거치고, 이후 나온 결과를 $$F(x)$$에 더해주는 것   
    - 이는 연산량이나 Parameter 수를 증가시키지 않는다는 장점이 있음   

&nbsp;

- **Skip Connection with ImageNet**  
  - Skip connection 기법을 ImageNet에 실험한 결과, 다음과 같은 효과를 검증함   
    - **Deep Residual Networks는 Optimize하기 쉬움**   
    - **Depth가 증가하면서 Acc의 증가를 얻음**   
  - Optimize가 잘 되는 것을 확인하기 위해 100~1000개의 Layer에 대해서 실험을 수행, 152개의 Layer로 구성된 모델이 VGGNet보다 정확도가 훨씬 높으면서 복잡도가 낮은 것을 확인함    

&nbsp;

## 1. Related Work   

&nbsp;

- **Residual Representations**   
  - Image Recognition에서 **VLAD**는 Residual vector를 이용하여 Encoding하는 방식   
  - Low-level vision과 Graphics 분야에서 편미분 방정식을 풀기 위해 여러 개의 Scale로 Sub-problem을 재구성하는 Multigrid Method를 사용    
    - 여기서 여러 Scale을 연결하기 위해 Residual vector를 사용   

&nbsp;

- **Skip Connections**   
  - 이전 연구에서도 MLP에서 Linear layer에 Input과 Output을 연결해주는 방법은 존재했음   
    - Auxiliary Classifier를 이용하여 Vanishing Gradients를 해결   
    - Shortcut branch로 구성된 Inception module    
  - 다만 위 방식들은 파라미터 수를 증가시키는 반면, 본 논문에서 제안한 Identity Shortcut은 별도의 파라미터를 요구하지 않음     

&nbsp;

## 2. Deep Residual Learning    
### 2-1. Residual Learning   

&nbsp;

- **Residual Learning**   
  - Nonlinear한 Layer가 축적된 복잡한 함수를 Approximate하는 과정   
    - 이는 $$H(x)-x$$와 같은 Residual Function을 Approximate하는 과정으로 해석 가능(이때, $$H(x)$$는 몇개의 Layer에 대한 Underlying Mapping, $$x$$는 입력값으로 가정)
    - 이때, Residual Function을 $$F(x)=H(x)-x$$로 가정하면, $$H(x)$$는 $$F(x)+x$$로 정의됨    
    - 따라서 Residual Function에 입력을 더하는 것을 Approximate하는 과정으로 귀결됨   

&nbsp;

- **Identity Mapping 적합성**  
  - 만약 Identity Mapping이 적합하다면 기존의 여러 개의 Non-linear layer의 weight를 모두 0으로 만들고 Identity Mapping이 되도록 함  
    - 즉, 기존의 Conv layer를 따라서 입력이 전달되는 것이 아니라 새로 추가된 Skip connection을 타고 Identity mapping이 적용된 결과가 output으로 나오는 것   
  - 사실 현실적으로 Identity mapping이 적합한 경우는 많지 않음   
    - 하지만, 이러한 방법은 전제 조건을 주는 것임   
    - 적합한 function이 Identity function과 유사할 때, 즉 Zero mapping을 하는 것이 아닐 때, 작은 변화에 더 쉽게 반응하도록 유도하는 것    

&nbsp;

### 2-2. Identity Mapping by Shortcuts  

&nbsp;

<div align= 'center'>
    $$y = F(x, {W_i}) + x$$
</div>

&nbsp;

- **Residual Block**   
  - 본 논문에서는 Residual Block의 정의를 위와 같이 함   
  - 여기서 $$x$$와 $$y$$는 각각 입력과 출력이고, 함수 $$F$$는 Residual Mapping function   
  - $$F + x$$는 Element-wise addition을 사용   
    - 즉, 각각의 element끼리 더한 후, Nonlinear function을 거치는 것   
  - 이러한 Skip connection은 파라미터 수와 계산 복잡도를 증가시키지 않는다는 장점이 있음, 이는 Plain network와의 가장 큰 차이점   
  - 또한, Identity mapping에 Linear Projection을 추가할 수 있음  
    - 이는 $$x$$의 Dimension과 $$F$$를 거친 결과의 Dimension이 다를 수 있기 때문   

&nbsp;

<div align= 'center'>
    $$y = F(x, {W_i}) + W_sx$$
</div>

&nbsp;

- **Linear Projection Matrix**   
  - $$W_s$$는 Linear Projection에 사용되는 Matrix   
  - $$F$$는 여러 개의 Layer가 사용될 수 있지만, 만약 한 개의 Layer가 사용될 경우에는 이점을 얻기 어려움   
    - 이는 Layer에 대한 식이 $$y = W_1x + x$$로 정의되기 때문인데, 이러면 $$y = (W_1+1)x$$와 같으므로 그저 weight 값에 1을 더한 Linear layer로 취급되기 때문   

&nbsp;

### 2-3. Network Architectures   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/12.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Plain Network**   
  - Baseline으로는 VGGNet에서 영감을 받은 34개의 Layer를 가진 모델을 준비   
  - 대부분의 경우, 3x3 filter로 이루어져 있으며, Feature Map output size는 동일   
    - 이는 Filter의 dimension이 두배가 될 때, Feature map size가 절반이 됨   
    - 이를 통해 Layer당 복잡도를 유지할 수 있음   
  - Stride 2를 가진 Conv를 이용해 Downsampling을 진행   
    - 이것만 해도 VGGNet보다 복잡도도 낮고 파라미터 수도 줄어듦   

&nbsp;

- **Residual Network**   
  - Plain Network 기반에 Skip connection이 추가된 구조   
  - I/O가 같은 Dimension이면 더하는데 문제가 없지만, Conv 결과가 Dimension이 2배 증가하였을 때는 두 가지 옵션이 존재   
    - Identity Mapping은 그대로 진행하고, 나머지 Dimension을 0으로 채우는 방법   
    - Projection Shortcut을 사용하는 방법   
    - 두 가지 옵션 모두 Feature map size를 맞추기 위해 Stride를 2로 설정   

&nbsp;

## 3. Experiments   
### 3-1. ImageNet Classification   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/13.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Plain Network**  
  - 위 자료는 18개 Layer로 이루어진 모델과 34개 Layer로 구성된 모델의 구조를 보여줌   
  - 34 layer plain 모델은 18 layer model보다 높은 Valid Error를 보임    
    - 이는 위 자료를 확인하면 성능 악화 문제가 발생했음을 알 수 있음   
  - 전체 학습과정에서는 34 layer model이 Training error가 높은 것을 확인 가능   
    - 이를 통해 Vanishing Gradients로 인해 Opitimization이 어렵다는 것을 알 수 있음   
    - 본 논문에서는 더 깊은 Plain network가 아주 작은 convergence rates를 가져서 Training Error가 줄어드는 것이 약하다고 추측함   
  
&nbsp;

<div align="center">
  <img src="/assets/images/Net/14.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Residual Network**  
  - Plain network와 동일하게 18 layer와 34 layer 모델을 비교  
  - Identity mapping 시, Dimension을 늘리기 위해 Zero-padding을 사용    
  - 위 자료를 보면, 34 layer 모델이 18 layer 모델보다 error가 작은 것을 확인 가능  
    - 이를 통해 모델이 깊어지면서 발생하는 성능 악화 문제는 해결되었다고 판단됨   
  - ResNet이 Plain Network보다 3.5% 낮은 error를 보임   
    - 이를 통해 Residual Learning의 효과를 검증 가능   
  - 마지막으로, Resnet이 Plain Network보다 초반 단계에서 빠르게 Convergence되는 것을 확인 가능함   

&nbsp;

- **Identity vs Projection Shortcuts**   
  - 지금까지는 Dimension 증가를 위해 Zero padding을 사용했지만, 이번에는 Projection을 사용, 3가지 경우로 나눔    
    - Zero Paddning   
    - Projection   
    - 모든 Shortcut을 Projection   
  - 위 자료를 보면 모든 경우에서 Plain Network보다 성능이 좋으며, C >> B > A 순으로 성능이 좋은 것을 알 수 있음   
    - 이는 A의 경우 Residual Learning을 하지 않아서이며, C의 경우 Extra parameter가 Projection Shortcut에 형성되었기 때문   
  - 본 논문에서는 속도와 모델 복잡도가 큰 C 옵션을 굳이 사용하지 않음   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/16.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Deeper Bottleneck**   
  - 모델을 더 깊게 만들고 싶지만, 학습 시간이 오래 걸린다는 문제가 있음   
    - 이를 충당하기 위해, Bottleneck 구조를 제안    
  - 각 Residual function마다 2개의 Layer 대신 3개의 Layer를 사용하였고, 이는 각각 1x1, 3x3, 1x1 Conv로 이루어져 있음    
    - 1x1 Layer는 3x3 Layer의 I/O Dim을 작게 만들기 위해 Dim을 줄였다가 키우기 위함   
  - 위 자료를 보면 Layer가 1개 더 많음에도 불구하고 Time complexity는 둘이 비슷한 것을 확인 가능함    
  - Bottleneck 구조에서 Identity shortcut은 중요한 역할   
    - 만약 Identity shortcut이 Projcetion으로 바뀐다면 Time complexity와 model size는 두배가 됨   
    - 따라서, Identity shortcut은 효율적인 Bottleneck 구조를 위해 반드시 필요함   

&nbsp;

### 3-2. CIFAR-10 Analysis   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/17.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
  <img src="/assets/images/Net/18.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **CIFAR-10 Experiments**   
  - 상기한 34 layer 모델과 비슷한 구조로 모델을 구성, 구조는 다음과 같음   
    - 첫 번째 Layer : 3x3  
    - 32x32 Ouput size에 대해서는 1+2n개의 Layer를 쌓음   
    - 16x16 Ouput size에 대해서는 2n개   
    - 8x8 Ouput size에 대해서는 2n개   
  - Skip connection이 사용될 때에는 3x3 Layer로 연결    
    - 이때, n을 3/5/7/9일 때로 나눠서 성능을 실험, 각각 20/32/44/56개의 Layer로 구성된 모델이 도출됨    
    - 위 자료를 보면 ResNet은 모델이 깊어질수록 더 좋은 성능을 보이는 것을 확인 가능함   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/19.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Analysis of Layer Responses**   
  - 해당 파트는 표준 편차를 활용하여 Layer Response를 확인하는 것으로 각 3x3 layer의 output을 확인    
  - 이를 ResNet에 적용하였을 때, ResNet은 일반적으로 Plain Net보다 작은 Response를 보임   
    - 이는 Non-residual function보다 residual function이 0에 더 가깝다는 것으로 해석됨    
    - 즉, Identity mapping이 되도록 weight가 0이 되게끔 학습이 되었다는 것을 의미   
  - 또한, 깊은 모델일수록 Response의 크기가 감소한 것을 확인 가능    
    - 이는 Layer가 증가할수록 신호의 변화가 줄어드는 경향성을 보인다고 판단됨   

&nbsp;

