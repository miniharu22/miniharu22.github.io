---
layout : single
title: "[EfficientNet] EfficientNet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

Rethinking Model Scaling for Convolutional Neural Networks

[참고 논문 링크](https://arxiv.org/abs/1905.11946)

- May 28 2019 
- [Mingxing Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+M), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le,+Q+V)   

## 0. Models Before EfficientNet   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/41.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **기존 아키텍쳐의 특징**    
  - EfficientNet 이전의 CNN 모델은 네트워크의 Width(Filter의 수), Depth(Layer의 수), Resolution(fmap의 크기) 중 하나에만 집중하는 경향을 보임   
    - 예를 들면 Depth engineering에 집중한 ResNet 등등..   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/42.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Width**   
  - Width는 각 Feature map의 채널 수를 의미   
  - 위 자료는 Width의 증가에 따른 ImageNet 학습 성능을 표시한 그래프로, Width가 증가함에 따라 모델의 performance가 향상됨     
    - Width의 증가는 곧 fmap이 더 많은 채널 정보를 포함하고 있음을 의미하고 이는 더 많은 종류의 정보를 보유함을 의미하기에 모델의 성능 또한 향상됨     
    - 이를 이용한 것이 ResNet의 Width를 증가시킨 Wide ResNet   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/43.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Depth**   
  - Depth는 모델의 깊이, 즉 Layer의 개수를 의미    
  - 위 자료는 Depth의 증가에 따른 ImageNet 학습 성능을 표시한 그래프로, Depth가 증가함에 따라 모델의 performance가 향상됨    
    - Layer 개수가 증가할수록 모델의 연산량이 증가하기에 결과적으로 성능 또한 향상될 수 있음, 이를 증명한 것이 바로 [VGGNet](https://miniharu22.github.io/classification%20model/VGGNet0/)    
    - 다만, 일정 깊이 이상이 되면 성능이 포화되는데 이는 Overfitting에 기인하며 이를 해결하기 위해 제시된 방식이 ResNet과 같은 Depth engineering 모델   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/44.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Resolution**   
  - Resolution(해상도)는 Input Image와 Feature map의 해상도, 즉 Width x Height size를 의미    
  - 위 자료는 Resolution의 증가에 따른 ImageNet 학습 성능을 표시한 그래프로, Resolution이 증가함에 따라 모델의 performance가 향상됨    
    - Resolution의 증가는 모델에 입력되는 정보의 양이 많아짐을 의미하기에 이는 당연한 결과    

&nbsp;

## 1. About EfficientNet    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/45.png" width="80%" height="80%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **EfficientNet의 초점**   
  - Width, Depth 그리고 Resolution이 증가함에 따라 Network의 성능이 향상됨을 확인했으며 대부분의 모델들은 각 변수들에 초점을 맞춰 Scaling하는 연구를 동반했음   
  - 다만, 위 세 가지 변수들을 모두 고려한 연구가 수행된 적이 없다는 것이 본 논문에서 저자가 지적한 부분    
  - 위 자료는 논문에서 표현하고 있는 Width, Depth, Resolution 변수들에 대한 것으로, 마지막에 Compound Scaling 방식이 묘사되어 있는데 이는 세 가지 변수들을 동시에 증가시키는 방법임     
    - **즉, EfficienNet은 Compound Scaling을 통해 세 가지 변수를 동시에 고려하여 모델의 성능 향상을 달성하는 것을 목표로 함**   

&nbsp;

## 2. Compound Scale Up   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/46.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Depth vs FLOPS**   
  - 위 그림은 Depth의 증가에 따른 FLOPS의 변화를 표현   
  - Depth가 𝛼배 증가되었을 때, 즉 Conv Layer가 𝛼배 증가하면 FLOPS 또한 𝛼배 증가한 것을 알 수 있음    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/47.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Width vs FLOPS**   
  - 위 그림은 Width의 증가에 따른 FLOPS의 변화를 표현   
  - Width가 𝛽배 증가되었을 때, 즉 fmap의 채널 수가 𝛽배 증가하면 두번째 fmap의 채널 수는 $$𝛽^2$$만큼 증가함    
    - FLOPS 또한 마찬가지로 각각 𝛽배와 $$𝛽^2$$배로 증가한 것을 알 수 있음    
    - 따라서 Width가 𝛽배 증가하면 FLOPS는 $$𝛽^2$$만큼 증가   

&nbsp;

<div align="center">
  <img src="/assets/images/Net/48.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Resolution vs FLOPS**    
  - 위 그림은 Resolution의 증가에 따른 FLOPS의 변화를 표현   
  - Resolution이 𝛾배 증가했을 때, 즉 Width와 Height가 각각 𝛾배 증가하면 fmap의 size는 $$𝛾^2$$배 증가함    
    - 이에 따라 FLOPS는 $$𝛾^2$$만큼 증가    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/49.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Compound Scaling**    
  - 종합하면 FLOPS는 Depth, Width^2, Resolution^2에 비례, 이를 수식으로 표현하면 위와 같음    
  - Depth는 가중치 𝛼에, Width는 가중치 𝛽에, Resolution은 가중치 𝛾에 비례한다고 가정할 때, 𝛼 x $$𝛽^2$$ x $$𝛾^2$$는 일정한 값을 유지해야 함(FLOPS를 일정하게 유지하기 위해)   
    - 해당 상태에서 최적의 𝛼, 𝛽, 𝛾를 구하는 것이 관건인데, 본 논문에서는 **Small Grid Search**로 이를 수행함    
    - 이는 𝛼 x $$𝛽^2$$ x $$𝛾^2$$를 만족하는 다양한 경우의 𝛼, 𝛽, 𝛾 조합들로 일일이 모델을 만들어 성능을 측정하면서 최종적으로 가장 performance가 우수한 조합을 선택하는 방식    

&nbsp;

## 3. How to find Most Efficient Network    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/50.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Baseline Model 구축**    
  - Compound Scale Up을 적용하기 전 Baseline 모델을 만들어야 함   
    - 본 논문에서는 이를 위해 **Neural Architecture Search(NAS)** 방법을 사용   
    - 이는 쉽게 말해 딥러닝을 사용하여 딥러닝 모델을 찾는 것을 의미   
  - **NAS**   
    - 위 그림은 NAS를 사용하여 Baseline 모델을 찾는 과정을 표현   
    - NASNet 이라는 딥러닝 모델이 있다고 가정할 때, 해당 모델은 Output으로 모델의 구조를 출력함 (예를 들면 Conv Block 구조, Attention 기능 구현 유무, Model Depth 등등)    

&nbsp;

- **Baseline Model 성능 측정**   
  - NASNet의 결과를 토대로 구현한 모델을 **Baseline Model**이라고 가정할 때, 해당 모델로 성능을 측정해야 함    
  - 이때, 모델의 성능(Accuracy)와 연산량(FLOPS)가 나올텐데, EfficientNet을 위해서는 연산량 대비 성능이 가장 좋은 모델을 찾는게 목표이며 이는 NASNet의 Loss Function으로 설정    
  - 이후, NASNet의 학습을 반복하며 최종적으로 얻은 모델을 Baseline Model로 지정   


&nbsp;

<div align="center">
  <img src="/assets/images/Net/51.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Compound Scale Up**   
  - Baseline model에 Small Grid Search 방식을 적용하여 최적의 𝛼, 𝛽, 𝛾 조합을 찾은 결과는 위 그림과 같음    
  - 다양한 𝛼, 𝛽, 𝛾 조합에서 𝛼=1.2, 𝛽=1.1, 𝛾=1.5일 때, 연산량 대비 성능이 가장 우수하다는 것을 알았다면 이제 이를 고려하여 최종적인 Network를 구현, 이를 **EfficientNet-B0**이라고 정의    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/52.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
  <img src="/assets/images/Net/53.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Scaling Up EfficientNet**   
  - 첫번째 자료는 EfficientNet-B0의 구성을 나타낸 표로 앞쪽의 Operator에 해당하는 부분은 NAS로 찾고, 뒤쪽의 Detph/Width/Resolution 부분은 Small Grid Search로 찾음   
  - 기본적인 EfficientNet의 틀이 정해졌다면 𝛼, 𝛽, 𝛾의 값을 고정한 상태에서 Scale만 키우주면 더 큰 규모의 EfficientNet을 만들 수 있음      
    - 두번째 자료를 보면 𝛼, 𝛽, 𝛾의 값은 고정한 상태에서 제곱값만 순차적으로 증가시켜 가며 Scale Up을 진행하는데, 이렇게 나온 모델들을 **EfficientNet-B1 ~ B7**이라고 정의함     

&nbsp;


