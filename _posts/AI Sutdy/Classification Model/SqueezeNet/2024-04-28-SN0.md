---
layout : single
title: "[SqueezeNet] SqueezeNet Paper Review"
categories: 
  - Classification Model 
toc: true
toc_sticky: true
use_math: true
---

SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size

[참고 논문 링크](https://arxiv.org/abs/1602.07360)

- Feburary 24 2016 
- [Forrest N. Iandola](https://arxiv.org/search/cs?searchtype=author&query=Iandola,+F+N), [Song Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+S), [Matthew W. Moskewicz](https://arxiv.org/search/cs?searchtype=author&query=Moskewicz,+M+W), [Khalid Ashraf](https://arxiv.org/search/cs?searchtype=author&query=Ashraf,+K), [William J. Dally](https://arxiv.org/search/cs?searchtype=author&query=Dally,+W+J), [Kurt Keutzer](https://arxiv.org/search/cs?searchtype=author&query=Keutzer,+K)   


## 0. About SqueezeNet    

&nbsp;

- **CNN 경량화의 장점**    
  - 경량화된 네트워크는 분산 학습 때 서버와 덜 통신함   
  - 클라우드 상에서 동작할 때 통신 대역폭을 덜 요구함   
  - FPGA와 같은 한정된 하드웨어에서 동작 가능함    

&nbsp;

- **SqueezeNet 요약**   
  - 상술했듯이 동일한 Accuracy를 갖춘 모델에 대해서, 더 적은 파라미터를 가질수록 장점을 갖출 수 있음   
  - 실제로 SqueezeNet은 AlexNet과 비교하여 50배 적은 파라미터 수를 가지면서도 성능 면에서는 동일한 수준을 보임    
  - 또한 Pruning이나 Quantization 등 Model Compression 테크닉을 활용해 AlexNet보다 510배 작은 크기인 0.5MB로 압축에 성공함    

&nbsp;

## 1. Network Structure    

&nbsp;

- **설계 전략**    
  - **3x3 Filter를 1x1 Filter로 대체**      
    - 파라미터 수를 9배 절약    
  - **3x3 Filter의 Input channel 수 감소**    
    - 3x3 Filter의 개수 뿐만 아니라 Input channel도 줄여 파라미터 수를 줄임    
    - Conv layer의 파라미터 수 공식 : (kernel)^2 x (N of Input channel) x (N of Filter)   
  - **Conv layer가 큰 size의 activation map을 갖도록 Downsampling을 늦게 수행**   
    - 일반적인 CNN은 Pooling으로 Downsampling하면서 이미지의 정보를 압축해감   
    - 보다 넓은 activation map을 가지고 있을수록 정보 압축에 의한 손실이 적어 성능이 높음    
    - 이를 위해 Network 후반부에 Downsampling을 수행     
  - 첫번째와 두번째 전략은 Accuracy를 유지하면서 파라미터 수를 줄이는 방법이며 세번째 전략은 제한된 파라미터 수에서 정확도를 최대한 높이는 방법    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/54.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Fire Module**   
  - 상술한 설계 전략을 바탕으로 SqueezeNet을 구성하는 Block    
  - 1x1 Conv를 활용하는 **Squeeze Conv layer**와 1x1/3x3 Conv를 함께 사용하는 **Expand Conv layer**로 구성됨     
  - Fire module은 각 Conv filter의 개수를 조절해 가면서의 모듈의 size를 조절할 수 있는 세 개의 hyper parameter를 제안함    
    - $$s_{1 \text{x} 1}$$ : Squeeze layer에서 1x1 Conv filter의 개수    
    - $$e_{1 \text{x} 1}$$ : Expand layer에서 1x1 Conv filter의 개수   
    - $$e_{3 \text{x} 3}$$ : Expand layer에서 3x3 Conv filter의 개수    
  - Fire module을 설계할 때는 위 두번째 전략(Input channel 수 감소)를 적용하기 위해 $$s_{1 \text{x} 1} < (e_{1 \text{x} 1} + e_{3 \text{x} 3})$$ 수식을 설정   
    - 해당 수식에 의해 Squeeze layer의 filter 수가 expand layer의 그것보다 크지 않도록 제한하여 전체 filter의 개수를 제한함   

&nbsp;

## 2. SqueezeNet Structure    

&nbsp;

<div align="center">
  <img src="/assets/images/Net/55.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **SqueezeNet의 구조**   
  - 위 자료는 SqueezeNet의 구조를 표현한 것으로 왼쪽이 Base 모델, 오른쪽은 bypass를 적용한 모델에 해당함    
  - 1개의 Conv layer로 시작, 이후 8개의 Fire module을 통과한 후, 마지막으로 Conv layer와 Softmax를 거쳐 결과를 출력     
  - Max pooling을 활용해 Resolution을 줄여나가며 마지막에는 Average pooling을 활용해 Output size를 조절     

&nbsp;

<div align="center">
  <img src="/assets/images/Net/56.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **SqueezeNet의 상세 정보**    
  - Suqeeze layer와 Expand layer의 output은 동일한 Resolution을 가짐    
    - 이를 위해 expand의 3x3 filter의 입력에는 1 pixel의 padding을 추가함    
  - ReLU Activation 사용    
  - 마지막 Fire module에는 50%의 Dropout을 적용    
  - Expand layer에서 1x1 filter와 3x3 filter는 나눠서 구현    
    - Caffe는 expand layer처럼 여러 size의 filter를 동시에 사용하는 것을 지원하지 않음   
    - 따라서 GoogLeNet과 유사하게 병렬로 연결된 Conv layer의 output을 concate하는 방식으로 구현     

&nbsp;

