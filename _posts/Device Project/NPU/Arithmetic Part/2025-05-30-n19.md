---
layout : single
title: "[Study] Arithmetic Part & Core"
categories: 
  - Universal NPU-CNN Accelarator
toc: true
toc_sticky: true
use_math: true
---

연산을 담당하는 Arithmetic 계층 구조에 대한 고찰    

## 0. Arithmetic Part & Core    

&nbsp;

<div align="left">
  <img src="/assets/images/npu/15.jpg" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Arithmetic Hierachy**   
  - 현재 고안 중인 연산 파트의 계층 구조는 위와 같음   
  - Arithmetic Part는 여러개의 동일한 Arithmetic Core로 구성되어 있으며, 각 Core들은 같은 Input feature map을 공유하지만, 각기 다른 weight를 입력받음    
  - **즉, 각 Core들은 동시에 여러 채널을 연산하며, 이는 각기 다른 Filter를 통과시키는 것과 동일**    

&nbsp;

## 1. Core with ReLU & Maxpooling   

&nbsp;

<div align="left">
  <img src="/assets/images/npu/17.jpg" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Arithmetic Core 구조도**   
  - Arithmetic Core의 대략적인 구조는 위와 같이 구상 중    
  - 임의의 Weight를 가지고 있는 Processing Element에서 Convolution 연산을 수행    
  - 연산된 output을 ReLU 모듈을 통과시켜 활성화 함수 연산을 수행    
  - ReLU 모듈의 output을 Maxpooling 모듈에 입력시켜 Pooling 연산을 수행     
  - 다만, Processing Element는 누적 합산을 위해 2번의 Flip Flop을 사용하므로 ReLU와 Maxpooling 모듈 또한 2clk 이후에 연산을 하게끔 Flip Flop을 추가    


&nbsp;

<div align="left">
  <img src="/assets/images/npu/105.png" width="40%" height="40%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **ReLU 모듈**   
  - ReLU 함수에 대한 자세한 설명은 [[Generalization] Vanishing Gradient & ReLU](https://miniharu22.github.io/deep%20learning%20basic/d18/)을 참고    
  - **동작 방식**     
    - `Relu_en`과 `input[7]`이 둘 다 `1`인 경우에만 무조건 `0`을 출력    
    - 이외의 경우에는 `input[n]`이 그대로 `output[n]`으로 출력    
    - `Relu_en`은 ReLU 모듈의 사용 여부를 결정하는 신호    

&nbsp;

<div align="center">
  <img src="/assets/images/npu/18.jpg" width="50%" height="50%" alt=""/>
  <p><em>Maxpooling : 기존 아키텍쳐</em></p>
</div>

<div align="center">
  <img src="/assets/images/npu/18.jpg" width="50%" height="50%" alt=""/>
  <p><em>Maxpooling : 안정화 아키텍쳐(대면적)</em></p>
</div>

&nbsp;

- **Maxpooling 모듈**    
  - 현재 구상중인 maxpooling은 2x2, stride 2 뿐으로, multi-clk adder와 동일하게 counter의 비트 수를 늘리면 3x3, stride 3이나 4x4, stride 4도 가능하지만 대부분의 신경망에서 2x2를 많이 사용하므로 구현하지 않을 것 같음    
  - 처음 고안한 아키텍쳐는 위와 같은데 문제점이 존재하는데, `out_en`이 `1`로 출력되는 사이클에 `en = 0`일 경우, 동작이 꼬일 가능성이 있음     
    - 이에 mux를 하나 더 사용하는 아키텍쳐를 작성했는데, 해당 구조는 안정성은 뛰어나지만, area 측면에서 별로임 (8bit Mux보다 OR Gate가 차지하는 면적이 훨씬 작기 때문)    
  - **동작 방식**     
    - `en = 1`일 때, input 값이 FF에 저장된 값보다 크면 FF의 값이 새로운 값으로 갱신됨     
    - `en` 신호가 들어온 횟수마다 count를 1씩 증가시켜 4번 들어올 때마다 `out_en = 1`을 출력      
    - 결과적으로 `en = 1`일 때 들어온 값이 4개일 때마다 `out_en = 1`을 출력하는 동시에 comparator에 최저값을 보내 input이 그대로 FF에 저장됨    

&nbsp;

## 2. Planned for Design   

&nbsp;

<div align="left">
  <img src="/assets/images/npu/17.jpg" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

<div align="left">
  <img src="/assets/images/npu/20.jpg" width="100%" height="100%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

