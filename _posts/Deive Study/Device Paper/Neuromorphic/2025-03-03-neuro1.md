---
layout : single
title: "Spiking Neural Network With Weight-Sharing Synaptic Array for Multi-input Processing"   
categories: 
  - Device Paper Review
tags:
  - FeFET  
  - Neuromorphic       
  - SNN   
  - Inference   
toc: true
toc_sticky: true
use_math: true
---



[논문 링크](https://ieeexplore.ieee.org/document/9852258)         

- [IEEE Electron Device Letters](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=55)   
  - **Volume: 43, Issue: 10, October 2022**   
  - **Page(s): 1657 - 1660**  
  - **Date of Publication: 08 August 2022**   
  - **DOI: 10.1109/LED.2022.3197239**    
  - **Print ISSN: 0741-3106, Electronic ISSN: 1558-0563**   
- **Inter-university Semiconductor Research Center(ISRC), Department of Electrical and Computer Engineering, Seoul National University, Seoul, Republic of Korea**      
  - [Seunghwan Song](https://ieeexplore.ieee.org/author/37088971575), [Munhyeon Kim](https://ieeexplore.ieee.org/author/37086855005), [Bosung Jeon](https://ieeexplore.ieee.org/author/37088936352), [Donghyun Ryu](https://ieeexplore.ieee.org/author/37086838647), [Sihyun Kim](https://ieeexplore.ieee.org/author/37085805964), [Kitae Lee](https://ieeexplore.ieee.org/author/37086309825), [Jongho Lee](https://ieeexplore.ieee.org/author/37085367913), [Jae-Joon Kim](https://ieeexplore.ieee.org/author/37076821100), [Byung-Gook Park](https://ieeexplore.ieee.org/author/37278999100)     
- **Department of Electrical and Information Engineering, Seoul National University of Science and Technology, Seoul, South Korea**    
  - [Wonbo Shim](https://ieeexplore.ieee.org/author/37394024400)      
- **Department of Electrical and Computer Engineering and the 3D Convergence Center, Inha University, Incheon, South Korea**   
  - [Daewoong Kwon](https://ieeexplore.ieee.org/author/37402105900)   


## 0. Abstract    

&nbsp;

- **MSS 제시**   
  - 본 논문에서는 기존의 Spiking Neural Network(SNN) 추론 시스템 대비 병렬 처리 능력을 향상시키기 위해 **Multi-inpout processing SNN inference System(MSS)**을 제시함     
    - Compute-in-Memory를 위한 Shared Synaptic Array를 사용하는 MSS는 여러개의 Input Sample을 동시에 처리할 수 있음   
    - 이를 통해 병렬 처리에 필요한 Synaptic Array의 수를 줄임으로써 병렬 처리 네트워크의 부담을 최소화함    

&nbsp;

- **MSS 검증**   
  - Shared Synaptic Array는 FeFET의 4bit Quantization 특성을 통해 평가를 수행했으며, MSS를 검증하기 위해 3-FC layer에서의 다중 입력을 통해 Batch action을 구현함    
  - Multi-input Processing의 수에 기반하여 MSS의 전력 소비량과 면적을 분석함   
  - 그 결과, 본 논문에서 제시된 MSS를 사용하여 여러 개의 Input sample을 동시에 처리할 경우, 전력 효율은 최대 9.12배, 면적 효율은 최대 242배까지 향상됨이 입증됨   

&nbsp;

## 1. Introduction   

&nbsp;

- **기존 SNN 시스템의 장단점**   
  - Neuromorphic Computing의 경우, 그 특유의 연산 구조 때문에 막대한 양의 병렬 처리를 요구하는데, SNN은 Event-driven 특성 덕분에 이에 최적화된 신경망 구조로 평가받음    
  - 또한 기존의 SNN 추론 시스템(Conventional SNN System, CSS)은 SNN이 아닌 신경망의 Pre-trained weight를 synaptic wight로써 호출이 가능하며, 특히 Off-chip 학습 기반 SNN은 추론에 대한 높은 수준의 Accuracy를 보여줌   
  - 하지만, CSS의 시간 및 전력 소비는 data의 양에 비례하므로, 병렬 처리에 있어 보다 효울적인 구조가 요구되고 있음    

&nbsp;

- **MSS 연구**   
  - 본 논문에서는 **Shared Synaptic Array**를 사용하여 여러 개의 Input sample을 동시에 처리하는 **Multi-input processing SNN inference System(MSS)**를 제시함   
  - FeFET에서 생성된 Multi-level synaptic weight와 함께 Analog Circuit Simulation을 통해 MSS의 검증을 수행했으며, Python-based MSS model이 다중 입력 추론 연산을 성공적으로 수행하는지에 대해 확인함    
  - 연구 결과, 병렬성이 극대화된 MSS를 이용하여 대량의 Input data를 동시에 처리할 경우, CSS 대비 전력 소비 및 면적 효율 측면에서 우수함이 입증됨    

&nbsp;

## 2. System Implementation & Operation    
### 2-1. Multi-Input SNN Inference Processing System   

&nbsp;

- **CSS의 한계**   
  - ANN에서는 weight를 여러번 복제함으로써 여러 개의 Input sample에 대해 동시에 Foward-propagation을 진행이 가능함    
  - CSS 또한 ANN에서 normalized & pre-trained weight를 import 후, 복제함으로써 여러 개의 Input sample을 동시에 처리할 수 있음    
    - 다만, CSS Batch 연산은 동일한 weight를 여러 Synaptic device에 반복해서 program해야 하므로, 면적 활용과 전력 소비 측면에서 비효율적임    
    - 반면, 본 논문에서 제시된 **MSS는 단 하나의 Synaptic Array를 공유함으로써 Batch SNN 연산을 효율적으로 수행 가능함**   

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/16.png" width="100%" height="100%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **MSS의 Operation Process Flow**   
  - Fig.1(a)는 MSS의 회로도와 전체 동작에 대한 Process Flow를 도식화한 것으로 이는 다음의 과정을 밟음    
    - **Word Line Rotating Module(WL-RM)은 Synaptic Array의 M개의 WL을 순차적으로 선택함**    
      - 각 WL의 Read time은 Presynaptic Spike input($$S_k^N$$)의 width와 동일하며, 여기서 $$K$$는 Sample Index, $$N$$은 Presynaptic Node의 Index에 해당    
    - **WL-RM이 동작하는 동안, 각 BL은 WL에 의해 선택된 Synaptic Device의 weighted current($$i_{BL}^N$$)가 흐르는데, 해당 BL 전류는 Current Duplication Module(CDM)의 입력으로 전달됨**     
    - **Pre-Processing input Module(PPM)은 $$k$$x$$N$$ Presynaptic Node channel로부터 입력된 $$s_k^N$$의 width를 조절하여 전처리된 입력 신호, $$x_k^N$$을 생성함**   
      - 여기서 전처리란 각 spike의 width를 WL-RM의 한 cycle로 확장함을 의미    
      - **PPM의 output, $$x_k^N$$은 CDM에서 복제된 BL 전류가 Data Line(DL)로 전달될지를 결정함**   
    - **$$k$$x$$N$$ 개의 CDM output current는 $$k$$개의 DL로 전달되어 각 sample 별로 독립적으로 누적되고, $$k$$ 개의 Partial Weighted Sum Current(PWSC)를 생성함**   
      - 각 PWSC는 $$N$$개의 Presynaptic Node로부터 하나의 Postsynaptic Neuron($$P_k^{MN}$$)으로 전달되는 모든 정보를 포함하고 있음    
      - 즉, $$k$$x$$N$$ 개의 Presynaptic Node에서 $$k$$x$$M$$ 개의 Postsynaptic Neuron으로 전달되는 Single time-step의 정보는 WL-RM의 한 cycle에서 처리됨   
    - **마지막으로, Current Collecting Module(CCM)은 $$M$$개의 Switch Transistor를 이용해 PWSC를 전달할 Postsynaptic Neuron을 선택함**   
      - $$M$$은 WL-RM에 의해 선택된 WL의 개수와 같기 때문에, CCM에 의해 선택된 Neuron의 address는 해당 WL과 동기화됨    

&nbsp;

- **MSS Timing Diagram**   
  - Fig.1(c)는 3개의 서로 다른 Presynaptic Spike가 MSS에 입력될 때의 Timing Diagram을 보여줌    
  - PPM은 각 Spike의 width를 WL-RM의 Cycle time에 맞게 확장함    
  - PWSC는 PPM의 출력 신호에 의해 선택된 Duplicated BL current를 합산함으로써 생성됨   
  - MSS는 이러한 Transmission Flow를 전체 신경망의 size인 $$N$$x$$M$$으로 확장하여 $$k$$의 sample을 동시에 처리함   

&nbsp;

### 2-2. Synaptic Device Characteristics    

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/17.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **FeFET Process Flow**   
  - 본 논문에서는 Synaptic Device로써 FeFET을 사용했으며 Process Flow는 Fig.2(a)에 나타나 있는데 이는 다음과 같음    
    - Active Pattering & Etching 후, Native Oxide 식각    
    - Standard Cleaning-1 (SC-1)을 통해 IL 형성   
    - ALD를 통해 HZO 증착    
    - 100nm 두께의 TiN Metal Gate를 증착 후, S/D 형성을 위한 Ion Implantation 진행   
  - 제조된 FeFET의 Gate Stack은 TEM을 통해 관측되었으며 HZO/IL 두께는 7/1.5nm에 해당함(Fig.2(b) 참고)   

&nbsp;

- **Shared FeFET Synaptic Array**   
  - Fig.2(b)에서 볼 수 있듯이, Shared FeFET Synaptic Array는 분극을 최대화하기 위해 AND type으로 구성되었는데, Transfer curve와 Multi-state 측정 결과는 각각 Fig.2(c)와 Fig.2(d)에 나타나 있음   
  - Conductance가 4.15~6.5μS의 범위로 표현되는 16개의 State가 구현됨을 바탕으로 해당 소자가 Neuromorphic Application에 적합함을 알 수 있음    
    - 특히, FeFET의 weight variation의 경우 약 0.41%에 해당함   

&nbsp;

## 3. Results & Discussion   

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/18.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **MSS 시뮬레이션**
  - MSS의 성능을 평가하기 위해 본 논문에서는 FeFET의 conductance을 Shared weight로 사용하는 Circult & SNN 시뮬레이션이 수행됨 (관련 Hyperparameter는 Table II 참고)  
  - Synaptic Array 내 WL의 개수($$N_{WL}$$)는 10으로 고정되었으며, 이를 초과하는 Output Neuron의 경우, 다른 BL로 재배치 됨    
    - 이는 CDM이 각 BL에 할당되기 때문에, Postsynaptic Neuron의 수가 $$N_{WL}$$을 초과하더라도 신경망 자체는 MSS 내에서 재구성이 가능하기 때문   
  - CSS와 MSS의 추론 모델 시뮬레이션을 위해 3-layer Non-spiking ANN을 MNIST Dataset에 대해 Pre-train을 수행    
    - Non-spiking ANN의 weight는 Data-based Normalization을 통해 FeFET synapse model의 Quantized Conductance로 변환함     

&nbsp;

> **Pre-trained weight → Conductance?**   
>   - ANN에 의해 학습된 weight는 floating point로 표현될텐데, FeFET synapse가 weight를 저장할려면 Conductance로 표현해야함    
>       - 즉, ANN의 weight를 FeFET의 Conductance 값으로 변환해야 하고 이를 본 논문에서는 **Quantization**이라고 설명하는 것    
>   - 만약 ANN의 pre-trained weight이 있다면 이를 FeFET이 표현할 수 있는 4.15μS ~ 6.5μS 범위의 16개의 state로 mapping을 진행해야 함   
>       - 이를 위해 필요한 것이 Normalization인데, **단순히 min-max 기준으로 매핑을 수행하면 weight의 분해능이 저하될수 있으므로**, 전체 weight의 분포를 분석, 이를 통해 중심값이나 분산 등을 고려하여 매핑을 수행해야 함    

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/19.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Postsynaptic 설계**
  - Postsynaptic Neuron에는 **Integrate & Fire model**이 HW model로 사용됨    
    - Fig.3(a)에서 볼 수 있듯이, CSS와 MSS 추론 모델에서 weighted sum current pair는 Postsynaptic Neuron에 연결됨    
    - 이때, **서로 다른 방향의 weighted sum current($$I_E, I_I$$)**는 Postsynaptic Neuron에 **Excitatory(흥분성)**와 **Inhibitory(억제성)**를 전달함   
  - **$$I_E$$와 $$I_E$$ 사이에 차이가 발생하면, Postsynaptic Neuron의 Membrane Potential이 변하는데, 해당 Potential이 Threshold level을 초과하게 되면 Neuron은 다음 계층으로 Spike를 발생시킴**    

