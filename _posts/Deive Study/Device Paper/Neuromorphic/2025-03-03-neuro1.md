---
layout : single
title: "Spiking Neural Network With Weight-Sharing Synaptic Array for Multi-input Processing"   
categories: 
  - Device Paper Review
tags:
  - FeFET  
  - Neuromorphic       
  - SNN   
  - Inference   
toc: true
toc_sticky: true
use_math: true
---



[논문 링크](https://ieeexplore.ieee.org/document/9852258)         

- [IEEE Electron Device Letters](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=55)   
  - **Volume: 43, Issue: 10, October 2022**   
  - **Page(s): 1657 - 1660**  
  - **Date of Publication: 08 August 2022**   
  - **DOI: 10.1109/LED.2022.3197239**    
  - **Print ISSN: 0741-3106, Electronic ISSN: 1558-0563**   
- **Inter-university Semiconductor Research Center(ISRC), Department of Electrical and Computer Engineering, Seoul National University, Seoul, Republic of Korea**      
  - [Seunghwan Song](https://ieeexplore.ieee.org/author/37088971575), [Munhyeon Kim](https://ieeexplore.ieee.org/author/37086855005), [Bosung Jeon](https://ieeexplore.ieee.org/author/37088936352), [Donghyun Ryu](https://ieeexplore.ieee.org/author/37086838647), [Sihyun Kim](https://ieeexplore.ieee.org/author/37085805964), [Kitae Lee](https://ieeexplore.ieee.org/author/37086309825), [Jongho Lee](https://ieeexplore.ieee.org/author/37085367913), [Jae-Joon Kim](https://ieeexplore.ieee.org/author/37076821100), [Byung-Gook Park](https://ieeexplore.ieee.org/author/37278999100)     
- **Department of Electrical and Information Engineering, Seoul National University of Science and Technology, Seoul, South Korea**    
  - [Wonbo Shim](https://ieeexplore.ieee.org/author/37394024400)      
- **Department of Electrical and Computer Engineering and the 3D Convergence Center, Inha University, Incheon, South Korea**   
  - [Daewoong Kwon](https://ieeexplore.ieee.org/author/37402105900)   


## 0. Abstract    

&nbsp;

- **MSS 제시**   
  - 본 논문에서는 기존의 Spiking Neural Network(SNN) 추론 시스템 대비 병렬 처리 능력을 향상시키기 위해 **Multi-input processing SNN inference System(MSS)**을 제시함     
    - Compute-in-Memory를 위한 Shared Synaptic Array를 사용하는 MSS는 여러개의 Input Sample을 동시에 처리할 수 있음   
    - 이를 통해 병렬 처리에 필요한 Synaptic Array의 수를 줄임으로써 병렬 처리 네트워크의 부담을 최소화함    

&nbsp;

- **MSS 검증**   
  - Shared Synaptic Array는 FeFET의 4bit Quantization 특성을 통해 평가를 수행했으며, MSS를 검증하기 위해 3-FC layer에서의 다중 입력을 통해 Batch action을 구현함    
  - Multi-input Processing의 수에 기반하여 MSS의 전력 소비량과 면적을 분석함   
  - 그 결과, 본 논문에서 제시된 MSS를 사용하여 여러 개의 Input sample을 동시에 처리할 경우, 전력 효율은 최대 9.12배, 면적 효율은 최대 242배까지 향상됨이 입증됨   

&nbsp;

## 1. Introduction   

&nbsp;

- **기존 SNN 시스템의 장단점**   
  - Neuromorphic Computing의 경우, 그 특유의 연산 구조 때문에 막대한 양의 병렬 처리를 요구하는데, SNN은 Event-driven 특성 덕분에 이에 최적화된 신경망 구조로 평가받음    
  - 또한 기존의 SNN 추론 시스템(Conventional SNN System, CSS)은 SNN이 아닌 신경망의 Pre-trained weight를 synaptic wight로써 호출이 가능하며, 특히 Off-chip 학습 기반 SNN은 추론에 대한 높은 수준의 Accuracy를 보여줌   
  - 하지만, CSS의 시간 및 전력 소비는 data의 양에 비례하므로, 병렬 처리에 있어 보다 효울적인 구조가 요구되고 있음    

&nbsp;

- **MSS 연구**   
  - 본 논문에서는 **Shared Synaptic Array**를 사용하여 여러 개의 Input sample을 동시에 처리하는 **Multi-input processing SNN inference System(MSS)**를 제시함   
  - FeFET에서 생성된 Multi-level synaptic weight와 함께 Analog Circuit Simulation을 통해 MSS의 검증을 수행했으며, Python-based MSS model이 다중 입력 추론 연산을 성공적으로 수행하는지에 대해 확인함    
  - 연구 결과, 병렬성이 극대화된 MSS를 이용하여 대량의 Input data를 동시에 처리할 경우, CSS 대비 전력 소비 및 면적 효율 측면에서 우수함이 입증됨    

&nbsp;

## 2. System Implementation & Operation    
### 2-1. Multi-Input SNN Inference Processing System   

&nbsp;

- **CSS의 한계**   
  - ANN에서는 weight를 여러번 복제함으로써 여러 개의 Input sample에 대해 동시에 Foward-propagation을 진행이 가능함    
  - CSS 또한 ANN에서 normalized & pre-trained weight를 import 후, 복제함으로써 여러 개의 Input sample을 동시에 처리할 수 있음    
    - 다만, CSS Batch 연산은 동일한 weight를 여러 Synaptic device에 반복해서 program해야 하므로, 면적 활용과 전력 소비 측면에서 비효율적임    
    - 반면, 본 논문에서 제시된 **MSS는 단 하나의 Synaptic Array를 공유함으로써 Batch SNN 연산을 효율적으로 수행 가능함**   

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/16.png" width="100%" height="100%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **MSS의 Operation Process Flow**   
  - Fig.1(a)는 MSS의 회로도와 전체 동작에 대한 Process Flow를 도식화한 것으로 이는 다음의 과정을 밟음    
    - **Word Line Rotating Module(WL-RM)은 Synaptic Array의 M개의 WL을 순차적으로 선택함**    
      - 각 WL의 Read time은 Presynaptic Spike input($$S_k^N$$)의 width와 동일하며, 여기서 $$K$$는 Sample Index, $$N$$은 Presynaptic Node의 Index에 해당    
    - **WL-RM이 동작하는 동안, 각 BL은 WL에 의해 선택된 Synaptic Device의 weighted current($$i_{BL}^N$$)가 흐르는데, 해당 BL 전류는 Current Duplication Module(CDM)의 입력으로 전달됨**     
    - **Pre-Processing input Module(PPM)은 $$k$$x$$N$$ Presynaptic Node channel로부터 입력된 $$s_k^N$$의 width를 조절하여 전처리된 입력 신호, $$x_k^N$$을 생성함**   
      - 여기서 전처리란 각 spike의 width를 WL-RM의 한 cycle로 확장함을 의미    
      - **PPM의 output, $$x_k^N$$은 CDM에서 복제된 BL 전류가 Data Line(DL)로 전달될지를 결정함**   
    - **$$k$$x$$N$$ 개의 CDM output current는 $$k$$개의 DL로 전달되어 각 sample 별로 독립적으로 누적되고, $$k$$ 개의 Partial Weighted Sum Current(PWSC)를 생성함**   
      - 각 PWSC는 $$N$$개의 Presynaptic Node로부터 하나의 Postsynaptic Neuron($$P_k^{MN}$$)으로 전달되는 모든 정보를 포함하고 있음    
      - 즉, $$k$$x$$N$$ 개의 Presynaptic Node에서 $$k$$x$$M$$ 개의 Postsynaptic Neuron으로 전달되는 Single time-step의 정보는 WL-RM의 한 cycle에서 처리됨   
    - **마지막으로, Current Collecting Module(CCM)은 $$M$$개의 Switch Transistor를 이용해 PWSC를 전달할 Postsynaptic Neuron을 선택함**   
      - $$M$$은 WL-RM에 의해 선택된 WL의 개수와 같기 때문에, CCM에 의해 선택된 Neuron의 address는 해당 WL과 동기화됨    

&nbsp;

- **MSS Timing Diagram**   
  - Fig.1(c)는 3개의 서로 다른 Presynaptic Spike가 MSS에 입력될 때의 Timing Diagram을 보여줌    
  - PPM은 각 Spike의 width를 WL-RM의 Cycle time에 맞게 확장함    
  - PWSC는 PPM의 출력 신호에 의해 선택된 Duplicated BL current를 합산함으로써 생성됨   
  - MSS는 이러한 Transmission Flow를 전체 신경망의 size인 $$N$$x$$M$$으로 확장하여 $$k$$의 sample을 동시에 처리함   

&nbsp;

### 2-2. Synaptic Device Characteristics    

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/17.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **FeFET Process Flow**   
  - 본 논문에서는 Synaptic Device로써 FeFET을 사용했으며 Process Flow는 Fig.2(a)에 나타나 있는데 이는 다음과 같음    
    - Active Pattering & Etching 후, Native Oxide 식각    
    - Standard Cleaning-1 (SC-1)을 통해 IL 형성   
    - ALD를 통해 HZO 증착    
    - 100nm 두께의 TiN Metal Gate를 증착 후, S/D 형성을 위한 Ion Implantation 진행   
  - 제조된 FeFET의 Gate Stack은 TEM을 통해 관측되었으며 HZO/IL 두께는 7/1.5nm에 해당함(Fig.2(b) 참고)   

&nbsp;

- **Shared FeFET Synaptic Array**   
  - Fig.2(b)에서 볼 수 있듯이, Shared FeFET Synaptic Array는 분극을 최대화하기 위해 AND type으로 구성되었는데, Transfer curve와 Multi-state 측정 결과는 각각 Fig.2(c)와 Fig.2(d)에 나타나 있음   
  - Conductance가 4.15~6.5μS의 범위로 표현되는 16개의 State가 구현됨을 바탕으로 해당 소자가 Neuromorphic Application에 적합함을 알 수 있음    
    - 특히, FeFET의 weight variation의 경우 약 0.41%에 해당함   

&nbsp;

## 3. Results & Discussion   

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/18.png" width="70%" height="70%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **MSS 시뮬레이션**
  - MSS의 성능을 평가하기 위해 본 논문에서는 FeFET의 conductance을 Shared weight로 사용하는 Circult & SNN 시뮬레이션이 수행됨 (관련 Hyperparameter는 Table II 참고)  
  - Synaptic Array 내 WL의 개수($$N_{WL}$$)는 10으로 고정되었으며, 이를 초과하는 Output Neuron의 경우, 다른 BL로 재배치 됨    
    - 이는 CDM이 각 BL에 할당되기 때문에, Postsynaptic Neuron의 수가 $$N_{WL}$$을 초과하더라도 신경망 자체는 MSS 내에서 재구성이 가능하기 때문   
  - CSS와 MSS의 추론 모델 시뮬레이션을 위해 3-layer Non-spiking ANN을 MNIST Dataset에 대해 Pre-train을 수행    
    - Non-spiking ANN의 weight는 Data-based Normalization을 통해 FeFET synapse model의 Quantized Conductance로 변환함     

&nbsp;

> **Pre-trained weight → Conductance?**   
>   - ANN에 의해 학습된 weight는 floating point로 표현될텐데, FeFET synapse가 weight를 저장할려면 Conductance로 표현해야함    
>       - 즉, ANN의 weight를 FeFET의 Conductance 값으로 변환해야 하고 이를 본 논문에서는 **Quantization**이라고 설명하는 것    
>   - 만약 ANN의 pre-trained weight이 있다면 이를 FeFET이 표현할 수 있는 4.15μS ~ 6.5μS 범위의 16개의 state로 mapping을 진행해야 함   
>       - 이를 위해 필요한 것이 Normalization인데, **단순히 min-max 기준으로 매핑을 수행하면 weight의 분해능이 저하될수 있으므로**, 전체 weight의 분포를 분석, 이를 통해 중심값이나 분산 등을 고려하여 매핑을 수행해야 함    

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/19.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **Postsynaptic 설계**
  - Postsynaptic Neuron에는 **Integrate & Fire model**이 HW model로 사용됨    
    - Fig.3(a)에서 볼 수 있듯이, CSS와 MSS 추론 모델에서 weighted sum current pair는 Postsynaptic Neuron에 연결됨    
    - 이때, **서로 다른 방향의 weighted sum current($$I_E, I_I$$)**는 Postsynaptic Neuron에 **Excitatory(흥분성)**와 **Inhibitory(억제성)**를 전달함   
  - **$$I_E$$와 $$I_E$$ 사이에 차이가 발생하면, Postsynaptic Neuron의 Membrane Potential이 변하는데, 해당 Potential이 Threshold level을 초과하게 되면 Neuron은 다음 계층으로 Spike를 발생시킴**    

&nbsp;

- **PWSC Error 측정**    
  - Fig.3(b)는 MSS에서 100개의 랜덤한 Presynaptic Spike 신호가 입력되었을 때, 측정된 PWSC와 Target PWSC 간의 오차를 보여줌    
    - Spice Simulation의 경우 BSIM-CMG model을 사용, CDM & CCM Transistor를 구현한 회로 기반 모델로 수행     
    - 해당 CDM은 FeFET synapse model이 16개의 서로 다른 conducatnce level로 생성한 weighted current를 DL로 전달함   
  - σw/μw, 즉 weight variation이 0%일 때, PWSC의 오차는 최대 0.08%에 불과한 것을 확인 가능한데, 이는 CDM과 CCM이 복수의 BL로부터 출력된 weighted current를 정상적으로 복제하고 종합함을 의미함   
    - 실제 FeFET synapse의 weight variation 수치인 0.41%에 대해서는 오차가 최대 0.29%까지 증가하는데 이는 MSS가 실제 동작에 대해서도 정상적으로 동작함을 입증함   
    - 마지막으로 weight variation이 6.25%일 때 최대 PWSC 오차는 1.24%로 확인되었는데, 이는 4bit quantization 용량의 한계에 해당함(16개의 multi-state이므로 4bit에 해당함)        

&nbsp;

- **Classification Accuracy 측정**    
  - Fig.3(c)는 10,000개의 MNIST Test set에 대한 SNN 모델의 Classificaiton 결과를 Plot한 것으로 FeFET Synapse의 Programming 정밀도를 반영하기 위해 CSS와 MSS 추론 모델의 weight 분포에 0.41%를 적용함     
  - MSS 추론에서는 한 번에 2,000개의 입력을 동시에 처리하는 DUT가 모델링되었고 10,000개의 sample을 처리하기 위해 총 5개의 DUT가 사용됨    
    - 각 DUT의 경우 분담받는 Test Set이 서로 다르기 때문에 Accuracy에 차이가 있음에 주의해야 함   
  - 5개의 DUT의 평균 Accuracy는 98.17%로 CSS의 98.18%와 Non-spiking ANN의 98.25%와 비교하면 각각 0.01%, 0.08% 낮은 수준임   
    - **다만, MSS 추론 모델은 5개의 DUT 만으로 전체 sample을 병렬 처리한 반면, CSS 추론 모델은 10,000번의 반복 추론을 수행했다는 점에 주목해야 함**   

&nbsp;

<div align="center">
  <img src="/assets/images/Ferro/20.png" width="60%" height="60%" alt=""/>
  <p><em></em></p>
</div>

&nbsp;

- **CSS와 MSS의 전력 소비량 측정**   
  - Fig.4(a)는 MSS와 CSS의 Batch processing에 대한 전력 소비량를 Plot함    
    - CSS의 경우, CSS synaptic array를 여러 번 복제함으로써 Batch processing을 수행     
    - 반면, MSS는 Shared synaptic array, 즉 CCM, CDM, PPM을 복제함으로써 Batch Processing을 수행하는데 이는 대량의 추론을 처리할 때 큰 이점을 제공함   
  - Fig.4(a)에서 CSS는 10회와 50회의 추론을 수행할 시, 12.4μJ와  62.2μJ를 소비하는 반면, MSS는 $$R_m$$이 0.1일 때, 각각 35.7μJ와 41.1μJ를 소비함     
    - MSS의 peripheral circuit의 전력 소비는 $$R_m$$에 비례하기 때문에 이를 조절함으로써 추가적인 에너지 절감이 가능함    

&nbsp;

- **CSS와 MSS의 에너지 효율성 측정**   
  - Fig.4(b)에서는 CSS와 MSS의 에너지 효율성을 비교함    
  - CSS는 synaptic array의 구조 자체를 복제하여 Batch processing을 수행하기 때문에, **Batch processing의 횟수와 관계없이 에너지 효율성이 0.44 TOPS/W로 고정됨**   
  - 반면, MSS는 Synaptic array가 아닌 CDM, CCM, PPM을 복제하여 Batch processing을 수행하기 때문에 **횟수가 증가할수록 전체 전력 소비에서 해당 모듈들의 비중이 늘어남**      
    - 결과적으로, MSS의 에너지 효율성은 4.03TOPS/W에 수렴하는데 이는 CSS의 9.12배에 해당함   
    - 이는 MSS의 Peripheral circuit이 CSS보다 전력을 적게 소비함을 의미함   

&nbsp;

- **CSS와 MSS의 면적 점유율 비교**   
  - Fig.4(c)에서는 MSS와 CSS의 면적 점유율을 비교함    
  - 1개의 sample을 Batch processing하기 위해 요구되는 면적 증가는 CSS의 경우 3.80mm², MSS는 0.09mm²로 추정됨    
    - **이는 MSS의 Peripheral circuit(CDM, CCM, PPM)이 CSS의 Synaptic array보다 훨씬 작은 면적을 차지함을 의미함**   
  - Fig.4(d)를 보면 MSS의 Batch Processing에 대한 면적 효율성이 CSS보다 최대 242배 높은 것을 알 수 있음    
    - 물론 FeFET의 Scaling 계수($$\alpha$$)가 0.1일 경우, 해당 수치는 2.48배로 감소하지만, 그럼에도 불구하고 MSS가 CSS보다 면적 점유율에서 유리한 것은 여전함   

&nbsp;

## 4. Summary    

&nbsp;

- **Multi-input processing SNN interface System(MSS)**   
  - 기존에 SNN 추론 시스템은 여러 개의 Synaptic array를 복제함으로써 Batch processing을 수행함   
  - 본 논문에서 제시한 MSS는 Synaptic Array를 복제하지 않되, **Peri 회로에 해당하는 CDM, CCM, PPM 모듈의 수를 늘림으로써 병렬 처리를 수행함**    
  - 각 모듈의 역할을 요약하면 다음과 같음    
    - **WL-RM** : Synaptic array의 WL을 선택    
    - **PPM** : 입력 spike를 전처리함으로써 Spike width 조절    
    - **CDM** : Synaptic array의 BL에서 출력된 weighted current를 복제, 누적하여 PWSC 생성 
    - **CCM** : CDM에서 출력된 PWSC를 입력받을 Postsynaptic Neuron을 선택    

&nbsp;

- **MSS의 장점**   
  - Target PSMC와 MSS에서 측정된 PSMC의 오차는 실제 FeFET synapse의 weight variation인 0.41%를 적용했을 때, 0.29%로 정상적인 수치로 나타남    
  - CSS와 비교하면, MSS의 Accuracy 자체는 미세한 수준으로 낮지만, **병렬 처리 측면에서는 MSS가 압도적으로 우세**    
  - CSS의 경우, Synaptic array를 복제해야 하기 때문에 전력 소비량과 에너지 효율성이 안좋은 반면, **MSS는 Peri 모듈을 복제함으로써 낮은 전력 소비량과 높은 에너지 효율성을 얻을 수 있음**   


&nbsp;


